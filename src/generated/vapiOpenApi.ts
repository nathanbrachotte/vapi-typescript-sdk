/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

export interface paths {
    "/call": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** List Calls */
        get: operations["CallController_findAll"];
        put?: never;
        /** Create Call */
        post: operations["CallController_create"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/call/{id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** Get Call */
        get: operations["CallController_findOne"];
        put?: never;
        post?: never;
        /** Delete Call Data */
        delete: operations["CallController_deleteCallData"];
        options?: never;
        head?: never;
        /** Update Call */
        patch: operations["CallController_update"];
        trace?: never;
    };
    "/assistant": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** List Assistants */
        get: operations["AssistantController_findAll"];
        put?: never;
        /** Create Assistant */
        post: operations["AssistantController_create"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/assistant/{id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** Get Assistant */
        get: operations["AssistantController_findOne"];
        put?: never;
        post?: never;
        /** Delete Assistant */
        delete: operations["AssistantController_remove"];
        options?: never;
        head?: never;
        /** Update Assistant */
        patch: operations["AssistantController_update"];
        trace?: never;
    };
    "/phone-number": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** List Phone Numbers */
        get: operations["PhoneNumberController_findAll"];
        put?: never;
        /** Create Phone Number */
        post: operations["PhoneNumberController_create"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/phone-number/{id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** Get Phone Number */
        get: operations["PhoneNumberController_findOne"];
        put?: never;
        post?: never;
        /** Delete Phone Number */
        delete: operations["PhoneNumberController_remove"];
        options?: never;
        head?: never;
        /** Update Phone Number */
        patch: operations["PhoneNumberController_update"];
        trace?: never;
    };
    "/squad": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** List Squads */
        get: operations["SquadController_findAll"];
        put?: never;
        /** Create Squad */
        post: operations["SquadController_create"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/squad/{id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** Get Squad */
        get: operations["SquadController_findOne"];
        put?: never;
        post?: never;
        /** Delete Squad */
        delete: operations["SquadController_remove"];
        options?: never;
        head?: never;
        /** Update Squad */
        patch: operations["SquadController_update"];
        trace?: never;
    };
    "/block": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** List Blocks */
        get: operations["BlockController_findAll"];
        put?: never;
        /** Create Block */
        post: operations["BlockController_create"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/block/{id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** Get Block */
        get: operations["BlockController_findOne"];
        put?: never;
        post?: never;
        /** Delete Block */
        delete: operations["BlockController_remove"];
        options?: never;
        head?: never;
        /** Update Block */
        patch: operations["BlockController_update"];
        trace?: never;
    };
    "/tool": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** List Tools */
        get: operations["ToolController_findAll"];
        put?: never;
        /** Create Tool */
        post: operations["ToolController_create"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/tool/{id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** Get Tool */
        get: operations["ToolController_findOne"];
        put?: never;
        post?: never;
        /** Delete Tool */
        delete: operations["ToolController_remove"];
        options?: never;
        head?: never;
        /** Update Tool */
        patch: operations["ToolController_update"];
        trace?: never;
    };
    "/file": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** List Files */
        get: operations["FileController_findAll"];
        put?: never;
        /** Upload File */
        post: operations["FileController_create"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
    "/file/{id}": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        /** Get File */
        get: operations["FileController_findOne"];
        put?: never;
        post?: never;
        /** Delete File */
        delete: operations["FileController_remove"];
        options?: never;
        head?: never;
        /** Update File */
        patch: operations["FileController_update"];
        trace?: never;
    };
    "/analytics": {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        get?: never;
        put?: never;
        /** Create Analytics Queries */
        post: operations["AnalyticsController_query"];
        delete?: never;
        options?: never;
        head?: never;
        patch?: never;
        trace?: never;
    };
}
export type webhooks = Record<string, never>;
export interface components {
    schemas: {
        DeepgramTranscriber: {
            /**
             * @description This is the transcription provider that will be used.
             * @enum {string}
             */
            provider: "deepgram";
            /** @description This is the Deepgram model that will be used. A list of models can be found here: https://developers.deepgram.com/docs/models-languages-overview */
            model?: ("nova-2" | "nova-2-general" | "nova-2-meeting" | "nova-2-phonecall" | "nova-2-finance" | "nova-2-conversationalai" | "nova-2-voicemail" | "nova-2-video" | "nova-2-medical" | "nova-2-drivethru" | "nova-2-automotive" | "nova" | "nova-general" | "nova-phonecall" | "nova-medical" | "enhanced" | "enhanced-general" | "enhanced-meeting" | "enhanced-phonecall" | "enhanced-finance" | "base" | "base-general" | "base-meeting" | "base-phonecall" | "base-finance" | "base-conversationalai" | "base-voicemail" | "base-video") | string;
            /**
             * @description This is the language that will be set for the transcription. The list of languages Deepgram supports can be found here: https://developers.deepgram.com/docs/models-languages-overview
             * @enum {string}
             */
            language?: "bg" | "ca" | "cs" | "da" | "da-DK" | "de" | "de-CH" | "el" | "en" | "en-AU" | "en-GB" | "en-IN" | "en-NZ" | "en-US" | "es" | "es-419" | "es-LATAM" | "et" | "fi" | "fr" | "fr-CA" | "hi" | "hi-Latn" | "hu" | "id" | "it" | "ja" | "ko" | "ko-KR" | "lt" | "lv" | "ms" | "multi" | "nl" | "nl-BE" | "no" | "pl" | "pt" | "pt-BR" | "ro" | "ru" | "sk" | "sv" | "sv-SE" | "ta" | "taq" | "th" | "th-TH" | "tr" | "uk" | "vi" | "zh" | "zh-CN" | "zh-Hans" | "zh-Hant" | "zh-TW";
            /**
             * @description This will be use smart format option provided by Deepgram. It's default disabled because it can sometimes format numbers as times but it's getting better.
             * @example false
             */
            smartFormat?: boolean;
            /** @description These keywords are passed to the transcription model to help it pick up use-case specific words. Anything that may not be a common word, like your company name, should be added here. */
            keywords?: string[];
            /** @description This is the timeout after which Deepgram will send transcription on user silence. You can read in-depth documentation here: https://developers.deepgram.com/docs/endpointing.
             *
             *     Here are the most important bits:
             *     - Defaults to 10. This is recommended for most use cases to optimize for latency.
             *     - 10 can cause some missing transcriptions since because of the shorter context. This mostly happens for one-word utterances. For those uses cases, it's recommended to try 300. It will add a bit of latency but the quality and reliability of the experience will be better.
             *     - If neither 10 nor 300 work, contact support@vapi.ai and we'll find another solution.
             *
             *     @default 10 */
            endpointing?: number;
        };
        TalkscriberTranscriber: {
            /**
             * @description This is the transcription provider that will be used.
             * @enum {string}
             */
            provider: "talkscriber";
            /**
             * @description This is the model that will be used for the transcription.
             * @enum {string}
             */
            model?: "whisper";
            /**
             * @description This is the language that will be set for the transcription. The list of languages Whisper supports can be found here: https://github.com/openai/whisper/blob/main/whisper/tokenizer.py
             * @enum {string}
             */
            language?: "en" | "zh" | "de" | "es" | "ru" | "ko" | "fr" | "ja" | "pt" | "tr" | "pl" | "ca" | "nl" | "ar" | "sv" | "it" | "id" | "hi" | "fi" | "vi" | "he" | "uk" | "el" | "ms" | "cs" | "ro" | "da" | "hu" | "ta" | "no" | "th" | "ur" | "hr" | "bg" | "lt" | "la" | "mi" | "ml" | "cy" | "sk" | "te" | "fa" | "lv" | "bn" | "sr" | "az" | "sl" | "kn" | "et" | "mk" | "br" | "eu" | "is" | "hy" | "ne" | "mn" | "bs" | "kk" | "sq" | "sw" | "gl" | "mr" | "pa" | "si" | "km" | "sn" | "yo" | "so" | "af" | "oc" | "ka" | "be" | "tg" | "sd" | "gu" | "am" | "yi" | "lo" | "uz" | "fo" | "ht" | "ps" | "tk" | "nn" | "mt" | "sa" | "lb" | "my" | "bo" | "tl" | "mg" | "as" | "tt" | "haw" | "ln" | "ha" | "ba" | "jw" | "su" | "yue";
        };
        GladiaTranscriber: {
            /**
             * @description This is the transcription provider that will be used.
             * @enum {string}
             */
            provider: "gladia";
            /** @description This is the Gladia model that will be used. Default is 'fast' */
            model?: "fast" | "accurate";
            /** @description Defines how the transcription model detects the audio language. Default value is 'automatic single language'. */
            languageBehaviour?: "manual" | "automatic single language" | "automatic multiple languages";
            /**
             * @description Defines the language to use for the transcription. Required when languageBehaviour is 'manual'.
             * @enum {string}
             */
            language?: "af" | "sq" | "am" | "ar" | "hy" | "as" | "az" | "ba" | "eu" | "be" | "bn" | "bs" | "br" | "bg" | "ca" | "zh" | "hr" | "cs" | "da" | "nl" | "en" | "et" | "fo" | "fi" | "fr" | "gl" | "ka" | "de" | "el" | "gu" | "ht" | "ha" | "haw" | "he" | "hi" | "hu" | "is" | "id" | "it" | "ja" | "jp" | "jv" | "kn" | "kk" | "km" | "ko" | "lo" | "la" | "lv" | "ln" | "lt" | "lb" | "mk" | "mg" | "ms" | "ml" | "mt" | "mi" | "mr" | "mn" | "mymr" | "ne" | "no" | "nn" | "oc" | "ps" | "fa" | "pl" | "pt" | "pa" | "ro" | "ru" | "sa" | "sr" | "sn" | "sd" | "si" | "sk" | "sl" | "so" | "es" | "su" | "sw" | "sv" | "tl" | "tg" | "ta" | "tt" | "te" | "th" | "bo" | "tr" | "tk" | "uk" | "ur" | "uz" | "vi" | "cy" | "yi" | "yo";
            /**
             * @description Provides a custom vocabulary to the model to improve accuracy of transcribing context specific words, technical terms, names, etc. If empty, this argument is ignored.
             *     ⚠️ Warning ⚠️: Please be aware that the transcription_hint field has a character limit of 600. If you provide a transcription_hint longer than 600 characters, it will be automatically truncated to meet this limit.
             * @example custom vocabulary
             */
            transcriptionHint?: string;
            /**
             * @description If prosody is true, you will get a transcription that can contain prosodies i.e. (laugh) (giggles) (malefic laugh) (toss) (music)… Default value is false.
             * @example false
             */
            prosody?: boolean;
            /**
             * @description If true, audio will be pre-processed to improve accuracy but latency will increase. Default value is false.
             * @example false
             */
            audioEnhancer?: boolean;
        };
        Condition: {
            /** @description This is the value you want to compare against the parameter. */
            value: string;
            /**
             * @description This is the operator you want to use to compare the parameter and value.
             * @enum {string}
             */
            operator: "eq" | "neq" | "gt" | "gte" | "lt" | "lte";
            /** @description This is the name of the parameter that you want to check. */
            param: string;
        };
        ToolMessageStart: {
            /**
             * @description This message is triggered when the tool call starts.
             *
             *     This message is never triggered for async tools.
             *
             *     If this message is not provided, one of the default filler messages "Hold on a sec", "One moment", "Just a sec", "Give me a moment" or "This'll just take a sec" will be used.
             * @enum {string}
             */
            type: "request-start";
            /** @description This is the content that the assistant says when this message is triggered. */
            content: string;
            /** @description This is an optional array of conditions that the tool call arguments must meet in order for this message to be triggered. */
            conditions?: components["schemas"]["Condition"][];
        };
        ToolMessageComplete: {
            /**
             * @description This message is triggered when the tool call is complete.
             *
             *     This message is triggered immediately without waiting for your server to respond for async tool calls.
             *
             *     If this message is not provided, the model will be requested to respond.
             *
             *     If this message is provided, only this message will be spoken and the model will not be requested to come up with a response. It's an exclusive OR.
             * @enum {string}
             */
            type: "request-complete";
            /**
             * @description This is optional and defaults to "assistant".
             *
             *     When role=assistant, `content` is said out loud.
             *
             *     When role=system, `content` is passed to the model in a system message. Example:
             *         system: default one
             *         assistant:
             *         user:
             *         assistant:
             *         user:
             *         assistant:
             *         user:
             *         assistant: tool called
             *         tool: your server response
             *         <--- system prompt as hint
             *         ---> model generates response which is spoken
             *     This is useful when you want to provide a hint to the model about what to say next.
             * @enum {string}
             */
            role?: "assistant" | "system";
            /**
             * @description This is an optional boolean that if true, the call will end after the message is spoken. Default is false.
             *
             *     This is ignored if `role` is set to `system`.
             *
             *     @default false
             * @example false
             */
            endCallAfterSpokenEnabled?: boolean;
            /** @description This is the content that the assistant says when this message is triggered. */
            content: string;
            /** @description This is an optional array of conditions that the tool call arguments must meet in order for this message to be triggered. */
            conditions?: components["schemas"]["Condition"][];
        };
        ToolMessageFailed: {
            /**
             * @description This message is triggered when the tool call fails.
             *
             *     This message is never triggered for async tool calls.
             *
             *     If this message is not provided, the model will be requested to respond.
             *
             *     If this message is provided, only this message will be spoken and the model will not be requested to come up with a response. It's an exclusive OR.
             * @enum {string}
             */
            type: "request-failed";
            /**
             * @description This is an optional boolean that if true, the call will end after the message is spoken. Default is false.
             *
             *     @default false
             * @example false
             */
            endCallAfterSpokenEnabled?: boolean;
            /** @description This is the content that the assistant says when this message is triggered. */
            content: string;
            /** @description This is an optional array of conditions that the tool call arguments must meet in order for this message to be triggered. */
            conditions?: components["schemas"]["Condition"][];
        };
        ToolMessageDelayed: {
            /**
             * @description This message is triggered when the tool call is delayed.
             *
             *     There are the two things that can trigger this message:
             *     1. The user talks with the assistant while your server is processing the request. Default is "Sorry, a few more seconds."
             *     2. The server doesn't respond within `timingMilliseconds`.
             *
             *     This message is never triggered for async tool calls.
             * @enum {string}
             */
            type: "request-response-delayed";
            /**
             * @description The number of milliseconds to wait for the server response before saying this message.
             * @example 1000
             */
            timingMilliseconds?: number;
            /** @description This is the content that the assistant says when this message is triggered. */
            content: string;
            /** @description This is an optional array of conditions that the tool call arguments must meet in order for this message to be triggered. */
            conditions?: components["schemas"]["Condition"][];
        };
        JsonSchema: {
            /**
             * @description This is the type of output you'd like.
             *
             *     `string`, `number`, `integer`, `boolean` are the primitive types and should be obvious.
             *
             *     `array` and `object` are more interesting and quite powerful. They allow you to define nested structures.
             *
             *     For `array`, you can define the schema of the items in the array using the `items` property.
             *
             *     For `object`, you can define the properties of the object using the `properties` property.
             * @enum {string}
             */
            type: "string" | "number" | "integer" | "boolean" | "array" | "object";
            /** @description This is required if the type is "array". This is the schema of the items in the array.
             *
             *     This is of type JsonSchema. However, Swagger doesn't support circular references. */
            items?: Record<string, never>;
            /** @description This is required if the type is "object". This specifies the properties of the object.
             *
             *     This is a map of string to JsonSchema. However, Swagger doesn't support circular references. */
            properties?: Record<string, never>;
            /** @description This is the description to help the model understand what it needs to output. */
            description?: string;
            /** @description This is a list of properties that are required.
             *
             *     This only makes sense if the type is "object". */
            required?: string[];
        };
        OpenAIFunctionParameters: {
            /**
             * @description This must be set to 'object'. It instructs the model to return a JSON object containing the function call properties.
             * @enum {string}
             */
            type: "object";
            /** @description This provides a description of the properties required by the function.
             *     JSON Schema can be used to specify expectations for each property.
             *     Refer to [this doc](https://ajv.js.org/json-schema.html#json-data-type) for a comprehensive guide on JSON Schema. */
            properties: {
                [key: string]: components["schemas"]["JsonSchema"] | undefined;
            };
            /** @description This specifies the properties that are required by the function. */
            required?: string[];
        };
        OpenAIFunction: {
            /** @description This is the the name of the function to be called.
             *
             *     Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64. */
            name: string;
            description?: string;
            /** @description These are the parameters the functions accepts, described as a JSON Schema object.
             *
             *     See the [OpenAI guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema) for documentation about the format.
             *
             *     Omitting parameters defines a function with an empty parameter list. */
            parameters?: components["schemas"]["OpenAIFunctionParameters"];
        };
        Server: {
            /**
             * @description This is the timeout in seconds for the request to your server. Defaults to 20 seconds.
             *
             *     @default 20
             * @example 20
             */
            timeoutSeconds?: number;
            /** @description API endpoint to send requests to. */
            url: string;
            /** @description This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as server. */
            secret?: string;
        };
        CreateDtmfToolDTO: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "dtmf" for DTMF tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "dtmf";
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        CreateEndCallToolDTO: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "endCall" for End Call tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "endCall";
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        CreateVoicemailToolDTO: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "voicemail". This uses the model itself to determine if a voicemil was reached. Can be used alternatively/alongside with TwilioVoicemailDetection
             * @enum {string}
             */
            type: "voicemail";
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        CreateFunctionToolDTO: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "function" for Function tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "function";
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        GhlToolMetadata: {
            workflowId?: string;
            locationId?: string;
        };
        CreateGhlToolDTO: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "ghl" for GHL tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "ghl";
            metadata: components["schemas"]["GhlToolMetadata"];
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        MakeToolMetadata: {
            scenarioId?: number;
            triggerHookId?: number;
        };
        CreateMakeToolDTO: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "make" for Make tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "make";
            metadata: components["schemas"]["MakeToolMetadata"];
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        TransferDestinationAssistant: {
            /** @enum {string} */
            type: "assistant";
            /**
             * @description This is the mode to use for the transfer. Default is `rolling-history`.
             *
             *     - `rolling-history`: This is the default mode. It keeps the entire conversation history and appends the new assistant's system message on transfer.
             *
             *       Example:
             *
             *       Pre-transfer:
             *         system: assistant1 system message
             *         assistant: assistant1 first message
             *         user: hey, good morning
             *         assistant: how can i help?
             *         user: i need help with my account
             *         assistant: (destination.message)
             *
             *       Post-transfer:
             *         system: assistant1 system message
             *         assistant: assistant1 first message
             *         user: hey, good morning
             *         assistant: how can i help?
             *         user: i need help with my account
             *         assistant: (destination.message)
             *         system: assistant2 system message
             *         assistant: assistant2 first message (or model generated if firstMessageMode is set to `assistant-speaks-first-with-model-generated-message`)
             *
             *     - `swap-system-message-in-history`: This replaces the original system message with the new assistant's system message on transfer.
             *
             *       Example:
             *
             *       Pre-transfer:
             *         system: assistant1 system message
             *         assistant: assistant1 first message
             *         user: hey, good morning
             *         assistant: how can i help?
             *         user: i need help with my account
             *         assistant: (destination.message)
             *
             *       Post-transfer:
             *         system: assistant2 system message
             *         assistant: assistant1 first message
             *         user: hey, good morning
             *         assistant: how can i help?
             *         user: i need help with my account
             *         assistant: (destination.message)
             *         assistant: assistant2 first message (or model generated if firstMessageMode is set to `assistant-speaks-first-with-model-generated-message`)
             * @enum {string}
             */
            transferMode?: "rolling-history" | "swap-system-message-in-history";
            /** @description This is the assistant to transfer the call to. */
            assistantName: string;
            /** @description This is the message to say before transferring the call to the destination.
             *
             *     If this is not provided and transfer tool messages is not provided, default is "Transferring the call now".
             *
             *     If set to "", nothing is spoken. This is useful when you want to silently transfer. This is especially useful when transferring between assistants in a squad. In this scenario, you likely also want to set `assistant.firstMessageMode=assistant-speaks-first-with-model-generated-message` for the destination assistant. */
            message?: string;
            /** @description This is the description of the destination, used by the AI to choose when and how to transfer the call. */
            description?: string;
        };
        TransferDestinationStep: {
            /** @enum {string} */
            type: "step";
            /** @description This is the step to transfer to. */
            stepName: string;
            /** @description This is the message to say before transferring the call to the destination.
             *
             *     If this is not provided and transfer tool messages is not provided, default is "Transferring the call now".
             *
             *     If set to "", nothing is spoken. This is useful when you want to silently transfer. This is especially useful when transferring between assistants in a squad. In this scenario, you likely also want to set `assistant.firstMessageMode=assistant-speaks-first-with-model-generated-message` for the destination assistant. */
            message?: string;
            /** @description This is the description of the destination, used by the AI to choose when and how to transfer the call. */
            description?: string;
        };
        TransferDestinationNumber: {
            /** @enum {string} */
            type: "number";
            /**
             * @description This is the flag to toggle the E164 check for the `number` field. This is an advanced property which should be used if you know your use case requires it.
             *
             *     Use cases:
             *     - `false`: To allow non-E164 numbers like `+001234567890`, `1234', or `abc`. This is useful for dialing out to non-E164 numbers on your SIP trunks.
             *     - `true` (default): To allow only E164 numbers like `+14155551234`. This is for most standard PSTN calls.
             *
             *     If `false`, the `number` is still required to only contain alphanumeric characters (regex: `/^\+?[a-zA-Z0-9]+$/`).
             *
             *     @default true (E164 check is enabled)
             * @default true
             */
            numberE164CheckEnabled: boolean;
            /** @description This is the phone number to transfer the call to. */
            number: string;
            /** @description This is the extension to dial after transferring the call to the `number`. */
            extension?: string;
            /** @description This is the message to say before transferring the call to the destination.
             *
             *     If this is not provided and transfer tool messages is not provided, default is "Transferring the call now".
             *
             *     If set to "", nothing is spoken. This is useful when you want to silently transfer. This is especially useful when transferring between assistants in a squad. In this scenario, you likely also want to set `assistant.firstMessageMode=assistant-speaks-first-with-model-generated-message` for the destination assistant. */
            message?: string;
            /** @description This is the description of the destination, used by the AI to choose when and how to transfer the call. */
            description?: string;
        };
        TransferDestinationSip: {
            /** @enum {string} */
            type: "sip";
            /** @description This is the SIP URI to transfer the call to. */
            sipUri: string;
            /** @description This is the message to say before transferring the call to the destination.
             *
             *     If this is not provided and transfer tool messages is not provided, default is "Transferring the call now".
             *
             *     If set to "", nothing is spoken. This is useful when you want to silently transfer. This is especially useful when transferring between assistants in a squad. In this scenario, you likely also want to set `assistant.firstMessageMode=assistant-speaks-first-with-model-generated-message` for the destination assistant. */
            message?: string;
            /** @description This is the description of the destination, used by the AI to choose when and how to transfer the call. */
            description?: string;
        };
        CreateTransferCallToolDTO: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description discriminator enum property added by openapi-typescript
             * @enum {string}
             */
            type: "transferCall";
            /** @description These are the destinations that the call can be transferred to. If no destinations are provided, server.url will be used to get the transfer destination once the tool is called. */
            destinations?: (components["schemas"]["TransferDestinationAssistant"] | components["schemas"]["TransferDestinationStep"] | components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"])[];
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        OpenAIMessage: {
            content: string | null;
            /** @enum {string} */
            role: "assistant" | "function" | "user" | "system" | "tool";
        };
        KnowledgeBase: {
            /** @enum {string} */
            provider: "canonical";
            topK?: number;
            fileIds: string[];
        };
        AnyscaleModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            /** @enum {string} */
            provider: "anyscale";
            /** @description This is the name of the model. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
            model: string;
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        AnthropicModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            /**
             * @description This is the Anthropic/Claude models that will be used.
             * @enum {string}
             */
            model: "claude-3-opus-20240229" | "claude-3-sonnet-20240229" | "claude-3-haiku-20240307" | "claude-3-5-sonnet-20240620";
            /** @enum {string} */
            provider: "anthropic";
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        CustomLLMModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            /**
             * @description This is the provider that will be used for the model. Any service, including your own server, that is compatible with the OpenAI API can be used.
             * @enum {string}
             */
            provider: "custom-llm";
            /**
             * @description This determines whether metadata is sent in requests to the custom provider.
             *
             *     - `off` will not send any metadata. payload will look like `{ messages }`
             *     - `variable` will send `assistant.metadata` as a variable on the payload. payload will look like `{ messages, metadata }`
             *     - `destructured` will send `assistant.metadata` fields directly on the payload. payload will look like `{ messages, ...metadata }`
             *
             *     Further, `variable` and `destructured` will send `call`, `phoneNumber`, and `customer` objects in the payload.
             *
             *     Default is `variable`.
             * @enum {string}
             */
            metadataSendMode?: "off" | "variable" | "destructured";
            /** @description These is the URL we'll use for the OpenAI client's `baseURL`. Ex. https://openrouter.ai/api/v1 */
            url: string;
            /** @description This is the name of the model. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
            model: string;
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        DeepInfraModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            /** @enum {string} */
            provider: "deepinfra";
            /** @description This is the name of the model. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
            model: string;
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        GroqModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            /**
             * @description This is the name of the model. Ex. cognitivecomputations/dolphin-mixtral-8x7b
             * @enum {string}
             */
            model: "llama-3.1-405b-reasoning" | "llama-3.1-70b-versatile" | "llama-3.1-8b-instant" | "mixtral-8x7b-32768" | "llama3-8b-8192" | "llama3-70b-8192" | "llama3-groq-8b-8192-tool-use-preview" | "llama3-groq-70b-8192-tool-use-preview" | "gemma-7b-it" | "gemma2-9b-it";
            /** @enum {string} */
            provider: "groq";
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        OpenAIModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            /**
             * @description This is the provider that will be used for the model.
             * @enum {string}
             */
            provider: "openai";
            /**
             * @description This is the OpenAI model that will be used.
             * @enum {string}
             */
            model: "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4o" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-turbo-preview" | "gpt-4-0125-preview" | "gpt-4-1106-preview" | "gpt-4" | "gpt-4-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0613";
            /**
             * @description These are the fallback models that will be used if the primary model fails. This shouldn't be specified unless you have a specific reason to do so. Vapi will automatically find the fastest fallbacks that make sense.
             * @example [
             *       "gpt-4-0125-preview",
             *       "gpt-4-0613"
             *     ]
             * @enum {string}
             */
            fallbackModels?: "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4o" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-turbo-preview" | "gpt-4-0125-preview" | "gpt-4-1106-preview" | "gpt-4" | "gpt-4-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0613";
            /** @example true */
            semanticCachingEnabled?: boolean;
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        OpenRouterModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            /** @enum {string} */
            provider: "openrouter";
            /** @description This is the name of the model. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
            model: string;
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        PerplexityAIModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            /** @enum {string} */
            provider: "perplexity-ai";
            /** @description This is the name of the model. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
            model: string;
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        TogetherAIModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            /** @enum {string} */
            provider: "together-ai";
            /** @description This is the name of the model. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
            model: string;
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        VapiModel: {
            /** @description This is the starting state for the conversation. */
            messages?: components["schemas"]["OpenAIMessage"][];
            /** @description These are the tools that the assistant can use during the call. To use existing tools, use `toolIds`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            tools?: (components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"])[];
            /** @description These are the tools that the assistant can use during the call. To use transient tools, use `tools`.
             *
             *     Both `tools` and `toolIds` can be used together. */
            toolIds?: string[];
            steps?: (components["schemas"]["HandoffStep"] | components["schemas"]["CallbackStep"])[];
            /** @enum {string} */
            provider: "vapi";
            /** @description This is the name of the model. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
            model: string;
            /** @description This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency. */
            temperature?: number;
            /** @description These are the options for the knowledge base. */
            knowledgeBase?: components["schemas"]["KnowledgeBase"];
            /** @description This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250. */
            maxTokens?: number;
            /** @description This determines whether we detect user's emotion while they speak and send it as an additional info to model.
             *
             *     Default `false` because the model is usually are good at understanding the user's emotion from text.
             *
             *     @default false */
            emotionRecognitionEnabled?: boolean;
            /** @description This sets how many turns at the start of the conversation to use a smaller, faster model from the same provider before switching to the primary model. Example, gpt-3.5-turbo if provider is openai.
             *
             *     Default is 0.
             *
             *     @default 0 */
            numFastTurns?: number;
        };
        AzureVoice: {
            /**
             * @description This determines whether the model output is preprocessed into chunks before being sent to the voice provider.
             *
             *     Default `true` because voice generation sounds better with chunking (and reformatting them).
             *
             *     To send every token from the model output directly to the voice provider and rely on the voice provider's audio generation logic, set this to `false`.
             *
             *     If disabled, vapi-provided audio control tokens like <flush /> will not work.
             * @example true
             */
            inputPreprocessingEnabled?: boolean;
            /**
             * @description This determines whether the chunk is reformatted before being sent to the voice provider. Many things are reformatted including phone numbers, emails and addresses to improve their enunciation.
             *
             *     Default `true` because voice generation sounds better with reformatting.
             *
             *     To disable chunk reformatting, set this to `false`.
             *
             *     To disable chunking completely, set `inputPreprocessingEnabled` to `false`.
             * @example true
             */
            inputReformattingEnabled?: boolean;
            /**
             * @description This is the minimum number of characters before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults to 30.
             *
             *     Increasing this value might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, increasing might be a good idea if you want to give voice provider bigger chunks so it can pronounce them better.
             *
             *     Decreasing this value might decrease latency but might also decrease quality if the voice provider struggles to pronounce the text correctly.
             * @example 30
             */
            inputMinCharacters?: number;
            /**
             * @description These are the punctuations that are considered valid boundaries before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults are chosen differently for each provider.
             *
             *     Constraining the delimiters might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, constraining might be a good idea if you want to give voice provider longer chunks so it can sound less disjointed across chunks. Eg. ['.'].
             * @example [
             *       "。",
             *       "，",
             *       ".",
             *       "!",
             *       "?",
             *       ";",
             *       "،",
             *       "۔",
             *       "।",
             *       "॥",
             *       "|",
             *       "||",
             *       ",",
             *       ":"
             *     ]
             * @enum {string}
             */
            inputPunctuationBoundaries?: "。" | "，" | "." | "!" | "?" | ";" | ")" | "،" | "۔" | "।" | "॥" | "|" | "||" | "," | ":";
            /**
             * @description This determines whether fillers are injected into the model output before inputting it into the voice provider.
             *
             *     Default `false` because you can achieve better results with prompting the model.
             * @example false
             */
            fillerInjectionEnabled?: boolean;
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider: "azure";
            /** @description This is the provider-specific ID that will be used. */
            voiceId: ("andrew" | "brian" | "emma") | string;
            /** @description This is the speed multiplier that will be used. */
            speed?: number;
        };
        CartesiaVoice: {
            /**
             * @description This determines whether the model output is preprocessed into chunks before being sent to the voice provider.
             *
             *     Default `true` because voice generation sounds better with chunking (and reformatting them).
             *
             *     To send every token from the model output directly to the voice provider and rely on the voice provider's audio generation logic, set this to `false`.
             *
             *     If disabled, vapi-provided audio control tokens like <flush /> will not work.
             * @example true
             */
            inputPreprocessingEnabled?: boolean;
            /**
             * @description This determines whether the chunk is reformatted before being sent to the voice provider. Many things are reformatted including phone numbers, emails and addresses to improve their enunciation.
             *
             *     Default `true` because voice generation sounds better with reformatting.
             *
             *     To disable chunk reformatting, set this to `false`.
             *
             *     To disable chunking completely, set `inputPreprocessingEnabled` to `false`.
             * @example true
             */
            inputReformattingEnabled?: boolean;
            /**
             * @description This is the minimum number of characters before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults to 30.
             *
             *     Increasing this value might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, increasing might be a good idea if you want to give voice provider bigger chunks so it can pronounce them better.
             *
             *     Decreasing this value might decrease latency but might also decrease quality if the voice provider struggles to pronounce the text correctly.
             * @example 30
             */
            inputMinCharacters?: number;
            /**
             * @description These are the punctuations that are considered valid boundaries before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults are chosen differently for each provider.
             *
             *     Constraining the delimiters might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, constraining might be a good idea if you want to give voice provider longer chunks so it can sound less disjointed across chunks. Eg. ['.'].
             * @example [
             *       "。",
             *       "，",
             *       ".",
             *       "!",
             *       "?",
             *       ";",
             *       "،",
             *       "۔",
             *       "।",
             *       "॥",
             *       "|",
             *       "||",
             *       ",",
             *       ":"
             *     ]
             * @enum {string}
             */
            inputPunctuationBoundaries?: "。" | "，" | "." | "!" | "?" | ";" | ")" | "،" | "۔" | "।" | "॥" | "|" | "||" | "," | ":";
            /**
             * @description This determines whether fillers are injected into the model output before inputting it into the voice provider.
             *
             *     Default `false` because you can achieve better results with prompting the model.
             * @example false
             */
            fillerInjectionEnabled?: boolean;
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider: "cartesia";
            /**
             * @description This is the model that will be used. This is optional and will default to the correct model for the voiceId.
             * @example sonic-english
             * @enum {string}
             */
            model?: "sonic-english" | "sonic-multilingual";
            /**
             * @description This is the language that will be used. This is optional and will default to the correct language for the voiceId.
             * @example en
             * @enum {string}
             */
            language?: "de" | "en" | "es" | "fr" | "ja" | "pt" | "zh";
            /** @description This is the provider-specific ID that will be used. */
            voiceId: string;
        };
        DeepgramVoice: {
            /**
             * @description This determines whether the model output is preprocessed into chunks before being sent to the voice provider.
             *
             *     Default `true` because voice generation sounds better with chunking (and reformatting them).
             *
             *     To send every token from the model output directly to the voice provider and rely on the voice provider's audio generation logic, set this to `false`.
             *
             *     If disabled, vapi-provided audio control tokens like <flush /> will not work.
             * @example true
             */
            inputPreprocessingEnabled?: boolean;
            /**
             * @description This determines whether the chunk is reformatted before being sent to the voice provider. Many things are reformatted including phone numbers, emails and addresses to improve their enunciation.
             *
             *     Default `true` because voice generation sounds better with reformatting.
             *
             *     To disable chunk reformatting, set this to `false`.
             *
             *     To disable chunking completely, set `inputPreprocessingEnabled` to `false`.
             * @example true
             */
            inputReformattingEnabled?: boolean;
            /**
             * @description This is the minimum number of characters before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults to 30.
             *
             *     Increasing this value might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, increasing might be a good idea if you want to give voice provider bigger chunks so it can pronounce them better.
             *
             *     Decreasing this value might decrease latency but might also decrease quality if the voice provider struggles to pronounce the text correctly.
             * @example 30
             */
            inputMinCharacters?: number;
            /**
             * @description These are the punctuations that are considered valid boundaries before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults are chosen differently for each provider.
             *
             *     Constraining the delimiters might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, constraining might be a good idea if you want to give voice provider longer chunks so it can sound less disjointed across chunks. Eg. ['.'].
             * @example [
             *       "。",
             *       "，",
             *       ".",
             *       "!",
             *       "?",
             *       ";",
             *       "،",
             *       "۔",
             *       "।",
             *       "॥",
             *       "|",
             *       "||",
             *       ",",
             *       ":"
             *     ]
             * @enum {string}
             */
            inputPunctuationBoundaries?: "。" | "，" | "." | "!" | "?" | ";" | ")" | "،" | "۔" | "।" | "॥" | "|" | "||" | "," | ":";
            /**
             * @description This determines whether fillers are injected into the model output before inputting it into the voice provider.
             *
             *     Default `false` because you can achieve better results with prompting the model.
             * @example false
             */
            fillerInjectionEnabled?: boolean;
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider: "deepgram";
            /** @description This is the provider-specific ID that will be used. */
            voiceId: ("asteria" | "luna" | "stella" | "athena" | "hera" | "orion" | "arcas" | "perseus" | "angus" | "orpheus" | "helios" | "zeus") | string;
        };
        ElevenLabsVoice: {
            /**
             * @description This determines whether the model output is preprocessed into chunks before being sent to the voice provider.
             *
             *     Default `true` because voice generation sounds better with chunking (and reformatting them).
             *
             *     To send every token from the model output directly to the voice provider and rely on the voice provider's audio generation logic, set this to `false`.
             *
             *     If disabled, vapi-provided audio control tokens like <flush /> will not work.
             * @example true
             */
            inputPreprocessingEnabled?: boolean;
            /**
             * @description This determines whether the chunk is reformatted before being sent to the voice provider. Many things are reformatted including phone numbers, emails and addresses to improve their enunciation.
             *
             *     Default `true` because voice generation sounds better with reformatting.
             *
             *     To disable chunk reformatting, set this to `false`.
             *
             *     To disable chunking completely, set `inputPreprocessingEnabled` to `false`.
             * @example true
             */
            inputReformattingEnabled?: boolean;
            /**
             * @description This is the minimum number of characters before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults to 30.
             *
             *     Increasing this value might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, increasing might be a good idea if you want to give voice provider bigger chunks so it can pronounce them better.
             *
             *     Decreasing this value might decrease latency but might also decrease quality if the voice provider struggles to pronounce the text correctly.
             * @example 30
             */
            inputMinCharacters?: number;
            /**
             * @description These are the punctuations that are considered valid boundaries before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults are chosen differently for each provider.
             *
             *     Constraining the delimiters might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, constraining might be a good idea if you want to give voice provider longer chunks so it can sound less disjointed across chunks. Eg. ['.'].
             * @example [
             *       "。",
             *       "，",
             *       ".",
             *       "!",
             *       "?",
             *       ";",
             *       "،",
             *       "۔",
             *       "।",
             *       "॥",
             *       "|",
             *       "||",
             *       ",",
             *       ":"
             *     ]
             * @enum {string}
             */
            inputPunctuationBoundaries?: "。" | "，" | "." | "!" | "?" | ";" | ")" | "،" | "۔" | "।" | "॥" | "|" | "||" | "," | ":";
            /**
             * @description This determines whether fillers are injected into the model output before inputting it into the voice provider.
             *
             *     Default `false` because you can achieve better results with prompting the model.
             * @example false
             */
            fillerInjectionEnabled?: boolean;
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider: "11labs";
            /** @description This is the provider-specific ID that will be used. Ensure the Voice is present in your 11Labs Voice Library. */
            voiceId: ("burt" | "marissa" | "andrea" | "sarah" | "phillip" | "steve" | "joseph" | "myra" | "paula" | "ryan" | "drew" | "paul" | "mrb" | "matilda" | "mark") | string;
            /**
             * @description Defines the stability for voice settings.
             * @example 0.5
             */
            stability?: number;
            /**
             * @description Defines the similarity boost for voice settings.
             * @example 0.75
             */
            similarityBoost?: number;
            /**
             * @description Defines the style for voice settings.
             * @example 0
             */
            style?: number;
            /**
             * @description Defines the use speaker boost for voice settings.
             * @example false
             */
            useSpeakerBoost?: boolean;
            /**
             * @description Defines the optimize streaming latency for voice settings. Defaults to 3.
             * @example 3
             */
            optimizeStreamingLatency?: number;
            /**
             * @description Defines the use of https://elevenlabs.io/docs/speech-synthesis/prompting#pronunciation. Disabled by default.
             * @example false
             */
            enableSsmlParsing?: boolean;
            /**
             * @description This is the model that will be used. Defaults to 'eleven_turbo_v2' if not specified.
             * @example eleven_turbo_v2_5
             * @enum {string}
             */
            model?: "eleven_multilingual_v2" | "eleven_turbo_v2" | "eleven_turbo_v2_5" | "eleven_monolingual_v1";
            /**
             * @description This is the duration to pause between digits when the assistant is speaking a phone number, in seconds.
             *
             *     This uses `<break time="${phoneNumberDigitPauseSeconds}s" />` tag from ElevenLabs.
             *
             *     Defaults to 0s because the model sometimes hallucinates when using the tag.
             * @example 0
             */
            phoneNumberDigitPauseSeconds?: number;
        };
        LMNTVoice: {
            /**
             * @description This determines whether the model output is preprocessed into chunks before being sent to the voice provider.
             *
             *     Default `true` because voice generation sounds better with chunking (and reformatting them).
             *
             *     To send every token from the model output directly to the voice provider and rely on the voice provider's audio generation logic, set this to `false`.
             *
             *     If disabled, vapi-provided audio control tokens like <flush /> will not work.
             * @example true
             */
            inputPreprocessingEnabled?: boolean;
            /**
             * @description This determines whether the chunk is reformatted before being sent to the voice provider. Many things are reformatted including phone numbers, emails and addresses to improve their enunciation.
             *
             *     Default `true` because voice generation sounds better with reformatting.
             *
             *     To disable chunk reformatting, set this to `false`.
             *
             *     To disable chunking completely, set `inputPreprocessingEnabled` to `false`.
             * @example true
             */
            inputReformattingEnabled?: boolean;
            /**
             * @description This is the minimum number of characters before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults to 30.
             *
             *     Increasing this value might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, increasing might be a good idea if you want to give voice provider bigger chunks so it can pronounce them better.
             *
             *     Decreasing this value might decrease latency but might also decrease quality if the voice provider struggles to pronounce the text correctly.
             * @example 30
             */
            inputMinCharacters?: number;
            /**
             * @description These are the punctuations that are considered valid boundaries before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults are chosen differently for each provider.
             *
             *     Constraining the delimiters might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, constraining might be a good idea if you want to give voice provider longer chunks so it can sound less disjointed across chunks. Eg. ['.'].
             * @example [
             *       "。",
             *       "，",
             *       ".",
             *       "!",
             *       "?",
             *       ";",
             *       "،",
             *       "۔",
             *       "।",
             *       "॥",
             *       "|",
             *       "||",
             *       ",",
             *       ":"
             *     ]
             * @enum {string}
             */
            inputPunctuationBoundaries?: "。" | "，" | "." | "!" | "?" | ";" | ")" | "،" | "۔" | "।" | "॥" | "|" | "||" | "," | ":";
            /**
             * @description This determines whether fillers are injected into the model output before inputting it into the voice provider.
             *
             *     Default `false` because you can achieve better results with prompting the model.
             * @example false
             */
            fillerInjectionEnabled?: boolean;
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider: "lmnt";
            /** @description This is the provider-specific ID that will be used. */
            voiceId: ("lily" | "daniel") | string;
            /**
             * @description This is the speed multiplier that will be used.
             * @example null
             */
            speed?: number;
        };
        NeetsVoice: {
            /**
             * @description This determines whether the model output is preprocessed into chunks before being sent to the voice provider.
             *
             *     Default `true` because voice generation sounds better with chunking (and reformatting them).
             *
             *     To send every token from the model output directly to the voice provider and rely on the voice provider's audio generation logic, set this to `false`.
             *
             *     If disabled, vapi-provided audio control tokens like <flush /> will not work.
             * @example true
             */
            inputPreprocessingEnabled?: boolean;
            /**
             * @description This determines whether the chunk is reformatted before being sent to the voice provider. Many things are reformatted including phone numbers, emails and addresses to improve their enunciation.
             *
             *     Default `true` because voice generation sounds better with reformatting.
             *
             *     To disable chunk reformatting, set this to `false`.
             *
             *     To disable chunking completely, set `inputPreprocessingEnabled` to `false`.
             * @example true
             */
            inputReformattingEnabled?: boolean;
            /**
             * @description This is the minimum number of characters before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults to 30.
             *
             *     Increasing this value might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, increasing might be a good idea if you want to give voice provider bigger chunks so it can pronounce them better.
             *
             *     Decreasing this value might decrease latency but might also decrease quality if the voice provider struggles to pronounce the text correctly.
             * @example 30
             */
            inputMinCharacters?: number;
            /**
             * @description These are the punctuations that are considered valid boundaries before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults are chosen differently for each provider.
             *
             *     Constraining the delimiters might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, constraining might be a good idea if you want to give voice provider longer chunks so it can sound less disjointed across chunks. Eg. ['.'].
             * @example [
             *       "。",
             *       "，",
             *       ".",
             *       "!",
             *       "?",
             *       ";",
             *       "،",
             *       "۔",
             *       "।",
             *       "॥",
             *       "|",
             *       "||",
             *       ",",
             *       ":"
             *     ]
             * @enum {string}
             */
            inputPunctuationBoundaries?: "。" | "，" | "." | "!" | "?" | ";" | ")" | "،" | "۔" | "।" | "॥" | "|" | "||" | "," | ":";
            /**
             * @description This determines whether fillers are injected into the model output before inputting it into the voice provider.
             *
             *     Default `false` because you can achieve better results with prompting the model.
             * @example false
             */
            fillerInjectionEnabled?: boolean;
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider: "neets";
            /** @description This is the provider-specific ID that will be used. */
            voiceId: ("vits" | "vits") | string;
        };
        OpenAIVoice: {
            /**
             * @description This determines whether the model output is preprocessed into chunks before being sent to the voice provider.
             *
             *     Default `true` because voice generation sounds better with chunking (and reformatting them).
             *
             *     To send every token from the model output directly to the voice provider and rely on the voice provider's audio generation logic, set this to `false`.
             *
             *     If disabled, vapi-provided audio control tokens like <flush /> will not work.
             * @example true
             */
            inputPreprocessingEnabled?: boolean;
            /**
             * @description This determines whether the chunk is reformatted before being sent to the voice provider. Many things are reformatted including phone numbers, emails and addresses to improve their enunciation.
             *
             *     Default `true` because voice generation sounds better with reformatting.
             *
             *     To disable chunk reformatting, set this to `false`.
             *
             *     To disable chunking completely, set `inputPreprocessingEnabled` to `false`.
             * @example true
             */
            inputReformattingEnabled?: boolean;
            /**
             * @description This is the minimum number of characters before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults to 30.
             *
             *     Increasing this value might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, increasing might be a good idea if you want to give voice provider bigger chunks so it can pronounce them better.
             *
             *     Decreasing this value might decrease latency but might also decrease quality if the voice provider struggles to pronounce the text correctly.
             * @example 30
             */
            inputMinCharacters?: number;
            /**
             * @description These are the punctuations that are considered valid boundaries before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults are chosen differently for each provider.
             *
             *     Constraining the delimiters might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, constraining might be a good idea if you want to give voice provider longer chunks so it can sound less disjointed across chunks. Eg. ['.'].
             * @example [
             *       "。",
             *       "，",
             *       ".",
             *       "!",
             *       "?",
             *       ";",
             *       "،",
             *       "۔",
             *       "।",
             *       "॥",
             *       "|",
             *       "||",
             *       ",",
             *       ":"
             *     ]
             * @enum {string}
             */
            inputPunctuationBoundaries?: "。" | "，" | "." | "!" | "?" | ";" | ")" | "،" | "۔" | "।" | "॥" | "|" | "||" | "," | ":";
            /**
             * @description This determines whether fillers are injected into the model output before inputting it into the voice provider.
             *
             *     Default `false` because you can achieve better results with prompting the model.
             * @example false
             */
            fillerInjectionEnabled?: boolean;
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider: "openai";
            /**
             * @description This is the provider-specific ID that will be used.
             * @enum {string}
             */
            voiceId: "alloy" | "echo" | "fable" | "onyx" | "nova" | "shimmer";
            /**
             * @description This is the speed multiplier that will be used.
             * @example null
             */
            speed?: number;
        };
        PlayHTVoice: {
            /**
             * @description This determines whether the model output is preprocessed into chunks before being sent to the voice provider.
             *
             *     Default `true` because voice generation sounds better with chunking (and reformatting them).
             *
             *     To send every token from the model output directly to the voice provider and rely on the voice provider's audio generation logic, set this to `false`.
             *
             *     If disabled, vapi-provided audio control tokens like <flush /> will not work.
             * @example true
             */
            inputPreprocessingEnabled?: boolean;
            /**
             * @description This determines whether the chunk is reformatted before being sent to the voice provider. Many things are reformatted including phone numbers, emails and addresses to improve their enunciation.
             *
             *     Default `true` because voice generation sounds better with reformatting.
             *
             *     To disable chunk reformatting, set this to `false`.
             *
             *     To disable chunking completely, set `inputPreprocessingEnabled` to `false`.
             * @example true
             */
            inputReformattingEnabled?: boolean;
            /**
             * @description This is the minimum number of characters before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults to 30.
             *
             *     Increasing this value might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, increasing might be a good idea if you want to give voice provider bigger chunks so it can pronounce them better.
             *
             *     Decreasing this value might decrease latency but might also decrease quality if the voice provider struggles to pronounce the text correctly.
             * @example 30
             */
            inputMinCharacters?: number;
            /**
             * @description These are the punctuations that are considered valid boundaries before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults are chosen differently for each provider.
             *
             *     Constraining the delimiters might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, constraining might be a good idea if you want to give voice provider longer chunks so it can sound less disjointed across chunks. Eg. ['.'].
             * @example [
             *       "。",
             *       "，",
             *       ".",
             *       "!",
             *       "?",
             *       ";",
             *       "،",
             *       "۔",
             *       "।",
             *       "॥",
             *       "|",
             *       "||",
             *       ",",
             *       ":"
             *     ]
             * @enum {string}
             */
            inputPunctuationBoundaries?: "。" | "，" | "." | "!" | "?" | ";" | ")" | "،" | "۔" | "।" | "॥" | "|" | "||" | "," | ":";
            /**
             * @description This determines whether fillers are injected into the model output before inputting it into the voice provider.
             *
             *     Default `false` because you can achieve better results with prompting the model.
             * @example false
             */
            fillerInjectionEnabled?: boolean;
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider: "playht";
            /** @description This is the provider-specific ID that will be used. */
            voiceId: ("jennifer" | "melissa" | "will" | "chris" | "matt" | "jack" | "ruby" | "davis" | "donna" | "michael") | string;
            /**
             * @description This is the speed multiplier that will be used.
             * @example null
             */
            speed?: number;
            /**
             * @description A floating point number between 0, exclusive, and 2, inclusive. If equal to null or not provided, the model's default temperature will be used. The temperature parameter controls variance. Lower temperatures result in more predictable results, higher temperatures allow each run to vary more, so the voice may sound less like the baseline voice.
             * @example null
             */
            temperature?: number;
            /**
             * @description An emotion to be applied to the speech.
             * @example null
             * @enum {string}
             */
            emotion?: "female_happy" | "female_sad" | "female_angry" | "female_fearful" | "female_disgust" | "female_surprised" | "male_happy" | "male_sad" | "male_angry" | "male_fearful" | "male_disgust" | "male_surprised";
            /**
             * @description A number between 1 and 6. Use lower numbers to reduce how unique your chosen voice will be compared to other voices.
             * @example null
             */
            voiceGuidance?: number;
            /**
             * @description A number between 1 and 30. Use lower numbers to to reduce how strong your chosen emotion will be. Higher numbers will create a very emotional performance.
             * @example null
             */
            styleGuidance?: number;
            /**
             * @description A number between 1 and 2. This number influences how closely the generated speech adheres to the input text. Use lower values to create more fluid speech, but with a higher chance of deviating from the input text. Higher numbers will make the generated speech more accurate to the input text, ensuring that the words spoken align closely with the provided text.
             * @example null
             */
            textGuidance?: number;
        };
        RimeAIVoice: {
            /**
             * @description This determines whether the model output is preprocessed into chunks before being sent to the voice provider.
             *
             *     Default `true` because voice generation sounds better with chunking (and reformatting them).
             *
             *     To send every token from the model output directly to the voice provider and rely on the voice provider's audio generation logic, set this to `false`.
             *
             *     If disabled, vapi-provided audio control tokens like <flush /> will not work.
             * @example true
             */
            inputPreprocessingEnabled?: boolean;
            /**
             * @description This determines whether the chunk is reformatted before being sent to the voice provider. Many things are reformatted including phone numbers, emails and addresses to improve their enunciation.
             *
             *     Default `true` because voice generation sounds better with reformatting.
             *
             *     To disable chunk reformatting, set this to `false`.
             *
             *     To disable chunking completely, set `inputPreprocessingEnabled` to `false`.
             * @example true
             */
            inputReformattingEnabled?: boolean;
            /**
             * @description This is the minimum number of characters before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults to 30.
             *
             *     Increasing this value might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, increasing might be a good idea if you want to give voice provider bigger chunks so it can pronounce them better.
             *
             *     Decreasing this value might decrease latency but might also decrease quality if the voice provider struggles to pronounce the text correctly.
             * @example 30
             */
            inputMinCharacters?: number;
            /**
             * @description These are the punctuations that are considered valid boundaries before a chunk is created. The chunks that are sent to the voice provider for the voice generation as the model tokens are streaming in. Defaults are chosen differently for each provider.
             *
             *     Constraining the delimiters might add latency as it waits for the model to output a full chunk before sending it to the voice provider. On the other hand, constraining might be a good idea if you want to give voice provider longer chunks so it can sound less disjointed across chunks. Eg. ['.'].
             * @example [
             *       "。",
             *       "，",
             *       ".",
             *       "!",
             *       "?",
             *       ";",
             *       "،",
             *       "۔",
             *       "।",
             *       "॥",
             *       "|",
             *       "||",
             *       ",",
             *       ":"
             *     ]
             * @enum {string}
             */
            inputPunctuationBoundaries?: "。" | "，" | "." | "!" | "?" | ";" | ")" | "،" | "۔" | "।" | "॥" | "|" | "||" | "," | ":";
            /**
             * @description This determines whether fillers are injected into the model output before inputting it into the voice provider.
             *
             *     Default `false` because you can achieve better results with prompting the model.
             * @example false
             */
            fillerInjectionEnabled?: boolean;
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider: "rime-ai";
            /** @description This is the provider-specific ID that will be used. */
            voiceId: ("marsh" | "bayou" | "creek" | "brook" | "flower" | "spore" | "glacier" | "gulch" | "alpine" | "cove" | "lagoon" | "tundra" | "steppe" | "mesa" | "grove" | "rainforest" | "moraine" | "wildflower" | "peak" | "boulder" | "abbie" | "allison" | "ally" | "alona" | "amber" | "ana" | "antoine" | "armon" | "brenda" | "brittany" | "carol" | "colin" | "courtney" | "elena" | "elliot" | "eva" | "geoff" | "gerald" | "hank" | "helen" | "hera" | "jen" | "joe" | "joy" | "juan" | "kendra" | "kendrick" | "kenneth" | "kevin" | "kris" | "linda" | "madison" | "marge" | "marina" | "marissa" | "marta" | "maya" | "nicholas" | "nyles" | "phil" | "reba" | "rex" | "rick" | "ritu" | "rob" | "rodney" | "rohan" | "rosco" | "samantha" | "sandy" | "selena" | "seth" | "sharon" | "stan" | "tamra" | "tanya" | "tibur" | "tj" | "tyler" | "viv" | "yadira") | string;
            /**
             * @description This is the model that will be used. Defaults to 'v1' when not specified.
             * @example v1
             * @enum {string}
             */
            model?: "v1" | "mist";
            /**
             * @description This is the speed multiplier that will be used.
             * @example null
             */
            speed?: number;
        };
        TransportConfigurationTwilio: {
            /** @enum {string} */
            provider: "twilio";
            /**
             * @description The integer number of seconds that we should allow the phone to ring before assuming there is no answer.
             *     The default is `60` seconds and the maximum is `600` seconds.
             *     For some call flows, we will add a 5-second buffer to the timeout value you provide.
             *     For this reason, a timeout value of 10 seconds could result in an actual timeout closer to 15 seconds.
             *     You can set this to a short time, such as `15` seconds, to hang up before reaching an answering machine or voicemail.
             *
             *     @default 60
             * @example 60
             */
            timeout?: number;
            /**
             * @description Whether to record the call.
             *     Can be `true` to record the phone call, or `false` to not.
             *     The default is `false`.
             *
             *     @default false
             * @example false
             */
            record?: boolean;
            /**
             * @description The number of channels in the final recording.
             *     Can be: `mono` or `dual`.
             *     The default is `mono`.
             *     `mono` records both legs of the call in a single channel of the recording file.
             *     `dual` records each leg to a separate channel of the recording file.
             *     The first channel of a dual-channel recording contains the parent call and the second channel contains the child call.
             *
             *     @default 'mono'
             * @example mono
             * @enum {string}
             */
            recordingChannels?: "mono" | "dual";
        };
        TwilioVoicemailDetection: {
            /**
             * @description This is the provider to use for voicemail detection.
             * @enum {string}
             */
            provider: "twilio";
            /**
             * @description These are the AMD messages from Twilio that are considered as voicemail. Default is ['machine_end_beep', 'machine_end_silence'].
             *
             *     @default {Array} ['machine_end_beep', 'machine_end_silence']
             * @example [
             *       "machine_end_beep",
             *       "machine_end_silence"
             *     ]
             * @enum {string}
             */
            voicemailDetectionTypes?: "machine_start" | "human" | "fax" | "unknown" | "machine_end_beep" | "machine_end_silence" | "machine_end_other";
            /** @description This sets whether the assistant should detect voicemail. Defaults to true.
             *
             *     @default true */
            enabled?: boolean;
            /** @description The number of seconds that Twilio should attempt to perform answering machine detection before timing out and returning AnsweredBy as unknown. Default is 30 seconds.
             *
             *     Increasing this value will provide the engine more time to make a determination. This can be useful when DetectMessageEnd is provided in the MachineDetection parameter and there is an expectation of long answering machine greetings that can exceed 30 seconds.
             *
             *     Decreasing this value will reduce the amount of time the engine has to make a determination. This can be particularly useful when the Enable option is provided in the MachineDetection parameter and you want to limit the time for initial detection.
             *
             *     Check the [Twilio docs](https://www.twilio.com/docs/voice/answering-machine-detection#optional-api-tuning-parameters) for more info.
             *
             *     @default 30 */
            machineDetectionTimeout?: number;
            /** @description The number of milliseconds that is used as the measuring stick for the length of the speech activity. Durations lower than this value will be interpreted as a human, longer as a machine. Default is 2400 milliseconds.
             *
             *     Increasing this value will reduce the chance of a False Machine (detected machine, actually human) for a long human greeting (e.g., a business greeting) but increase the time it takes to detect a machine.
             *
             *     Decreasing this value will reduce the chances of a False Human (detected human, actually machine) for short voicemail greetings. The value of this parameter may need to be reduced by more than 1000ms to detect very short voicemail greetings. A reduction of that significance can result in increased False Machine detections. Adjusting the MachineDetectionSpeechEndThreshold is likely the better approach for short voicemails. Decreasing MachineDetectionSpeechThreshold will also reduce the time it takes to detect a machine.
             *
             *     Check the [Twilio docs](https://www.twilio.com/docs/voice/answering-machine-detection#optional-api-tuning-parameters) for more info.
             *
             *     @default 2400 */
            machineDetectionSpeechThreshold?: number;
            /** @description The number of milliseconds of silence after speech activity at which point the speech activity is considered complete. Default is 1200 milliseconds.
             *
             *     Increasing this value will typically be used to better address the short voicemail greeting scenarios. For short voicemails, there is typically 1000-2000ms of audio followed by 1200-2400ms of silence and then additional audio before the beep. Increasing the MachineDetectionSpeechEndThreshold to ~2500ms will treat the 1200-2400ms of silence as a gap in the greeting but not the end of the greeting and will result in a machine detection. The downsides of such a change include:
             *     - Increasing the delay for human detection by the amount you increase this parameter, e.g., a change of 1200ms to 2500ms increases human detection delay by 1300ms.
             *     - Cases where a human has two utterances separated by a period of silence (e.g. a "Hello", then 2000ms of silence, and another "Hello") may be interpreted as a machine.
             *
             *     Decreasing this value will result in faster human detection. The consequence is that it can lead to increased False Human (detected human, actually machine) detections because a silence gap in a voicemail greeting (not necessarily just in short voicemail scenarios) can be incorrectly interpreted as the end of speech.
             *
             *     Check the [Twilio docs](https://www.twilio.com/docs/voice/answering-machine-detection#optional-api-tuning-parameters) for more info.
             *
             *     @default 1200 */
            machineDetectionSpeechEndThreshold?: number;
            /** @description The number of milliseconds of initial silence after which an unknown AnsweredBy result will be returned. Default is 5000 milliseconds.
             *
             *     Increasing this value will result in waiting for a longer period of initial silence before returning an 'unknown' AMD result.
             *
             *     Decreasing this value will result in waiting for a shorter period of initial silence before returning an 'unknown' AMD result.
             *
             *     Check the [Twilio docs](https://www.twilio.com/docs/voice/answering-machine-detection#optional-api-tuning-parameters) for more info.
             *
             *     @default 5000 */
            machineDetectionSilenceTimeout?: number;
        };
        AnalysisPlan: {
            /** @description This is the prompt that's used to summarize the call. The output is stored in `call.analysis.summary`.
             *
             *     Default is "You are an expert note-taker. You will be given a transcript of a call. Summarize the call in 2-3 sentences. DO NOT return anything except the summary.".
             *
             *     Set to '' or 'off' to disable. */
            summaryPrompt?: string;
            /** @description This is how long the request is tried before giving up. When request times out, `call.analysis.summary` will be empty. Increasing this timeout will delay the end of call report.
             *
             *     Default is 5 seconds. */
            summaryRequestTimeoutSeconds?: number;
            /** @description This is how long the request is tried before giving up. When request times out, `call.analysis.structuredData` will be empty. Increasing this timeout will delay the end of call report.
             *
             *     Default is 5 seconds. */
            structuredDataRequestTimeoutSeconds?: number;
            /** @description This is the prompt that's used to evaluate if the call was successful. The output is stored in `call.analysis.successEvaluation`.
             *
             *     Default is "You are an expert call evaluator. You will be given a transcript of a call and the system prompt of the AI participant. Determine if the call was successful based on the objectives inferred from the system prompt. DO NOT return anything except the result.".
             *
             *     Set to '' or 'off' to disable.
             *
             *     You can use this standalone or in combination with `successEvaluationRubric`. If both are provided, they are concatenated into appropriate instructions. */
            successEvaluationPrompt?: string;
            /**
             * @description This enforces the rubric of the evaluation. The output is stored in `call.analysis.successEvaluation`.
             *
             *     Options include:
             *     - 'NumericScale': A scale of 1 to 10.
             *     - 'DescriptiveScale': A scale of Excellent, Good, Fair, Poor.
             *     - 'Checklist': A checklist of criteria and their status.
             *     - 'Matrix': A grid that evaluates multiple criteria across different performance levels.
             *     - 'PercentageScale': A scale of 0% to 100%.
             *     - 'LikertScale': A scale of Strongly Agree, Agree, Neutral, Disagree, Strongly Disagree.
             *     - 'AutomaticRubric': Automatically break down evaluation into several criteria, each with its own score.
             *     - 'PassFail': A simple 'true' if call passed, 'false' if not.
             *
             *     For 'Checklist' and 'Matrix', provide the criteria in `successEvaluationPrompt`.
             *
             *     Default is 'PassFail' if `successEvaluationPrompt` is not provided, and null if `successEvaluationPrompt` is provided.
             *
             *     You can use this standalone or in combination with `successEvaluationPrompt`. If both are provided, they are concatenated into appropriate instructions.
             * @enum {string}
             */
            successEvaluationRubric?: "NumericScale" | "DescriptiveScale" | "Checklist" | "Matrix" | "PercentageScale" | "LikertScale" | "AutomaticRubric" | "PassFail";
            /** @description This is how long the request is tried before giving up. When request times out, `call.analysis.successEvaluation` will be empty. Increasing this timeout will delay the end of call report.
             *
             *     Default is 5 seconds. */
            successEvaluationRequestTimeoutSeconds?: number;
            /** @description This is the prompt that's used to extract structured data from the call. The output is stored in `call.analysis.structuredData`.
             *
             *     Disabled by default.
             *
             *     You can use this standalone or in combination with `structuredDataSchema`. If both are provided, they are concatenated into appropriate instructions. */
            structuredDataPrompt?: string;
            /** @description This enforces the schema of the structured data. This output is stored in `call.analysis.structuredData`.
             *
             *     Complete guide on JSON Schema can be found [here](https://ajv.js.org/json-schema.html#json-data-type).
             *
             *     Disabled by default.
             *
             *     You can use this standalone or in combination with `structuredDataPrompt`. If both are provided, they are concatenated into appropriate instructions. */
            structuredDataSchema?: components["schemas"]["JsonSchema"];
        };
        ArtifactPlan: {
            /** @description This determines whether the video is recorded during the call. Default is false. Only relevant for `webCall` type. */
            videoRecordingEnabled?: boolean;
            /** @description This is the S3 path prefix for the audio recording. This is only used if you have provided S3 credentials. Check the Providers page in the Dashboard.
             *
             *     If credential.s3PathPrefix is set, this will append to it.
             *
             *     Example: `/my-prefix`. Default is `/`. */
            recordingS3PathPrefix?: string;
        };
        MessagePlan: {
            /** @description This are the messages that the assistant will speak when the user hasn't responded for `idleTimeoutSeconds`. Each time the timeout is triggered, a random message will be chosen from this array.
             *
             *     @default null (no idle message is spoken) */
            idleMessages?: string[];
            /** @description This determines the maximum number of times `idleMessages` can be spoken during the call.
             *
             *     @default 3 */
            idleMessageMaxSpokenCount?: number;
            /** @description This is the timeout in seconds before a message from `idleMessages` is spoken. The clock starts when the assistant finishes speaking and remains active until the user speaks.
             *
             *     @default 10 */
            idleTimeoutSeconds?: number;
        };
        MinWaitBeforeModelRequestConfiguration: {
            /**
             * @description The minimum number of seconds to wait after transcription ending with punctuation before sending a request to the model. Defaults to 0.1.
             *
             *     This setting exists because the transcriber punctuates the transcription when it's more confident that customer has completed a thought.
             *
             *     @default 0.1
             * @example 0.1
             */
            onTranscriptionWithPunctuationSeconds?: number;
            /**
             * @description The minimum number of seconds to wait after transcription ending without punctuation before sending a request to the model. Defaults to 1.5.
             *
             *     This setting exists to catch the cases where the transcriber was not confident enough to punctuate the transcription, but the customer is done and has been silent for a long time.
             *
             *     @default 1.5
             * @example 1.5
             */
            onTranscriptionWithoutPunctuationSeconds?: number;
            /**
             * @description The minimum number of seconds to wait after transcription ending with a number before sending a request to the model. Defaults to 0.4.
             *
             *     This setting exists because the transcriber will sometimes punctuate the transcription ending with a number, even though the customer hasn't uttered the full number. This happens commonly for long numbers when the customer reads the number in chunks.
             *
             *     @default 0.5
             * @example 0.5
             */
            onTranscriptionWithNumberSeconds?: number;
        };
        PipelineConfiguration: {
            /**
             * @description The minimum number of seconds after customer speech before the assistant starts speaking. Defaults to 0.4.
             *
             *     This setting helps avoid the assistant jumping in accidentally when the customer is still speaking but pauses for a moment. If customer is expected to take long pauses, set this to a higher value.
             *
             *     @default 0.4
             * @example 0.4
             */
            minWaitBeforeSpeakingSeconds?: number;
            /**
             * @description The number of words to wait for before interrupting the assistant.
             *
             *     Words like "stop", "actually", "no", etc. will always interrupt immediately regardless of this value.
             *
             *     Words like "okay", "yeah", "right" will never interrupt.
             *
             *     When set to 0, it will rely solely on the VAD (Voice Activity Detector) and will not wait for any transcription. Defaults to this (0).
             *
             *     @default 0
             * @example 0
             */
            numWordsToInterruptAssistant?: number;
            /**
             * @description This determines if the VAP model is to be used to detect endpoints and backchannels.
             *
             *     Default `false` since experimental.
             *     @default false
             * @example false
             */
            smartTurnDetectionEnabled?: boolean;
            /** @description The minimum number of seconds after transcription before sending a request to the model. */
            minWaitBeforeModelRequestConfiguration: components["schemas"]["MinWaitBeforeModelRequestConfiguration"];
        };
        CreateAssistantDTO: {
            /** @description These are the options for the assistant's transcriber. */
            transcriber?: components["schemas"]["DeepgramTranscriber"] | components["schemas"]["GladiaTranscriber"] | components["schemas"]["TalkscriberTranscriber"];
            /** @description These are the options for the assistant's LLM. */
            model?: components["schemas"]["AnyscaleModel"] | components["schemas"]["AnthropicModel"] | components["schemas"]["CustomLLMModel"] | components["schemas"]["DeepInfraModel"] | components["schemas"]["GroqModel"] | components["schemas"]["OpenAIModel"] | components["schemas"]["OpenRouterModel"] | components["schemas"]["PerplexityAIModel"] | components["schemas"]["TogetherAIModel"] | components["schemas"]["VapiModel"];
            /**
             * @description These are the options for the assistant's voice.
             * @default {
             *       "provider": "playht",
             *       "voiceId": "jennifer"
             *     }
             */
            voice: components["schemas"]["AzureVoice"] | components["schemas"]["CartesiaVoice"] | components["schemas"]["DeepgramVoice"] | components["schemas"]["ElevenLabsVoice"] | components["schemas"]["LMNTVoice"] | components["schemas"]["NeetsVoice"] | components["schemas"]["OpenAIVoice"] | components["schemas"]["PlayHTVoice"] | components["schemas"]["RimeAIVoice"];
            /**
             * @description This is the mode for the first message. Default is 'assistant-speaks-first'.
             *
             *     Use:
             *     - 'assistant-speaks-first' to have the assistant speak first.
             *     - 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
             *     - 'assistant-speaks-first-with-model-generated-message' to have the assistant speak first with a message generated by the model based on the conversation state. (`assistant.model.messages` at call start, `call.messages` at squad transfer points).
             *
             *     @default 'assistant-speaks-first'
             * @example assistant-speaks-first
             * @enum {string}
             */
            firstMessageMode?: "assistant-speaks-first" | "assistant-speaks-first-with-model-generated-message" | "assistant-waits-for-user";
            /**
             * @description This sets whether the assistant's calls are recorded. Defaults to true.
             * @example true
             */
            recordingEnabled?: boolean;
            /**
             * @description When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false.
             * @example false
             */
            hipaaEnabled?: boolean;
            /**
             * @description These are the messages that will be sent to your Client SDKs. Default is conversation-update,function-call,hang,model-output,speech-update,status-update,transcript,tool-calls,user-interrupted,voice-input. You can check the shape of the messages in ClientMessage schema.
             * @example [
             *       "conversation-update",
             *       "function-call",
             *       "hang",
             *       "model-output",
             *       "speech-update",
             *       "status-update",
             *       "transcript",
             *       "tool-calls",
             *       "user-interrupted",
             *       "voice-input"
             *     ]
             * @enum {string}
             */
            clientMessages?: "conversation-update" | "function-call" | "function-call-result" | "hang" | "metadata" | "model-output" | "speech-update" | "status-update" | "transcript" | "tool-calls" | "tool-calls-result" | "user-interrupted" | "voice-input";
            /**
             * @description These are the messages that will be sent to your Server URL. Default is conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,user-interrupted. You can check the shape of the messages in ServerMessage schema.
             * @example [
             *       "conversation-update",
             *       "end-of-call-report",
             *       "function-call",
             *       "hang",
             *       "speech-update",
             *       "status-update",
             *       "tool-calls",
             *       "transfer-destination-request",
             *       "user-interrupted"
             *     ]
             * @enum {string}
             */
            serverMessages?: "conversation-update" | "end-of-call-report" | "function-call" | "hang" | "model-output" | "phone-call-control" | "speech-update" | "status-update" | "transcript" | "tool-calls" | "transfer-destination-request" | "transfer-update" | "user-interrupted" | "voice-input";
            /**
             * @description How many seconds of silence to wait before ending the call. Defaults to 30.
             *
             *     @default 30
             * @example 30
             */
            silenceTimeoutSeconds?: number;
            /**
             * @description This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
             *
             *     @default 1800 (~30 minutes)
             * @example 1800
             */
            maxDurationSeconds?: number;
            /**
             * @description This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
             * @example office
             * @enum {string}
             */
            backgroundSound?: "off" | "office";
            /**
             * @description This determines whether the model says 'mhmm', 'ahem' etc. while user is speaking.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            backchannelingEnabled?: boolean;
            /**
             * @description This enables filtering of noise and background speech while the user is talking.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            backgroundDenoisingEnabled?: boolean;
            /**
             * @description This determines whether the model's output is used in conversation history rather than the transcription of assistant's speech.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            modelOutputInMessagesEnabled?: boolean;
            /** @description These are the configurations to be passed to the transport providers of assistant's calls, like Twilio. You can store multiple configurations for different transport providers. For a call, only the configuration matching the call transport provider is used. */
            transportConfigurations?: components["schemas"]["TransportConfigurationTwilio"][];
            /** @description This is the name of the assistant.
             *
             *     This is required when you want to transfer between assistants in a call. */
            name?: string;
            /** @description This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).
             *
             *     If unspecified, assistant will wait for user to speak and use the model to respond once they speak. */
            firstMessage?: string;
            /** @description These are the settings to configure or disable voicemail detection. Alternatively, voicemail detection can be configured using the model.tools=[VoicemailTool].
             *     This uses Twilio's built-in detection while the VoicemailTool relies on the model to detect if a voicemail was reached.
             *     You can use neither of them, one of them, or both of them. By default, Twilio built-in detection is enabled while VoicemailTool is not. */
            voicemailDetection?: components["schemas"]["TwilioVoicemailDetection"];
            /** @description This is the message that the assistant will say if the call is forwarded to voicemail.
             *
             *     If unspecified, it will hang up. */
            voicemailMessage?: string;
            /** @description This is the message that the assistant will say if it ends the call.
             *
             *     If unspecified, it will hang up without saying anything. */
            endCallMessage?: string;
            /** @description This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive. */
            endCallPhrases?: string[];
            /** @description This is for metadata you want to store on the assistant. */
            metadata?: Record<string, never>;
            /** @description This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.
             *
             *     All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl */
            serverUrl?: string;
            /** @description This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
            /** @description This is the plan for analysis of assistant's calls. Stored in `call.analysis`. */
            analysisPlan?: components["schemas"]["AnalysisPlan"];
            /** @description This is the plan for artifacts generated during assistant's calls. Stored in `call.artifact`.
             *
             *     Note: `recordingEnabled` is currently at the root level. It will be moved to `artifactPlan` in the future, but will remain backwards compatible. */
            artifactPlan?: components["schemas"]["ArtifactPlan"];
            /** @description This is the plan for static messages that can be spoken by the assistant during the call, like `idleMessages`.
             *
             *     Note: `firstMessage`, `voicemailMessage`, and `endCallMessage` are currently at the root level. They will be moved to `messagePlan` in the future, but will remain backwards compatible. */
            messagePlan?: components["schemas"]["MessagePlan"];
            /** @description This is the pipeline configuration for assistant's calls. */
            pipelineConfiguration?: components["schemas"]["PipelineConfiguration"];
            /** @description These are the credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can provide a subset using this. */
            credentialIds?: string[];
        };
        AssistantOverrides: {
            /** @description These are the options for the assistant's transcriber. */
            transcriber?: components["schemas"]["DeepgramTranscriber"] | components["schemas"]["GladiaTranscriber"] | components["schemas"]["TalkscriberTranscriber"];
            /** @description These are the options for the assistant's LLM. */
            model?: components["schemas"]["AnyscaleModel"] | components["schemas"]["AnthropicModel"] | components["schemas"]["CustomLLMModel"] | components["schemas"]["DeepInfraModel"] | components["schemas"]["GroqModel"] | components["schemas"]["OpenAIModel"] | components["schemas"]["OpenRouterModel"] | components["schemas"]["PerplexityAIModel"] | components["schemas"]["TogetherAIModel"] | components["schemas"]["VapiModel"];
            /**
             * @description These are the options for the assistant's voice.
             * @default {
             *       "provider": "playht",
             *       "voiceId": "jennifer"
             *     }
             */
            voice: components["schemas"]["AzureVoice"] | components["schemas"]["CartesiaVoice"] | components["schemas"]["DeepgramVoice"] | components["schemas"]["ElevenLabsVoice"] | components["schemas"]["LMNTVoice"] | components["schemas"]["NeetsVoice"] | components["schemas"]["OpenAIVoice"] | components["schemas"]["PlayHTVoice"] | components["schemas"]["RimeAIVoice"];
            /**
             * @description This is the mode for the first message. Default is 'assistant-speaks-first'.
             *
             *     Use:
             *     - 'assistant-speaks-first' to have the assistant speak first.
             *     - 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
             *     - 'assistant-speaks-first-with-model-generated-message' to have the assistant speak first with a message generated by the model based on the conversation state. (`assistant.model.messages` at call start, `call.messages` at squad transfer points).
             *
             *     @default 'assistant-speaks-first'
             * @example assistant-speaks-first
             * @enum {string}
             */
            firstMessageMode?: "assistant-speaks-first" | "assistant-speaks-first-with-model-generated-message" | "assistant-waits-for-user";
            /**
             * @description This sets whether the assistant's calls are recorded. Defaults to true.
             * @example true
             */
            recordingEnabled?: boolean;
            /**
             * @description When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false.
             * @example false
             */
            hipaaEnabled?: boolean;
            /**
             * @description These are the messages that will be sent to your Client SDKs. Default is conversation-update,function-call,hang,model-output,speech-update,status-update,transcript,tool-calls,user-interrupted,voice-input. You can check the shape of the messages in ClientMessage schema.
             * @example [
             *       "conversation-update",
             *       "function-call",
             *       "hang",
             *       "model-output",
             *       "speech-update",
             *       "status-update",
             *       "transcript",
             *       "tool-calls",
             *       "user-interrupted",
             *       "voice-input"
             *     ]
             * @enum {string}
             */
            clientMessages?: "conversation-update" | "function-call" | "function-call-result" | "hang" | "metadata" | "model-output" | "speech-update" | "status-update" | "transcript" | "tool-calls" | "tool-calls-result" | "user-interrupted" | "voice-input";
            /**
             * @description These are the messages that will be sent to your Server URL. Default is conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,user-interrupted. You can check the shape of the messages in ServerMessage schema.
             * @example [
             *       "conversation-update",
             *       "end-of-call-report",
             *       "function-call",
             *       "hang",
             *       "speech-update",
             *       "status-update",
             *       "tool-calls",
             *       "transfer-destination-request",
             *       "user-interrupted"
             *     ]
             * @enum {string}
             */
            serverMessages?: "conversation-update" | "end-of-call-report" | "function-call" | "hang" | "model-output" | "phone-call-control" | "speech-update" | "status-update" | "transcript" | "tool-calls" | "transfer-destination-request" | "transfer-update" | "user-interrupted" | "voice-input";
            /**
             * @description How many seconds of silence to wait before ending the call. Defaults to 30.
             *
             *     @default 30
             * @example 30
             */
            silenceTimeoutSeconds?: number;
            /**
             * @description This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
             *
             *     @default 1800 (~30 minutes)
             * @example 1800
             */
            maxDurationSeconds?: number;
            /**
             * @description This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
             * @example office
             * @enum {string}
             */
            backgroundSound?: "off" | "office";
            /**
             * @description This determines whether the model says 'mhmm', 'ahem' etc. while user is speaking.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            backchannelingEnabled?: boolean;
            /**
             * @description This enables filtering of noise and background speech while the user is talking.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            backgroundDenoisingEnabled?: boolean;
            /**
             * @description This determines whether the model's output is used in conversation history rather than the transcription of assistant's speech.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            modelOutputInMessagesEnabled?: boolean;
            /** @description These are the configurations to be passed to the transport providers of assistant's calls, like Twilio. You can store multiple configurations for different transport providers. For a call, only the configuration matching the call transport provider is used. */
            transportConfigurations?: components["schemas"]["TransportConfigurationTwilio"][];
            /** @description These are values that will be used to replace the template variables in the assistant messages and other text-based fields. */
            variableValues?: Record<string, never>;
            /** @description This is the name of the assistant.
             *
             *     This is required when you want to transfer between assistants in a call. */
            name?: string;
            /** @description This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).
             *
             *     If unspecified, assistant will wait for user to speak and use the model to respond once they speak. */
            firstMessage?: string;
            /** @description These are the settings to configure or disable voicemail detection. Alternatively, voicemail detection can be configured using the model.tools=[VoicemailTool].
             *     This uses Twilio's built-in detection while the VoicemailTool relies on the model to detect if a voicemail was reached.
             *     You can use neither of them, one of them, or both of them. By default, Twilio built-in detection is enabled while VoicemailTool is not. */
            voicemailDetection?: components["schemas"]["TwilioVoicemailDetection"];
            /** @description This is the message that the assistant will say if the call is forwarded to voicemail.
             *
             *     If unspecified, it will hang up. */
            voicemailMessage?: string;
            /** @description This is the message that the assistant will say if it ends the call.
             *
             *     If unspecified, it will hang up without saying anything. */
            endCallMessage?: string;
            /** @description This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive. */
            endCallPhrases?: string[];
            /** @description This is for metadata you want to store on the assistant. */
            metadata?: Record<string, never>;
            /** @description This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.
             *
             *     All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl */
            serverUrl?: string;
            /** @description This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
            /** @description This is the plan for analysis of assistant's calls. Stored in `call.analysis`. */
            analysisPlan?: components["schemas"]["AnalysisPlan"];
            /** @description This is the plan for artifacts generated during assistant's calls. Stored in `call.artifact`.
             *
             *     Note: `recordingEnabled` is currently at the root level. It will be moved to `artifactPlan` in the future, but will remain backwards compatible. */
            artifactPlan?: components["schemas"]["ArtifactPlan"];
            /** @description This is the plan for static messages that can be spoken by the assistant during the call, like `idleMessages`.
             *
             *     Note: `firstMessage`, `voicemailMessage`, and `endCallMessage` are currently at the root level. They will be moved to `messagePlan` in the future, but will remain backwards compatible. */
            messagePlan?: components["schemas"]["MessagePlan"];
            /** @description This is the pipeline configuration for assistant's calls. */
            pipelineConfiguration?: components["schemas"]["PipelineConfiguration"];
            /** @description These are the credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can provide a subset using this. */
            credentialIds?: string[];
        };
        SquadMemberDTO: {
            /** @description This is the assistant that will be used for the call. To use a transient assistant, use `assistant` instead. */
            assistantId?: string | null;
            /** @description This is the assistant that will be used for the call. To use an existing assistant, use `assistantId` instead. */
            assistant?: components["schemas"]["CreateAssistantDTO"];
            /** @description This can be used to override the assistant's settings and provide values for it's template variables. */
            assistantOverrides?: components["schemas"]["AssistantOverrides"];
            /** @description These are the others assistants that this assistant can transfer to.
             *
             *     If the assistant already has transfer call tool, these destinations are just appended to existing ones. */
            assistantDestinations?: components["schemas"]["TransferDestinationAssistant"][];
        };
        CreateSquadDTO: {
            /** @description This is the name of the squad. */
            name?: string;
            /** @description This is the list of assistants that make up the squad.
             *
             *     The call will start with the first assistant in the list. */
            members: components["schemas"]["SquadMemberDTO"][];
            /** @description This can be used to override all the assistants' settings and provide values for their template variables.
             *
             *     Both `membersOverrides` and `members[n].assistantOverrides` can be used together. First, `members[n].assistantOverrides` is applied. Then, `membersOverrides` is applied as a global override. */
            membersOverrides?: components["schemas"]["AssistantOverrides"];
        };
        ImportTwilioPhoneNumberDTO: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @deprecated
             * @description These are the digits of the phone number you own on your Twilio.
             */
            twilioPhoneNumber: string;
            /** @description This is your Twilio Account SID that will be used to handle this phone number. */
            twilioAccountSid: string;
            /** @description This is the Twilio Auth Token that will be used to handle this phone number. */
            twilioAuthToken: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
        };
        CreateCustomerDTO: {
            /**
             * @description This is the flag to toggle the E164 check for the `number` field. This is an advanced property which should be used if you know your use case requires it.
             *
             *     Use cases:
             *     - `false`: To allow non-E164 numbers like `+001234567890`, `1234', or `abc`. This is useful for dialing out to non-E164 numbers on your SIP trunks.
             *     - `true` (default): To allow only E164 numbers like `+14155551234`. This is for most standard PSTN calls.
             *
             *     If `false`, the `number` is still required to only contain alphanumeric characters (regex: `/^\+?[a-zA-Z0-9]+$/`).
             *
             *     @default true (E164 check is enabled)
             * @default true
             */
            numberE164CheckEnabled: boolean;
            /**
             * @description This is the extension that will be dialed after the call is answered.
             * @example null
             */
            extension?: string;
            /** @description This is the number of the customer. */
            number?: string;
            /** @description This is the SIP URI of the customer. */
            sipUri?: string;
            /** @description This is the name of the customer. This is just for your own reference. */
            name?: string;
        };
        CreateCallDTO: {
            /** @description This is the name of the call. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for the call. To use a transient assistant, use `assistant` instead. */
            assistantId?: string;
            /** @description This is the assistant that will be used for the call. To use an existing assistant, use `assistantId` instead. */
            assistant?: components["schemas"]["CreateAssistantDTO"];
            /** @description These are the overrides for the `assistant` or `assistantId`'s settings and template variables. */
            assistantOverrides?: components["schemas"]["AssistantOverrides"];
            /** @description This is the squad that will be used for the call. To use a transient squad, use `squad` instead. */
            squadId?: string;
            /** @description This is a squad that will be used for the call. To use an existing squad, use `squadId` instead. */
            squad?: components["schemas"]["CreateSquadDTO"];
            /** @description This is the phone number that will be used for the call. To use a transient number, use `phoneNumber` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            phoneNumberId?: string;
            /** @description This is the phone number that will be used for the call. To use an existing number, use `phoneNumberId` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            phoneNumber?: components["schemas"]["ImportTwilioPhoneNumberDTO"];
            /** @description This is the customer that will be called. To call a transient customer , use `customer` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            customerId?: string;
            /** @description This is the customer that will be called. To call an existing customer, use `customerId` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            customer?: components["schemas"]["CreateCustomerDTO"];
        };
        AnalysisCostBreakdown: {
            /** @description This is the cost to summarize the call. */
            summary?: number;
            /** @description This is the number of prompt tokens used to summarize the call. */
            summaryPromptTokens?: number;
            /** @description This is the number of completion tokens used to summarize the call. */
            summaryCompletionTokens?: number;
            /** @description This is the cost to extract structured data from the call. */
            structuredData?: number;
            /** @description This is the number of prompt tokens used to extract structured data from the call. */
            structuredDataPromptTokens?: number;
            /** @description This is the number of completion tokens used to extract structured data from the call. */
            structuredDataCompletionTokens?: number;
            /** @description This is the cost to evaluate if the call was successful. */
            successEvaluation?: number;
            /** @description This is the number of prompt tokens used to evaluate if the call was successful. */
            successEvaluationPromptTokens?: number;
            /** @description This is the number of completion tokens used to evaluate if the call was successful. */
            successEvaluationCompletionTokens?: number;
        };
        CostBreakdown: {
            /** @description This is the cost of the transport provider, like Twilio or Vonage. */
            transport?: number;
            /** @description This is the cost of the speech-to-text service. */
            stt?: number;
            /** @description This is the cost of the language model. */
            llm?: number;
            /** @description This is the cost of the text-to-speech service. */
            tts?: number;
            /** @description This is the cost of Vapi. */
            vapi?: number;
            /** @description This is the total cost of the call. */
            total?: number;
            /** @description This is the LLM prompt tokens used for the call. */
            llmPromptTokens?: number;
            /** @description This is the LLM completion tokens used for the call. */
            llmCompletionTokens?: number;
            /** @description This is the TTS characters used for the call. */
            ttsCharacters?: number;
            /** @description This is the cost of the analysis. */
            analysisCostBreakdown?: components["schemas"]["AnalysisCostBreakdown"];
        };
        Artifact: {
            /** @description These are the messages that were spoken during the call. */
            messages?: (components["schemas"]["UserMessage"] | components["schemas"]["SystemMessage"] | components["schemas"]["BotMessage"] | components["schemas"]["ToolCallMessage"] | components["schemas"]["ToolCallResultMessage"])[];
            /** @description These are the messages that were spoken during the call, formatted for OpenAI. */
            messagesOpenAIFormatted?: components["schemas"]["OpenAIMessage"][];
            /** @description This is video recording url for the call. Enable by setting `assistant.artifactPlan.videoRecordingEnabled`. */
            videoRecordingUrl?: string;
            /** @description This is video recording start delay in ms. Only available when `assistant.artifactPlan.videoRecordingEnabled` is enabled. */
            videoRecordingStartDelaySeconds?: number;
        };
        Analysis: {
            /** @description This is the summary of the call. Customize by setting `assistant.analysisPlan.summaryPrompt`. */
            summary?: string;
            /** @description This is the structured data extracted from the call. Customize by setting `assistant.analysisPlan.structuredDataPrompt` and/or `assistant.analysisPlan.structuredDataSchema`. */
            structuredData?: Record<string, never>;
            /** @description This is the evaluation of the call. Customize by setting `assistant.analysisPlan.successEvaluationPrompt` and/or `assistant.analysisPlan.successEvaluationRubric`. */
            successEvaluation?: string;
        };
        Call: {
            /**
             * @description This is the type of call.
             * @enum {string}
             */
            type?: "inboundPhoneCall" | "outboundPhoneCall" | "webCall";
            /** @description These are the messages that were spoken during the call. */
            messages?: (components["schemas"]["UserMessage"] | components["schemas"]["SystemMessage"] | components["schemas"]["BotMessage"] | components["schemas"]["FunctionCallMessage"] | components["schemas"]["ToolCallMessage"] | components["schemas"]["ToolCallResultMessage"] | components["schemas"]["FunctionResultMessage"])[];
            /**
             * @description This is the provider of the call.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type.
             * @enum {string}
             */
            phoneCallProvider?: "twilio" | "vonage" | "vapi";
            /**
             * @description This is the transport of the phone call.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type.
             * @enum {string}
             */
            phoneCallTransport?: "sip" | "pstn";
            /**
             * @description This is the status of the call.
             * @enum {string}
             */
            status?: "queued" | "ringing" | "in-progress" | "forwarding" | "ended";
            /**
             * @description This is the explanation for how the call ended.
             * @enum {string}
             */
            endedReason?: "assistant-error" | "assistant-not-found" | "db-error" | "no-server-available" | "pipeline-error-openai-llm-failed" | "pipeline-error-azure-openai-llm-failed" | "pipeline-error-groq-llm-failed" | "pipeline-error-openai-voice-failed" | "pipeline-error-cartesia-voice-failed" | "pipeline-error-deepgram-transcriber-failed" | "pipeline-error-deepgram-voice-failed" | "pipeline-error-gladia-transcriber-failed" | "pipeline-error-eleven-labs-voice-failed" | "pipeline-error-playht-voice-failed" | "pipeline-error-lmnt-voice-failed" | "pipeline-error-azure-voice-failed" | "pipeline-error-rime-ai-voice-failed" | "pipeline-error-neets-voice-failed" | "pipeline-no-available-model" | "worker-shutdown" | "twilio-failed-to-connect-call" | "unknown-error" | "vonage-disconnected" | "vonage-failed-to-connect-call" | "phone-call-provider-bypass-enabled-but-no-call-received" | "vapi-error-phone-call-worker-setup-socket-error" | "vapi-error-phone-call-worker-worker-setup-socket-timeout" | "vapi-error-phone-call-worker-could-not-find-call" | "vapi-error-phone-call-worker-call-never-connected" | "vapi-error-web-call-worker-setup-failed" | "assistant-not-invalid" | "assistant-not-provided" | "call-start-error-neither-assistant-nor-server-set" | "assistant-request-failed" | "assistant-request-returned-error" | "assistant-request-returned-unspeakable-error" | "assistant-request-returned-invalid-assistant" | "assistant-request-returned-no-assistant" | "assistant-request-returned-forwarding-phone-number" | "assistant-ended-call" | "assistant-said-end-call-phrase" | "assistant-forwarded-call" | "assistant-join-timed-out" | "customer-busy" | "customer-ended-call" | "customer-did-not-answer" | "customer-did-not-give-microphone-permission" | "assistant-said-message-with-end-call-enabled" | "exceeded-max-duration" | "manually-canceled" | "phone-call-provider-closed-websocket" | "pipeline-error-anthropic-llm-failed" | "pipeline-error-together-ai-llm-failed" | "pipeline-error-anyscale-llm-failed" | "pipeline-error-openrouter-llm-failed" | "pipeline-error-perplexity-ai-llm-failed" | "pipeline-error-deepinfra-llm-failed" | "pipeline-error-runpod-llm-failed" | "pipeline-error-custom-llm-llm-failed" | "pipeline-error-eleven-labs-voice-not-found" | "pipeline-error-eleven-labs-quota-exceeded" | "pipeline-error-eleven-labs-unauthorized-access" | "pipeline-error-eleven-labs-unauthorized-to-access-model" | "pipeline-error-eleven-labs-professional-voices-only-for-creator-plus" | "pipeline-error-eleven-labs-blocked-free-plan-and-requested-upgrade" | "pipeline-error-eleven-labs-blocked-concurrent-requests-and-requested-upgrade" | "pipeline-error-eleven-labs-blocked-using-instant-voice-clone-and-requested-upgrade" | "pipeline-error-eleven-labs-system-busy-and-requested-upgrade" | "pipeline-error-eleven-labs-voice-not-fine-tuned" | "pipeline-error-eleven-labs-invalid-api-key" | "pipeline-error-eleven-labs-invalid-voice-samples" | "pipeline-error-eleven-labs-voice-disabled-by-owner" | "pipeline-error-eleven-labs-blocked-account-in-probation" | "pipeline-error-eleven-labs-blocked-content-against-their-policy" | "pipeline-error-playht-request-timed-out" | "pipeline-error-playht-invalid-voice" | "pipeline-error-playht-unexpected-error" | "pipeline-error-playht-out-of-credits" | "pipeline-error-playht-rate-limit-exceeded" | "pipeline-error-playht-502-gateway-error" | "pipeline-error-playht-504-gateway-error" | "pipeline-error-gladia-transcriber-failed" | "sip-gateway-failed-to-connect-call" | "silence-timed-out" | "voicemail" | "vonage-rejected";
            /** @description This is the destination where the call ended up being transferred to. If the call was not transferred, this will be empty. */
            destination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /** @description This is the unique identifier for the call. */
            id: string;
            /** @description This is the unique identifier for the org that this call belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the call was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the call was last updated.
             */
            updatedAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the call was started.
             */
            startedAt?: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the call was ended.
             */
            endedAt?: string;
            /** @description This is the cost of the call in USD. */
            cost?: number;
            /** @description This is the cost of the call in USD. */
            costBreakdown?: components["schemas"]["CostBreakdown"];
            /** @description These are the costs of individual components of the call in USD. */
            costs?: Record<string, never>[];
            /** @description This is the transcript of the call. */
            transcript?: string;
            /** @description This is the URL of the recording of the call. */
            recordingUrl?: string;
            /** @description This is the URL of the recording of the call in two channels. */
            stereoRecordingUrl?: string;
            /** @description This stores artifacts of the call. Customize what artifacts are created by configuring `assistant.artifactPlan`. */
            artifact?: components["schemas"]["Artifact"];
            /** @description This is a copy of assistant artifact plan. This isn't actually stored on the call but rather just returned in POST /call/web to enable artifact creation client side. */
            artifactPlan?: components["schemas"]["ArtifactPlan"];
            /** @description This is the analysis of the call. Customize the analysis by configuring `assistant.analysisPlan`. */
            analysis?: components["schemas"]["Analysis"];
            /** @description The ID of the call as provided by the phone number service. callSid in Twilio. conversationUuid in Vonage.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            phoneCallProviderId?: string;
            /** @description This is the assistant that will be used for the call. To use a transient assistant, use `assistant` instead. */
            assistantId?: string;
            /** @description This is the assistant that will be used for the call. To use an existing assistant, use `assistantId` instead. */
            assistant?: components["schemas"]["CreateAssistantDTO"];
            /** @description These are the overrides for the `assistant` or `assistantId`'s settings and template variables. */
            assistantOverrides?: components["schemas"]["AssistantOverrides"];
            /** @description This is the squad that will be used for the call. To use a transient squad, use `squad` instead. */
            squadId?: string;
            /** @description This is a squad that will be used for the call. To use an existing squad, use `squadId` instead. */
            squad?: components["schemas"]["CreateSquadDTO"];
            /** @description This is the phone number that will be used for the call. To use a transient number, use `phoneNumber` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            phoneNumberId?: string;
            /** @description This is the phone number that will be used for the call. To use an existing number, use `phoneNumberId` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            phoneNumber?: components["schemas"]["ImportTwilioPhoneNumberDTO"];
            /** @description This is the customer that will be called. To call a transient customer , use `customer` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            customerId?: string;
            /** @description This is the customer that will be called. To call an existing customer, use `customerId` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            customer?: components["schemas"]["CreateCustomerDTO"];
            /** @description This is the name of the call. This is just for your own reference. */
            name?: string;
        };
        PaginationMeta: {
            itemsPerPage: number;
            totalItems: number;
            currentPage: number;
        };
        CallPaginatedResponse: {
            results: components["schemas"]["Call"][];
            metadata: components["schemas"]["PaginationMeta"];
        };
        CreateOutboundCallDTO: {
            /** @description This is the name of the call. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for the call. To use a transient assistant, use `assistant` instead. */
            assistantId?: string;
            /** @description This is the assistant that will be used for the call. To use an existing assistant, use `assistantId` instead. */
            assistant?: components["schemas"]["CreateAssistantDTO"];
            /** @description These are the overrides for the `assistant` or `assistantId`'s settings and template variables. */
            assistantOverrides?: components["schemas"]["AssistantOverrides"];
            /** @description This is the squad that will be used for the call. To use a transient squad, use `squad` instead. */
            squadId?: string;
            /** @description This is a squad that will be used for the call. To use an existing squad, use `squadId` instead. */
            squad?: components["schemas"]["CreateSquadDTO"];
            /** @description This is the phone number that will be used for the call. To use a transient number, use `phoneNumber` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            phoneNumberId?: string;
            /** @description This is the phone number that will be used for the call. To use an existing number, use `phoneNumberId` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            phoneNumber?: components["schemas"]["ImportTwilioPhoneNumberDTO"];
            /** @description This is the customer that will be called. To call a transient customer , use `customer` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            customerId?: string;
            /** @description This is the customer that will be called. To call an existing customer, use `customerId` instead.
             *
             *     Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
            customer?: components["schemas"]["CreateCustomerDTO"];
        };
        CreateWebCallDTO: {
            /** @description This is the assistant that will be used for the call. To use a transient assistant, use `assistant` instead. */
            assistantId?: string;
            /** @description This is the assistant that will be used for the call. To use an existing assistant, use `assistantId` instead. */
            assistant?: components["schemas"]["CreateAssistantDTO"];
            /** @description These are the overrides for the `assistant` or `assistantId`'s settings and template variables. */
            assistantOverrides?: components["schemas"]["AssistantOverrides"];
            /** @description This is the squad that will be used for the call. To use a transient squad, use `squad` instead. */
            squadId?: string;
            /** @description This is a squad that will be used for the call. To use an existing squad, use `squadId` instead. */
            squad?: components["schemas"]["CreateSquadDTO"];
        };
        UpdateCallDTO: {
            /** @description This is the name of the call. This is just for your own reference. */
            name?: string;
        };
        Assistant: {
            /** @description These are the options for the assistant's transcriber. */
            transcriber?: components["schemas"]["DeepgramTranscriber"] | components["schemas"]["GladiaTranscriber"] | components["schemas"]["TalkscriberTranscriber"];
            /** @description These are the options for the assistant's LLM. */
            model?: components["schemas"]["AnyscaleModel"] | components["schemas"]["AnthropicModel"] | components["schemas"]["CustomLLMModel"] | components["schemas"]["DeepInfraModel"] | components["schemas"]["GroqModel"] | components["schemas"]["OpenAIModel"] | components["schemas"]["OpenRouterModel"] | components["schemas"]["PerplexityAIModel"] | components["schemas"]["TogetherAIModel"] | components["schemas"]["VapiModel"];
            /**
             * @description These are the options for the assistant's voice.
             * @default {
             *       "provider": "playht",
             *       "voiceId": "jennifer"
             *     }
             */
            voice: components["schemas"]["AzureVoice"] | components["schemas"]["CartesiaVoice"] | components["schemas"]["DeepgramVoice"] | components["schemas"]["ElevenLabsVoice"] | components["schemas"]["LMNTVoice"] | components["schemas"]["NeetsVoice"] | components["schemas"]["OpenAIVoice"] | components["schemas"]["PlayHTVoice"] | components["schemas"]["RimeAIVoice"];
            /**
             * @description This is the mode for the first message. Default is 'assistant-speaks-first'.
             *
             *     Use:
             *     - 'assistant-speaks-first' to have the assistant speak first.
             *     - 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
             *     - 'assistant-speaks-first-with-model-generated-message' to have the assistant speak first with a message generated by the model based on the conversation state. (`assistant.model.messages` at call start, `call.messages` at squad transfer points).
             *
             *     @default 'assistant-speaks-first'
             * @example assistant-speaks-first
             * @enum {string}
             */
            firstMessageMode?: "assistant-speaks-first" | "assistant-speaks-first-with-model-generated-message" | "assistant-waits-for-user";
            /**
             * @description This sets whether the assistant's calls are recorded. Defaults to true.
             * @example true
             */
            recordingEnabled?: boolean;
            /**
             * @description When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false.
             * @example false
             */
            hipaaEnabled?: boolean;
            /**
             * @description These are the messages that will be sent to your Client SDKs. Default is conversation-update,function-call,hang,model-output,speech-update,status-update,transcript,tool-calls,user-interrupted,voice-input. You can check the shape of the messages in ClientMessage schema.
             * @example [
             *       "conversation-update",
             *       "function-call",
             *       "hang",
             *       "model-output",
             *       "speech-update",
             *       "status-update",
             *       "transcript",
             *       "tool-calls",
             *       "user-interrupted",
             *       "voice-input"
             *     ]
             * @enum {string}
             */
            clientMessages?: "conversation-update" | "function-call" | "function-call-result" | "hang" | "metadata" | "model-output" | "speech-update" | "status-update" | "transcript" | "tool-calls" | "tool-calls-result" | "user-interrupted" | "voice-input";
            /**
             * @description These are the messages that will be sent to your Server URL. Default is conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,user-interrupted. You can check the shape of the messages in ServerMessage schema.
             * @example [
             *       "conversation-update",
             *       "end-of-call-report",
             *       "function-call",
             *       "hang",
             *       "speech-update",
             *       "status-update",
             *       "tool-calls",
             *       "transfer-destination-request",
             *       "user-interrupted"
             *     ]
             * @enum {string}
             */
            serverMessages?: "conversation-update" | "end-of-call-report" | "function-call" | "hang" | "model-output" | "phone-call-control" | "speech-update" | "status-update" | "transcript" | "tool-calls" | "transfer-destination-request" | "transfer-update" | "user-interrupted" | "voice-input";
            /**
             * @description How many seconds of silence to wait before ending the call. Defaults to 30.
             *
             *     @default 30
             * @example 30
             */
            silenceTimeoutSeconds?: number;
            /**
             * @description This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
             *
             *     @default 1800 (~30 minutes)
             * @example 1800
             */
            maxDurationSeconds?: number;
            /**
             * @description This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
             * @example office
             * @enum {string}
             */
            backgroundSound?: "off" | "office";
            /**
             * @description This determines whether the model says 'mhmm', 'ahem' etc. while user is speaking.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            backchannelingEnabled?: boolean;
            /**
             * @description This enables filtering of noise and background speech while the user is talking.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            backgroundDenoisingEnabled?: boolean;
            /**
             * @description This determines whether the model's output is used in conversation history rather than the transcription of assistant's speech.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            modelOutputInMessagesEnabled?: boolean;
            /** @description These are the configurations to be passed to the transport providers of assistant's calls, like Twilio. You can store multiple configurations for different transport providers. For a call, only the configuration matching the call transport provider is used. */
            transportConfigurations?: components["schemas"]["TransportConfigurationTwilio"][];
            isServerUrlSecretSet: Record<string, never>;
            /** @description This is the name of the assistant.
             *
             *     This is required when you want to transfer between assistants in a call. */
            name?: string;
            /** @description This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).
             *
             *     If unspecified, assistant will wait for user to speak and use the model to respond once they speak. */
            firstMessage?: string;
            /** @description These are the settings to configure or disable voicemail detection. Alternatively, voicemail detection can be configured using the model.tools=[VoicemailTool].
             *     This uses Twilio's built-in detection while the VoicemailTool relies on the model to detect if a voicemail was reached.
             *     You can use neither of them, one of them, or both of them. By default, Twilio built-in detection is enabled while VoicemailTool is not. */
            voicemailDetection?: components["schemas"]["TwilioVoicemailDetection"];
            /** @description This is the message that the assistant will say if the call is forwarded to voicemail.
             *
             *     If unspecified, it will hang up. */
            voicemailMessage?: string;
            /** @description This is the message that the assistant will say if it ends the call.
             *
             *     If unspecified, it will hang up without saying anything. */
            endCallMessage?: string;
            /** @description This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive. */
            endCallPhrases?: string[];
            /** @description This is for metadata you want to store on the assistant. */
            metadata?: Record<string, never>;
            /** @description This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.
             *
             *     All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl */
            serverUrl?: string;
            /** @description This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
            /** @description This is the plan for analysis of assistant's calls. Stored in `call.analysis`. */
            analysisPlan?: components["schemas"]["AnalysisPlan"];
            /** @description This is the plan for artifacts generated during assistant's calls. Stored in `call.artifact`.
             *
             *     Note: `recordingEnabled` is currently at the root level. It will be moved to `artifactPlan` in the future, but will remain backwards compatible. */
            artifactPlan?: components["schemas"]["ArtifactPlan"];
            /** @description This is the plan for static messages that can be spoken by the assistant during the call, like `idleMessages`.
             *
             *     Note: `firstMessage`, `voicemailMessage`, and `endCallMessage` are currently at the root level. They will be moved to `messagePlan` in the future, but will remain backwards compatible. */
            messagePlan?: components["schemas"]["MessagePlan"];
            /** @description This is the pipeline configuration for assistant's calls. */
            pipelineConfiguration?: components["schemas"]["PipelineConfiguration"];
            /** @description These are the credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can provide a subset using this. */
            credentialIds?: string[];
            /** @description This is the unique identifier for the assistant. */
            id: string;
            /** @description This is the unique identifier for the org that this assistant belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        UpdateAssistantDTO: {
            /** @description These are the options for the assistant's transcriber. */
            transcriber?: components["schemas"]["DeepgramTranscriber"] | components["schemas"]["GladiaTranscriber"] | components["schemas"]["TalkscriberTranscriber"];
            /** @description These are the options for the assistant's LLM. */
            model?: components["schemas"]["AnyscaleModel"] | components["schemas"]["AnthropicModel"] | components["schemas"]["CustomLLMModel"] | components["schemas"]["DeepInfraModel"] | components["schemas"]["GroqModel"] | components["schemas"]["OpenAIModel"] | components["schemas"]["OpenRouterModel"] | components["schemas"]["PerplexityAIModel"] | components["schemas"]["TogetherAIModel"] | components["schemas"]["VapiModel"];
            /**
             * @description These are the options for the assistant's voice.
             * @default {
             *       "provider": "playht",
             *       "voiceId": "jennifer"
             *     }
             */
            voice: components["schemas"]["AzureVoice"] | components["schemas"]["CartesiaVoice"] | components["schemas"]["DeepgramVoice"] | components["schemas"]["ElevenLabsVoice"] | components["schemas"]["LMNTVoice"] | components["schemas"]["NeetsVoice"] | components["schemas"]["OpenAIVoice"] | components["schemas"]["PlayHTVoice"] | components["schemas"]["RimeAIVoice"];
            /**
             * @description This is the mode for the first message. Default is 'assistant-speaks-first'.
             *
             *     Use:
             *     - 'assistant-speaks-first' to have the assistant speak first.
             *     - 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
             *     - 'assistant-speaks-first-with-model-generated-message' to have the assistant speak first with a message generated by the model based on the conversation state. (`assistant.model.messages` at call start, `call.messages` at squad transfer points).
             *
             *     @default 'assistant-speaks-first'
             * @example assistant-speaks-first
             * @enum {string}
             */
            firstMessageMode?: "assistant-speaks-first" | "assistant-speaks-first-with-model-generated-message" | "assistant-waits-for-user";
            /**
             * @description This sets whether the assistant's calls are recorded. Defaults to true.
             * @example true
             */
            recordingEnabled?: boolean;
            /**
             * @description When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false.
             * @example false
             */
            hipaaEnabled?: boolean;
            /**
             * @description These are the messages that will be sent to your Client SDKs. Default is conversation-update,function-call,hang,model-output,speech-update,status-update,transcript,tool-calls,user-interrupted,voice-input. You can check the shape of the messages in ClientMessage schema.
             * @example [
             *       "conversation-update",
             *       "function-call",
             *       "hang",
             *       "model-output",
             *       "speech-update",
             *       "status-update",
             *       "transcript",
             *       "tool-calls",
             *       "user-interrupted",
             *       "voice-input"
             *     ]
             * @enum {string}
             */
            clientMessages?: "conversation-update" | "function-call" | "function-call-result" | "hang" | "metadata" | "model-output" | "speech-update" | "status-update" | "transcript" | "tool-calls" | "tool-calls-result" | "user-interrupted" | "voice-input";
            /**
             * @description These are the messages that will be sent to your Server URL. Default is conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,user-interrupted. You can check the shape of the messages in ServerMessage schema.
             * @example [
             *       "conversation-update",
             *       "end-of-call-report",
             *       "function-call",
             *       "hang",
             *       "speech-update",
             *       "status-update",
             *       "tool-calls",
             *       "transfer-destination-request",
             *       "user-interrupted"
             *     ]
             * @enum {string}
             */
            serverMessages?: "conversation-update" | "end-of-call-report" | "function-call" | "hang" | "model-output" | "phone-call-control" | "speech-update" | "status-update" | "transcript" | "tool-calls" | "transfer-destination-request" | "transfer-update" | "user-interrupted" | "voice-input";
            /**
             * @description How many seconds of silence to wait before ending the call. Defaults to 30.
             *
             *     @default 30
             * @example 30
             */
            silenceTimeoutSeconds?: number;
            /**
             * @description This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
             *
             *     @default 1800 (~30 minutes)
             * @example 1800
             */
            maxDurationSeconds?: number;
            /**
             * @description This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
             * @example office
             * @enum {string}
             */
            backgroundSound?: "off" | "office";
            /**
             * @description This determines whether the model says 'mhmm', 'ahem' etc. while user is speaking.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            backchannelingEnabled?: boolean;
            /**
             * @description This enables filtering of noise and background speech while the user is talking.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            backgroundDenoisingEnabled?: boolean;
            /**
             * @description This determines whether the model's output is used in conversation history rather than the transcription of assistant's speech.
             *
             *     Default `false` while in beta.
             *
             *     @default false
             * @example false
             */
            modelOutputInMessagesEnabled?: boolean;
            /** @description These are the configurations to be passed to the transport providers of assistant's calls, like Twilio. You can store multiple configurations for different transport providers. For a call, only the configuration matching the call transport provider is used. */
            transportConfigurations?: components["schemas"]["TransportConfigurationTwilio"][];
            /** @description This is the name of the assistant.
             *
             *     This is required when you want to transfer between assistants in a call. */
            name?: string;
            /** @description This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).
             *
             *     If unspecified, assistant will wait for user to speak and use the model to respond once they speak. */
            firstMessage?: string;
            /** @description These are the settings to configure or disable voicemail detection. Alternatively, voicemail detection can be configured using the model.tools=[VoicemailTool].
             *     This uses Twilio's built-in detection while the VoicemailTool relies on the model to detect if a voicemail was reached.
             *     You can use neither of them, one of them, or both of them. By default, Twilio built-in detection is enabled while VoicemailTool is not. */
            voicemailDetection?: components["schemas"]["TwilioVoicemailDetection"];
            /** @description This is the message that the assistant will say if the call is forwarded to voicemail.
             *
             *     If unspecified, it will hang up. */
            voicemailMessage?: string;
            /** @description This is the message that the assistant will say if it ends the call.
             *
             *     If unspecified, it will hang up without saying anything. */
            endCallMessage?: string;
            /** @description This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive. */
            endCallPhrases?: string[];
            /** @description This is for metadata you want to store on the assistant. */
            metadata?: Record<string, never>;
            /** @description This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.
             *
             *     All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl */
            serverUrl?: string;
            /** @description This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
            /** @description This is the plan for analysis of assistant's calls. Stored in `call.analysis`. */
            analysisPlan?: components["schemas"]["AnalysisPlan"];
            /** @description This is the plan for artifacts generated during assistant's calls. Stored in `call.artifact`.
             *
             *     Note: `recordingEnabled` is currently at the root level. It will be moved to `artifactPlan` in the future, but will remain backwards compatible. */
            artifactPlan?: components["schemas"]["ArtifactPlan"];
            /** @description This is the plan for static messages that can be spoken by the assistant during the call, like `idleMessages`.
             *
             *     Note: `firstMessage`, `voicemailMessage`, and `endCallMessage` are currently at the root level. They will be moved to `messagePlan` in the future, but will remain backwards compatible. */
            messagePlan?: components["schemas"]["MessagePlan"];
            /** @description This is the pipeline configuration for assistant's calls. */
            pipelineConfiguration?: components["schemas"]["PipelineConfiguration"];
            /** @description These are the credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can provide a subset using this. */
            credentialIds?: string[];
        };
        ByoPhoneNumber: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @description This is to bring your own phone numbers from your own SIP trunks or Carriers. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            provider: "byo-phone-number";
            /**
             * @description This is the flag to toggle the E164 check for the `number` field. This is an advanced property which should be used if you know your use case requires it.
             *
             *     Use cases:
             *     - `false`: To allow non-E164 numbers like `+001234567890`, `1234', or `abc`. This is useful for dialing out to non-E164 numbers on your SIP trunks.
             *     - `true` (default): To allow only E164 numbers like `+14155551234`. This is for most standard PSTN calls.
             *
             *     If `false`, the `number` is still required to only contain alphanumeric characters (regex: `/^\+?[a-zA-Z0-9]+$/`).
             *
             *     @default true (E164 check is enabled)
             * @default true
             */
            numberE164CheckEnabled: boolean;
            /** @description This is the unique identifier for the phone number. */
            id: string;
            /** @description This is the unique identifier for the org that this phone number belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the phone number was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the phone number was last updated.
             */
            updatedAt: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
            /** @description This is the number of the customer. */
            number?: string;
            /** @description This is the credential of your own SIP trunk or Carrier (type `byo-sip-trunk`) which can be used to make calls to this phone number.
             *
             *     You can add the SIP trunk or Carrier credential in the Provider Credentials page on the Dashboard to get the credentialId. */
            credentialId: string;
        };
        TwilioPhoneNumber: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @description This is to use numbers bought on Twilio. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            provider: "twilio";
            /** @description This is the unique identifier for the phone number. */
            id: string;
            /** @description This is the unique identifier for the org that this phone number belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the phone number was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the phone number was last updated.
             */
            updatedAt: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
            /** @description These are the digits of the phone number you own on your Twilio. */
            number: string;
            /** @description This is the Twilio Account SID for the phone number. */
            twilioAccountSid: string;
            /** @description This is the Twilio Auth Token for the phone number. */
            twilioAuthToken: string;
        };
        VonagePhoneNumber: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @description This is to use numbers bought on Vonage. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            provider: "vonage";
            /** @description This is the unique identifier for the phone number. */
            id: string;
            /** @description This is the unique identifier for the org that this phone number belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the phone number was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the phone number was last updated.
             */
            updatedAt: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
            /** @description These are the digits of the phone number you own on your Vonage. */
            number: string;
            /** @description This is the credential that is used to make outgoing calls, and do operations like call transfer and hang up. */
            credentialId: string;
        };
        VapiPhoneNumber: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @description This is to create free SIP phone numbers on Vapi. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            provider: "vapi";
            /** @description This is the unique identifier for the phone number. */
            id: string;
            /** @description This is the unique identifier for the org that this phone number belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the phone number was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the phone number was last updated.
             */
            updatedAt: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
            /** @description This is the SIP URI of the phone number. You can SIP INVITE this. The assistant attached to this number will answer. */
            sipUri: string;
        };
        CreateByoPhoneNumberDTO: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @description This is to bring your own phone numbers from your own SIP trunks or Carriers. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            provider: "byo-phone-number";
            /**
             * @description This is the flag to toggle the E164 check for the `number` field. This is an advanced property which should be used if you know your use case requires it.
             *
             *     Use cases:
             *     - `false`: To allow non-E164 numbers like `+001234567890`, `1234', or `abc`. This is useful for dialing out to non-E164 numbers on your SIP trunks.
             *     - `true` (default): To allow only E164 numbers like `+14155551234`. This is for most standard PSTN calls.
             *
             *     If `false`, the `number` is still required to only contain alphanumeric characters (regex: `/^\+?[a-zA-Z0-9]+$/`).
             *
             *     @default true (E164 check is enabled)
             * @default true
             */
            numberE164CheckEnabled: boolean;
            /** @description This is the number of the customer. */
            number?: string;
            /** @description This is the credential of your own SIP trunk or Carrier (type `byo-sip-trunk`) which can be used to make calls to this phone number.
             *
             *     You can add the SIP trunk or Carrier credential in the Provider Credentials page on the Dashboard to get the credentialId. */
            credentialId: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
        };
        CreateTwilioPhoneNumberDTO: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @description This is to use numbers bought on Twilio. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            provider: "twilio";
            /** @description These are the digits of the phone number you own on your Twilio. */
            number: string;
            /** @description This is the Twilio Account SID for the phone number. */
            twilioAccountSid: string;
            /** @description This is the Twilio Auth Token for the phone number. */
            twilioAuthToken: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
        };
        CreateVonagePhoneNumberDTO: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @description This is to use numbers bought on Vonage. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            provider: "vonage";
            /** @description These are the digits of the phone number you own on your Vonage. */
            number: string;
            /** @description This is the credential that is used to make outgoing calls, and do operations like call transfer and hang up. */
            credentialId: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
        };
        CreateVapiPhoneNumberDTO: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @description This is to create free SIP phone numbers on Vapi. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            provider: "vapi";
            /** @description This is the SIP URI of the phone number. You can SIP INVITE this. The assistant attached to this number will answer. */
            sipUri: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
        };
        BuyPhoneNumberDTO: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /** @description This is the area code of the phone number to purchase. */
            areaCode: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
        };
        ImportVonagePhoneNumberDTO: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /**
             * @deprecated
             * @description These are the digits of the phone number you own on your Vonage.
             */
            vonagePhoneNumber: string;
            /** @description This is the credential that is used to make outgoing calls, and do operations like call transfer and hang up.
             *
             *     You can add the Vonage Credential in the Provider Credentials page on the dashboard to get the credentialId. */
            credentialId: string;
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
        };
        UpdatePhoneNumberDTO: {
            /** @description This is the fallback destination an inbound call will be transferred to if:
             *     1. `assistantId` is not set
             *     2. `squadId` is not set
             *     3. and, `assistant-request` message to the `serverUrl` fails
             *
             *     If this is not set and above conditions are met, the inbound call is hung up with an error message. */
            fallbackDestination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /** @description This is the name of the phone number. This is just for your own reference. */
            name?: string;
            /** @description This is the assistant that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            assistantId?: string;
            /** @description This is the squad that will be used for incoming calls to this phone number.
             *
             *     If neither `assistantId` nor `squadId` is set, `assistant-request` will be sent to your Server URL. Check `ServerMessage` and `ServerMessageResponse` for the shape of the message and response that is expected. */
            squadId?: string;
            /** @description This is the server URL where messages will be sent for calls on this number. This includes the `assistant-request` message.
             *
             *     You can see the shape of the messages sent in `ServerMessage`.
             *
             *     This overrides the `org.serverUrl`. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
            serverUrl?: string;
            /** @description This is the secret Vapi will send with every message to your server. It's sent as a header called x-vapi-secret.
             *
             *     Same precedence logic as serverUrl. */
            serverUrlSecret?: string;
        };
        Squad: {
            /** @description This is the name of the squad. */
            name?: string;
            /** @description This is the list of assistants that make up the squad.
             *
             *     The call will start with the first assistant in the list. */
            members: components["schemas"]["SquadMemberDTO"][];
            /** @description This can be used to override all the assistants' settings and provide values for their template variables.
             *
             *     Both `membersOverrides` and `members[n].assistantOverrides` can be used together. First, `members[n].assistantOverrides` is applied. Then, `membersOverrides` is applied as a global override. */
            membersOverrides?: components["schemas"]["AssistantOverrides"];
            /** @description This is the unique identifier for the squad. */
            id: string;
            /** @description This is the unique identifier for the org that this squad belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the squad was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the squad was last updated.
             */
            updatedAt: string;
        };
        UpdateSquadDTO: {
            /** @description This is the name of the squad. */
            name?: string;
            /** @description This is the list of assistants that make up the squad.
             *
             *     The call will start with the first assistant in the list. */
            members: components["schemas"]["SquadMemberDTO"][];
            /** @description This can be used to override all the assistants' settings and provide values for their template variables.
             *
             *     Both `membersOverrides` and `members[n].assistantOverrides` can be used together. First, `members[n].assistantOverrides` is applied. Then, `membersOverrides` is applied as a global override. */
            membersOverrides?: components["schemas"]["AssistantOverrides"];
        };
        ConversationBlock: {
            /** @description These are the pre-configured messages that will be spoken to the user while the block is running. */
            messages?: (components["schemas"]["BlockStartMessage"] | components["schemas"]["BlockCompleteMessage"])[];
            /** @description This is the input schema for the block. This is the input the block needs to run. It's given to the block as `steps[0].input`
             *
             *     These are accessible as variables:
             *     - ({{input.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.input.propertyName}}) in context of the workflow */
            inputSchema?: components["schemas"]["JsonSchema"];
            /** @description This is the output schema for the block. This is the output the block will return to the workflow (`{{stepName.output}}`).
             *
             *     These are accessible as variables:
             *     - ({{output.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.output.propertyName}}) in context of the workflow */
            outputSchema?: components["schemas"]["JsonSchema"];
            /**
             * @description This block is used for conversation. This can be a free flow conversation or a conversation with a specific goal like collecting some information.
             *
             *     For free block conversation, put clearly in the `instruction` when the block can be considered done.
             *     ```
             *     {
             *      "type": "conversation",
             *      "instruction": "Chit chat with the user asking them about their day. When user asks a specific question or once you have talked to the user for a couple of turns of conversation, move on."
             *     }
             *     ```
             *
             *     For conversation with a specific goal, you can define an `outputSchema` with required fields. The block won't be considered done until the user has provided all the required fields in the `outputSchema`.
             *     ```
             *     {
             *      "type": "conversation",
             *      "instruction": "Ask the user about their hobbies, hopes and dreams.",
             *      "outputSchema": {
             *        "type": "object",
             *        "properties": {
             *          "hobbies": {
             *            "type": "string"
             *          },
             *          "hopes": {
             *            "type": "string"
             *          },
             *          "dreams": {
             *            "type": "string"
             *          }
             *        },
             *        "required": ["hobbies"]
             *      }
             *     }
             *     ```
             *     For the above example, the conversation block will be considered done once the user has provided the `hobbies` (even if they have not provided the `hopes` and `dreams`). (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "conversation";
            /** @description This is the unique identifier for the block. */
            id: string;
            /** @description This is the unique identifier for the organization that this block belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the block was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the block was last updated.
             */
            updatedAt: string;
            /** @description This is the name of the block. This is just for your reference. */
            name?: string;
            /** @description This is the instruction to the model.
             *
             *     You can reference any variable in the context of the current block execution (step):
             *     - "{{input.your-property-name}}" for the current step's input
             *     - "{{your-step-name.output.your-property-name}}" for another step's output (in the same workflow)
             *     - "{{your-step-name.input.your-property-name}}" for another step's input (in the same workflow)
             *     - "{{workflow.input.your-property-name}}" for the current workflow's input
             *     - "{{global.your-property-name}}" for the global context
             *
             *     This can be as simple or as complex as you want it to be.
             *     - "say hello and ask the user about their day!"
             *     - "collect the user's first and last name"
             *     - "user is {{input.firstName}} {{input.lastName}}. their age is {{input.age}}. ask them about their salary and if they might be interested in buying a house. we offer {{input.offer}}" */
            instruction: string;
        };
        ToolCallBlock: {
            /** @description These are the pre-configured messages that will be spoken to the user while the block is running. */
            messages?: (components["schemas"]["BlockStartMessage"] | components["schemas"]["BlockCompleteMessage"])[];
            /** @description This is the input schema for the block. This is the input the block needs to run. It's given to the block as `steps[0].input`
             *
             *     These are accessible as variables:
             *     - ({{input.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.input.propertyName}}) in context of the workflow */
            inputSchema?: components["schemas"]["JsonSchema"];
            /** @description This is the output schema for the block. This is the output the block will return to the workflow (`{{stepName.output}}`).
             *
             *     These are accessible as variables:
             *     - ({{output.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.output.propertyName}}) in context of the workflow */
            outputSchema?: components["schemas"]["JsonSchema"];
            /**
             * @description This block makes a tool call. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "tool-call";
            /** @description This is the tool that the block will call. To use an existing tool, use `toolId`. */
            tool?: components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"];
            /** @description This is the unique identifier for the block. */
            id: string;
            /** @description This is the unique identifier for the organization that this block belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the block was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the block was last updated.
             */
            updatedAt: string;
            /** @description This is the name of the block. This is just for your reference. */
            name?: string;
            /** @description This is the id of the tool that the block will call. To use a transient tool, use `tool`. */
            toolId?: string;
        };
        WorkflowBlock: {
            /** @description These are the pre-configured messages that will be spoken to the user while the block is running. */
            messages?: (components["schemas"]["BlockStartMessage"] | components["schemas"]["BlockCompleteMessage"])[];
            /** @description This is the input schema for the block. This is the input the block needs to run. It's given to the block as `steps[0].input`
             *
             *     These are accessible as variables:
             *     - ({{input.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.input.propertyName}}) in context of the workflow */
            inputSchema?: components["schemas"]["JsonSchema"];
            /** @description This is the output schema for the block. This is the output the block will return to the workflow (`{{stepName.output}}`).
             *
             *     These are accessible as variables:
             *     - ({{output.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.output.propertyName}}) in context of the workflow */
            outputSchema?: components["schemas"]["JsonSchema"];
            /**
             * @description This creates a workflow which can contain any number of steps (block executions). (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "workflow";
            /** @description These are the steps in the workflow. */
            steps?: (components["schemas"]["HandoffStep"] | components["schemas"]["CallbackStep"])[];
            /** @description This is the unique identifier for the block. */
            id: string;
            /** @description This is the unique identifier for the organization that this block belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the block was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the block was last updated.
             */
            updatedAt: string;
            /** @description This is the name of the block. This is just for your reference. */
            name?: string;
        };
        RuleBasedCondition: {
            /**
             * @description This condition is based on a strict rule.
             * @enum {string}
             */
            type: "rule-based";
            /**
             * @description This is the operator you want to use to compare the left side and right side.
             *
             *     The operation becomes `(leftSide) operator (rightSide)`.
             * @enum {string}
             */
            operator: "eq" | "neq" | "gt" | "gte" | "lt" | "lte";
            /** @description This is the left side of the operation.
             *
             *     You can reference any variable in the context of the current block execution (step):
             *     - "{{output.your-property-name}}" for current step's output
             *     - "{{input.your-property-name}}" for current step's input
             *     - "{{your-step-name.output.your-property-name}}" for another step's output (in the same workflow)
             *     - "{{your-step-name.input.your-property-name}}" for another step's input (in the same workflow)
             *     - "{{workflow.input.your-property-name}}" for the current workflow's input
             *     - "{{global.your-property-name}}" for the global context
             *
             *     Or, you can use a constant:
             *     - "1"
             *     - "text"
             *     - "true"
             *     - "false"
             *
             *     Or, you can mix and match with string interpolation:
             *     - "{{your-property-name}}-{{input.your-property-name-2}}-1" */
            leftSide: string;
            /** @description This is the right side of the operation.
             *
             *     You can reference any variable in the context of the current block execution (step):
             *     - "{{output.your-property-name}}" for current step's output
             *     - "{{input.your-property-name}}" for current step's input
             *     - "{{your-step-name.output.your-property-name}}" for another step's output (in the same workflow)
             *     - "{{your-step-name.input.your-property-name}}" for another step's input (in the same workflow)
             *     - "{{workflow.input.your-property-name}}" for the current workflow's input
             *     - "{{global.your-property-name}}" for the global context
             *
             *     Or, you can use a constant:
             *     - "1"
             *     - "text"
             *     - "true"
             *     - "false"
             *
             *     Or, you can mix and match with string interpolation:
             *     - "{{your-property-name}}-{{input.your-property-name-2}}-1" */
            rightSide: string;
        };
        ModelBasedCondition: {
            /**
             * @description This condition is based on a model.
             * @enum {string}
             */
            type: "model-based";
            /** @description This is the instruction which should output a boolean value when passed to a model.
             *
             *     You can reference any variable in the context of the current block execution (step):
             *     - "{{output.your-property-name}}" for current step's output
             *     - "{{input.your-property-name}}" for current step's input
             *     - "{{your-step-name.output.your-property-name}}" for another step's output (in the same workflow)
             *     - "{{your-step-name.input.your-property-name}}" for another step's input (in the same workflow)
             *     - "{{workflow.input.your-property-name}}" for the current workflow's input
             *     - "{{global.your-property-name}}" for the global context
             *
             *     You can also talk about the current step's output or input directly:
             *     - "{{output.your-property-name}} is greater than 10"
             *     - "{{input.your-property-name}} is greater than 10"
             *
             *     Examples:
             *      - "{{input.age}} is greater than 10"
             *      - "{{input.age}} is greater than {{input.age2}}"
             *      - "{{output.age}} is greater than 10" */
            instruction: string;
        };
        BlockStartMessage: {
            /** @description This is an optional array of conditions that must be met for this message to be triggered. */
            conditions?: (components["schemas"]["ModelBasedCondition"] | components["schemas"]["RuleBasedCondition"])[];
            /**
             * @description This is the message type that is triggered when the block starts.
             * @enum {string}
             */
            type: "block-start";
            /** @description This is the content that the assistant will say when this message is triggered. */
            content: string;
        };
        BlockCompleteMessage: {
            /** @description This is an optional array of conditions that must be met for this message to be triggered. */
            conditions?: (components["schemas"]["ModelBasedCondition"] | components["schemas"]["RuleBasedCondition"])[];
            /**
             * @description This is the message type that is triggered when the block completes.
             * @enum {string}
             */
            type: "block-complete";
            /** @description This is the content that the assistant will say when this message is triggered. */
            content: string;
        };
        CreateConversationBlockDTO: {
            /** @description These are the pre-configured messages that will be spoken to the user while the block is running. */
            messages?: (components["schemas"]["BlockStartMessage"] | components["schemas"]["BlockCompleteMessage"])[];
            /** @description This is the input schema for the block. This is the input the block needs to run. It's given to the block as `steps[0].input`
             *
             *     These are accessible as variables:
             *     - ({{input.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.input.propertyName}}) in context of the workflow */
            inputSchema?: components["schemas"]["JsonSchema"];
            /** @description This is the output schema for the block. This is the output the block will return to the workflow (`{{stepName.output}}`).
             *
             *     These are accessible as variables:
             *     - ({{output.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.output.propertyName}}) in context of the workflow */
            outputSchema?: components["schemas"]["JsonSchema"];
            /**
             * @description This block is used for conversation. This can be a free flow conversation or a conversation with a specific goal like collecting some information.
             *
             *     For free block conversation, put clearly in the `instruction` when the block can be considered done.
             *     ```
             *     {
             *      "type": "conversation",
             *      "instruction": "Chit chat with the user asking them about their day. When user asks a specific question or once you have talked to the user for a couple of turns of conversation, move on."
             *     }
             *     ```
             *
             *     For conversation with a specific goal, you can define an `outputSchema` with required fields. The block won't be considered done until the user has provided all the required fields in the `outputSchema`.
             *     ```
             *     {
             *      "type": "conversation",
             *      "instruction": "Ask the user about their hobbies, hopes and dreams.",
             *      "outputSchema": {
             *        "type": "object",
             *        "properties": {
             *          "hobbies": {
             *            "type": "string"
             *          },
             *          "hopes": {
             *            "type": "string"
             *          },
             *          "dreams": {
             *            "type": "string"
             *          }
             *        },
             *        "required": ["hobbies"]
             *      }
             *     }
             *     ```
             *     For the above example, the conversation block will be considered done once the user has provided the `hobbies` (even if they have not provided the `hopes` and `dreams`). (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "conversation";
            /** @description This is the instruction to the model.
             *
             *     You can reference any variable in the context of the current block execution (step):
             *     - "{{input.your-property-name}}" for the current step's input
             *     - "{{your-step-name.output.your-property-name}}" for another step's output (in the same workflow)
             *     - "{{your-step-name.input.your-property-name}}" for another step's input (in the same workflow)
             *     - "{{workflow.input.your-property-name}}" for the current workflow's input
             *     - "{{global.your-property-name}}" for the global context
             *
             *     This can be as simple or as complex as you want it to be.
             *     - "say hello and ask the user about their day!"
             *     - "collect the user's first and last name"
             *     - "user is {{input.firstName}} {{input.lastName}}. their age is {{input.age}}. ask them about their salary and if they might be interested in buying a house. we offer {{input.offer}}" */
            instruction: string;
            /** @description This is the name of the block. This is just for your reference. */
            name?: string;
        };
        CreateToolCallBlockDTO: {
            /** @description These are the pre-configured messages that will be spoken to the user while the block is running. */
            messages?: (components["schemas"]["BlockStartMessage"] | components["schemas"]["BlockCompleteMessage"])[];
            /** @description This is the input schema for the block. This is the input the block needs to run. It's given to the block as `steps[0].input`
             *
             *     These are accessible as variables:
             *     - ({{input.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.input.propertyName}}) in context of the workflow */
            inputSchema?: components["schemas"]["JsonSchema"];
            /** @description This is the output schema for the block. This is the output the block will return to the workflow (`{{stepName.output}}`).
             *
             *     These are accessible as variables:
             *     - ({{output.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.output.propertyName}}) in context of the workflow */
            outputSchema?: components["schemas"]["JsonSchema"];
            /**
             * @description This block makes a tool call. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "tool-call";
            /** @description This is the tool that the block will call. To use an existing tool, use `toolId`. */
            tool?: components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"];
            /** @description This is the id of the tool that the block will call. To use a transient tool, use `tool`. */
            toolId?: string;
            /** @description This is the name of the block. This is just for your reference. */
            name?: string;
        };
        StepDestination: {
            /** @enum {string} */
            type: "step";
            /** @description This is an optional array of conditions that must be met for this destination to be triggered. If empty, this is the default destination that the step transfers to. */
            conditions?: (components["schemas"]["ModelBasedCondition"] | components["schemas"]["RuleBasedCondition"])[];
            stepName: string;
        };
        HandoffStep: {
            /** @description This is the block to use. To use an existing block, use `blockId`. */
            block?: components["schemas"]["CreateConversationBlockDTO"] | components["schemas"]["CreateToolCallBlockDTO"] | components["schemas"]["CreateWorkflowBlockDTO"];
            /**
             * @description This is a step that takes a handoff from the previous step. This means it won't return to the calling step. The workflow execution will continue linearly.
             *
             *     Use case:
             *     - You want to collect information linearly (e.g. a form, provide information, etc).
             * @enum {string}
             */
            type: "handoff";
            /** @description These are the destinations that the step can go to after it's done. */
            destinations?: components["schemas"]["StepDestination"][];
            /** @description This is the name of the step. */
            name: string;
            /** @description This is the id of the block to use. To use a transient block, use `block`. */
            blockId?: string;
            /** @description This is the input to the block. You can use any key-value map as input to the block.
             *
             *     Example:
             *     {
             *       "name": "John Doe",
             *       "age": 20
             *     }
             *
             *     You can reference any variable in the context of the current block:
             *     - "{{your-step-name.output.your-property-name}}" for another step's output (in the same workflow)
             *     - "{{your-step-name.input.your-property-name}}" for another step's input (in the same workflow)
             *     - "{{workflow.input.your-property-name}}" for the current workflow's input
             *     - "{{global.your-property-name}}" for the global context
             *
             *     Example:
             *     {
             *       "name": "{{my-tool-call-step.output.name}}",
             *       "age": "{{my-tool-call-step.input.age}}",
             *       "date": "{{workflow.input.date}}"
             *     }
             *
             *     You can dynamically change the key name.
             *
             *     Example:
             *     {
             *       "{{my-tool-call-step.output.key-name-for-name}}": "{{name}}",
             *       "{{my-tool-call-step.input.key-name-for-age}}": "{{age}}",
             *       "{{workflow.input.key-name-for-date}}": "{{date}}"
             *     }
             *
             *     You can represent the value as a string, number, boolean, array, or object.
             *
             *     Example:
             *     {
             *       "name": "john",
             *       "age": 20,
             *       "date": "2021-01-01",
             *       "metadata": {
             *         "unique-key": "{{my-tool-call-step.output.unique-key}}"
             *       },
             *       "array": ["A", "B", "C"],
             *     } */
            input?: Record<string, never>;
        };
        AssignmentMutation: {
            /** @description This is an optional array of conditions that must be met for this mutation to be triggered. */
            conditions?: (components["schemas"]["ModelBasedCondition"] | components["schemas"]["RuleBasedCondition"])[];
            /**
             * @description This mutation assigns a new value to an existing or new variable.
             * @enum {string}
             */
            type: "assignment";
            /** @description This is the variable to assign a new value to.
             *
             *     You can reference any variable in the context of the current block execution (step):
             *     - "output.your-property-name" for current step's output
             *     - "your-step-name.output.your-property-name" for another step's output (in the same workflow)
             *     - "global.your-property-name" for the global context
             *
             *     This needs to be the key path of the variable. If you use {{}}, it'll dereference that to the value of the variable before assignment. This can be useful if the path is dynamic. Example:
             *     - "global.{{my-tool-call-step.output.my-key-name}}"
             *
             *     You can also string interpolate multiple variables to get the key name:
             *     - "global.{{my-tool-call-step.output.my-key-name-suffix}}-{{my-tool-call-step.output.my-key-name}}"
             *
             *     The path to the new variable is created if it doesn't exist. Example:
             *     - "global.this-does-not-exist.neither-does-this" will create `this-does-not-exist` object with `neither-does-this` as a key */
            variable: string;
            /** @description The value to assign to the variable.
             *
             *     You can reference any variable in the context of the current block execution (step):
             *     - "{{output.your-property-name}}" for current step's output
             *     - "{{your-step-name.output.your-property-name}}" for another step's output (in the same workflow)
             *     - "{{global.your-property-name}}" for the global context
             *
             *     Or, you can use a constant:
             *     - "1"
             *     - "text"
             *     - "true"
             *     - "false"
             *
             *     Or, you can mix and match with string interpolation:
             *     - "{{your-property-name}}-{{input.your-property-name-2}}-1" */
            value: string;
        };
        CallbackStep: {
            /** @description This is the block to use. To use an existing block, use `blockId`. */
            block?: components["schemas"]["CreateConversationBlockDTO"] | components["schemas"]["CreateToolCallBlockDTO"] | components["schemas"]["CreateWorkflowBlockDTO"];
            /**
             * @description This is a step that calls back to the previous step after it's done. This effectively means we're spawning a new conversation thread. The previous conversation thread will resume where it left off once this step is done.
             *
             *     Use case:
             *     - You are collecting a customer's order and while they were on one item, they start a new item or try to modify a previous one. You would make a OrderUpdate block which calls the same block repeatedly when a new update starts.
             * @enum {string}
             */
            type: "callback";
            /** @description This is the mutations to apply to the context after the step is done. */
            mutations?: components["schemas"]["AssignmentMutation"][];
            /** @description This is the name of the step. */
            name: string;
            /** @description This is the id of the block to use. To use a transient block, use `block`. */
            blockId?: string;
            /** @description This is the input to the block. You can use any key-value map as input to the block.
             *
             *     Example:
             *     {
             *       "name": "John Doe",
             *       "age": 20
             *     }
             *
             *     You can reference any variable in the context of the current block:
             *     - "{{your-step-name.output.your-property-name}}" for another step's output (in the same workflow)
             *     - "{{your-step-name.input.your-property-name}}" for another step's input (in the same workflow)
             *     - "{{workflow.input.your-property-name}}" for the current workflow's input
             *     - "{{global.your-property-name}}" for the global context
             *
             *     Example:
             *     {
             *       "name": "{{my-tool-call-step.output.name}}",
             *       "age": "{{my-tool-call-step.input.age}}",
             *       "date": "{{workflow.input.date}}"
             *     }
             *
             *     You can dynamically change the key name.
             *
             *     Example:
             *     {
             *       "{{my-tool-call-step.output.key-name-for-name}}": "{{name}}",
             *       "{{my-tool-call-step.input.key-name-for-age}}": "{{age}}",
             *       "{{workflow.input.key-name-for-date}}": "{{date}}"
             *     }
             *
             *     You can represent the value as a string, number, boolean, array, or object.
             *
             *     Example:
             *     {
             *       "name": "john",
             *       "age": 20,
             *       "date": "2021-01-01",
             *       "metadata": {
             *         "unique-key": "{{my-tool-call-step.output.unique-key}}"
             *       },
             *       "array": ["A", "B", "C"],
             *     } */
            input?: Record<string, never>;
        };
        CreateWorkflowBlockDTO: {
            /** @description These are the pre-configured messages that will be spoken to the user while the block is running. */
            messages?: (components["schemas"]["BlockStartMessage"] | components["schemas"]["BlockCompleteMessage"])[];
            /** @description This is the input schema for the block. This is the input the block needs to run. It's given to the block as `steps[0].input`
             *
             *     These are accessible as variables:
             *     - ({{input.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.input.propertyName}}) in context of the workflow */
            inputSchema?: components["schemas"]["JsonSchema"];
            /** @description This is the output schema for the block. This is the output the block will return to the workflow (`{{stepName.output}}`).
             *
             *     These are accessible as variables:
             *     - ({{output.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.output.propertyName}}) in context of the workflow */
            outputSchema?: components["schemas"]["JsonSchema"];
            /**
             * @description This creates a workflow which can contain any number of steps (block executions). (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "workflow";
            /** @description These are the steps in the workflow. */
            steps?: (components["schemas"]["HandoffStep"] | components["schemas"]["CallbackStep"])[];
            /** @description This is the name of the block. This is just for your reference. */
            name?: string;
        };
        UpdateBlockDTO: {
            /** @description These are the pre-configured messages that will be spoken to the user while the block is running. */
            messages?: (components["schemas"]["BlockStartMessage"] | components["schemas"]["BlockCompleteMessage"])[];
            /** @description This is the input schema for the block. This is the input the block needs to run. It's given to the block as `steps[0].input`
             *
             *     These are accessible as variables:
             *     - ({{input.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.input.propertyName}}) in context of the workflow */
            inputSchema?: components["schemas"]["JsonSchema"];
            /** @description This is the output schema for the block. This is the output the block will return to the workflow (`{{stepName.output}}`).
             *
             *     These are accessible as variables:
             *     - ({{output.propertyName}}) in context of the block execution (step)
             *     - ({{stepName.output.propertyName}}) in context of the workflow */
            outputSchema?: components["schemas"]["JsonSchema"];
            /** @description This is the tool that the block will call. To use an existing tool, use `toolId`. */
            tool?: components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"];
            /** @description These are the steps in the workflow. */
            steps?: (components["schemas"]["HandoffStep"] | components["schemas"]["CallbackStep"])[];
            /** @description This is the name of the block. This is just for your reference. */
            name?: string;
            /** @description This is the instruction to the model.
             *
             *     You can reference any variable in the context of the current block execution (step):
             *     - "{{input.your-property-name}}" for the current step's input
             *     - "{{your-step-name.output.your-property-name}}" for another step's output (in the same workflow)
             *     - "{{your-step-name.input.your-property-name}}" for another step's input (in the same workflow)
             *     - "{{workflow.input.your-property-name}}" for the current workflow's input
             *     - "{{global.your-property-name}}" for the global context
             *
             *     This can be as simple or as complex as you want it to be.
             *     - "say hello and ask the user about their day!"
             *     - "collect the user's first and last name"
             *     - "user is {{input.firstName}} {{input.lastName}}. their age is {{input.age}}. ask them about their salary and if they might be interested in buying a house. we offer {{input.offer}}" */
            instruction?: string;
            /** @description This is the id of the tool that the block will call. To use a transient tool, use `tool`. */
            toolId?: string;
        };
        DtmfTool: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "dtmf" for DTMF tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "dtmf";
            /** @description This is the unique identifier for the tool. */
            id: string;
            /** @description This is the unique identifier for the organization that this tool belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was last updated.
             */
            updatedAt: string;
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        EndCallTool: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "endCall" for End Call tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "endCall";
            /** @description This is the unique identifier for the tool. */
            id: string;
            /** @description This is the unique identifier for the organization that this tool belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was last updated.
             */
            updatedAt: string;
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        FunctionTool: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "function" for Function tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "function";
            /** @description This is the unique identifier for the tool. */
            id: string;
            /** @description This is the unique identifier for the organization that this tool belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was last updated.
             */
            updatedAt: string;
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        GhlTool: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "ghl" for GHL tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "ghl";
            /** @description This is the unique identifier for the tool. */
            id: string;
            /** @description This is the unique identifier for the organization that this tool belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was last updated.
             */
            updatedAt: string;
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
            metadata: components["schemas"]["GhlToolMetadata"];
        };
        MakeTool: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "make" for Make tool. (enum property replaced by openapi-typescript)
             * @enum {string}
             */
            type: "make";
            /** @description This is the unique identifier for the tool. */
            id: string;
            /** @description This is the unique identifier for the organization that this tool belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was last updated.
             */
            updatedAt: string;
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
            metadata: components["schemas"]["MakeToolMetadata"];
        };
        TransferCallTool: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description discriminator enum property added by openapi-typescript
             * @enum {string}
             */
            type: "transferCall";
            /** @description These are the destinations that the call can be transferred to. If no destinations are provided, server.url will be used to get the transfer destination once the tool is called. */
            destinations?: (components["schemas"]["TransferDestinationAssistant"] | components["schemas"]["TransferDestinationStep"] | components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"])[];
            /** @description This is the unique identifier for the tool. */
            id: string;
            /** @description This is the unique identifier for the organization that this tool belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the tool was last updated.
             */
            updatedAt: string;
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        UpdateToolDTO: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        CreateFileDTO: {
            /**
             * Format: binary
             * @description This is the File you want to upload for use with the Knowledge Base.
             */
            file: string;
        };
        File: {
            /** @enum {string} */
            object?: "file";
            /** @enum {string} */
            status?: "indexed" | "not_indexed";
            /** @description This is the name of the file. This is just for your own reference. */
            name?: string;
            originalName?: string;
            bytes?: number;
            purpose?: string;
            mimetype?: string;
            key?: string;
            path?: string;
            bucket?: string;
            url?: string;
            metadata?: Record<string, never>;
            /** @description This is the unique identifier for the file. */
            id: string;
            /** @description This is the unique identifier for the org that this file belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the file was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the file was last updated.
             */
            updatedAt: string;
        };
        UpdateFileDTO: {
            /** @description This is the name of the file. This is just for your own reference. */
            name?: string;
        };
        Metrics: {
            orgId: string;
            rangeStart: string;
            rangeEnd: string;
            bill: number;
            billWithinBillingLimit: boolean;
            billDailyBreakdown: Record<string, never>;
            callActive: number;
            callActiveWithinConcurrencyLimit: boolean;
            callMinutes: number;
            callMinutesDailyBreakdown: Record<string, never>;
            callMinutesAverage: number;
            callMinutesAverageDailyBreakdown: Record<string, never>;
            callCount: number;
            callCountDailyBreakdown: Record<string, never>;
        };
        TimeRange: {
            /**
             * @description This is the time step for aggregations.
             *
             *     If not provided, defaults to returning for the entire time range.
             * @enum {string}
             */
            step?: "minute" | "hour" | "day" | "week" | "month" | "quarter" | "year" | "decade" | "century" | "millennium";
            /**
             * Format: date-time
             * @description This is the start date for the time range.
             *
             *     If not provided, defaults to the 7 days ago.
             */
            start?: string;
            /**
             * Format: date-time
             * @description This is the end date for the time range.
             *
             *     If not provided, defaults to now.
             */
            end?: string;
            /** @description This is the timezone you want to set for the query.
             *
             *     If not provided, defaults to UTC. */
            timezone?: string;
        };
        AnalyticsOperation: {
            /**
             * @description This is the aggregation operation you want to perform.
             * @enum {string}
             */
            operation: "sum" | "avg" | "count" | "min" | "max";
            /**
             * @description This is the columns you want to perform the aggregation operation on.
             * @enum {string}
             */
            column: "id" | "cost" | "costBreakdown.llm" | "costBreakdown.stt" | "costBreakdown.tts" | "costBreakdown.vapi" | "costBreakdown.ttsCharacters" | "costBreakdown.llmPromptTokens" | "costBreakdown.llmCompletionTokens" | "duration";
            /** @description This is the alias for column name returned. Defaults to `${operation}${column}`. */
            alias?: string;
        };
        AnalyticsQuery: {
            /**
             * @description This is the table you want to query.
             * @enum {string}
             */
            table: "call";
            /**
             * @description This is the list of columns you want to group by.
             * @enum {string}
             */
            groupBy?: "type" | "assistantId" | "endedReason" | "analysis.successEvaluation" | "status";
            /** @description This is the name of the query. This will be used to identify the query in the response. */
            name: string;
            /** @description This is the time range for the query. */
            timeRange?: components["schemas"]["TimeRange"];
            /** @description This is the list of operations you want to perform. */
            operations: components["schemas"]["AnalyticsOperation"][];
        };
        AnalyticsQueryDTO: {
            /** @description This is the list of metric queries you want to perform. */
            queries: components["schemas"]["AnalyticsQuery"][];
        };
        AnalyticsQueryResult: {
            /** @description This is the unique key for the query. */
            name: string;
            /** @description This is the time range for the query. */
            timeRange: components["schemas"]["TimeRange"];
            /** @description This is the result of the query, a list of unique groups with result of their aggregations.
             *
             *     Example:
             *     "result": [
             *       { "date": "2023-01-01", "assistantId": "123", "endedReason": "customer-ended-call", "sumDuration": 120, "avgCost": 10.5 },
             *       { "date": "2023-01-02", "assistantId": "123", "endedReason": "customer-did-not-give-microphone-permission", "sumDuration": 0, "avgCost": 0 },
             *       // Additional results
             *     ] */
            result: Record<string, never>[];
        };
        CallLogPrivileged: {
            /** @description This is the unique identifier for the call. */
            callId: string;
            /** @description This is the unique identifier for the org that this call log belongs to. */
            orgId: string;
            /** @description This is the log message associated with the call. */
            log: string;
            /**
             * @description This is the level of the log message.
             * @enum {string}
             */
            level: "INFO" | "LOG" | "WARN" | "ERROR" | "CHECKPOINT";
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the log was created.
             */
            time: string;
        };
        CallLogsPaginatedResponse: {
            results: components["schemas"]["CallLogPrivileged"][];
            metadata: components["schemas"]["PaginationMeta"];
        };
        AnyscaleCredential: {
            /** @enum {string} */
            provider: "anyscale";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        AnthropicCredential: {
            /** @enum {string} */
            provider: "anthropic";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        AzureOpenAICredential: {
            /** @enum {string} */
            provider: "azure-openai";
            /** @enum {string} */
            region: "australia" | "canada" | "eastus2" | "eastus" | "france" | "india" | "japan" | "northcentralus" | "norway" | "southcentralus" | "sweden" | "switzerland" | "uk" | "westus" | "westus3";
            /**
             * @example [
             *       "gpt-4-0125-preview",
             *       "gpt-4-0613"
             *     ]
             * @enum {string}
             */
            models: "gpt-4o-2024-05-13-global";
            /** @description This is not returned in the API. */
            openAIKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
            openAIEndpoint: string;
        };
        SipTrunkGateway: {
            /** @description This is the IPv4 address of the gateway. */
            ip: string;
            /** @description This is the port number of the gateway. Default is 5060.
             *
             *     @default 5060 */
            port?: number;
            /** @description This is the netmask of the gateway. Defaults to 32.
             *
             *     @default 32 */
            netmask?: number;
            /** @description This is whether inbound calls are allowed from this gateway. Default is true.
             *
             *     @default true */
            inboundEnabled?: boolean;
            /** @description This is whether outbound calls should be sent to this gateway. Default is true.
             *
             *     Note, if netmask is less than 32, it doesn't affect the outbound IPs that are tried. 1 attempt is made to `ip:port`.
             *
             *     @default true */
            outboundEnabled?: boolean;
            /**
             * @description This is the protocol to use for SIP signaling outbound calls. Default is udp.
             *
             *     @default udp
             * @enum {string}
             */
            outboundProtocol?: "tls/srtp" | "tcp" | "tls" | "udp";
            /** @description This is whether to send options ping to the gateway. This can be used to check if the gateway is reachable. Default is false.
             *
             *     This is useful for high availability setups where you want to check if the gateway is reachable before routing calls to it. Note, if no gateway for a trunk is reachable, outbound calls will be rejected.
             *
             *     @default false */
            optionsPingEnabled?: boolean;
        };
        SipTrunkOutboundSipRegisterPlan: {
            domain?: string;
            username?: string;
            realm?: string;
        };
        SipTrunkOutboundAuthenticationPlan: {
            /** @description This is not returned in the API. */
            authPassword?: string;
            authUsername?: string;
            /** @description This can be used to configure if SIP register is required by the SIP trunk. If not provided, no SIP registration will be attempted. */
            sipRegisterPlan?: components["schemas"]["SipTrunkOutboundSipRegisterPlan"];
        };
        SbcConfiguration: Record<string, never>;
        ByoSipTrunkCredential: {
            /**
             * @description This can be used to bring your own SIP trunks or to connect to a Carrier.
             * @enum {string}
             */
            provider?: "byo-sip-trunk";
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
            /** @description This is the list of SIP trunk's gateways. */
            gateways: components["schemas"]["SipTrunkGateway"][];
            /** @description This is the name of the SIP trunk. This is just for your reference. */
            name?: string;
            /** @description This can be used to configure the outbound authentication if required by the SIP trunk. */
            outboundAuthenticationPlan?: components["schemas"]["SipTrunkOutboundAuthenticationPlan"];
            /** @description This ensures the outbound origination attempts have a leading plus. Defaults to false to match conventional telecom behavior. */
            outboundLeadingPlusEnabled?: boolean;
            /** @description This is an advanced configuration for enterprise deployments. This uses the onprem SBC to trunk into the SIP trunk's `gateways`, rather than the managed SBC provided by Vapi. */
            sbcConfiguration?: components["schemas"]["SbcConfiguration"];
        };
        CartesiaCredential: {
            /** @enum {string} */
            provider: "cartesia";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        CustomLLMCredential: {
            /** @enum {string} */
            provider: "custom-llm";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        DeepgramCredential: {
            /** @enum {string} */
            provider: "deepgram";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        GladiaCredential: {
            /** @enum {string} */
            provider: "gladia";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        DeepInfraCredential: {
            /** @enum {string} */
            provider: "deepinfra";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        ElevenLabsCredential: {
            /** @enum {string} */
            provider: "11labs";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        GoHighLevelCredential: {
            /** @enum {string} */
            provider: "gohighlevel";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        GroqCredential: {
            /** @enum {string} */
            provider: "groq";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        LmntCredential: {
            /** @enum {string} */
            provider: "lmnt";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        MakeCredential: {
            /** @enum {string} */
            provider: "make";
            /** @description Team ID */
            teamId: string;
            /** @description Region of your application. For example: eu1, eu2, us1, us2 */
            region: string;
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        OpenAICredential: {
            /** @enum {string} */
            provider: "openai";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        OpenRouterCredential: {
            /** @enum {string} */
            provider: "openrouter";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        PerplexityAICredential: {
            /** @enum {string} */
            provider: "perplexity-ai";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        PlayHTCredential: {
            /** @enum {string} */
            provider: "playht";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
            userId: string;
        };
        RimeAICredential: {
            /** @enum {string} */
            provider: "rime-ai";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        RunpodCredential: {
            /** @enum {string} */
            provider: "runpod";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        S3Credential: {
            /**
             * @description Credential provider. Only allowed value is s3
             * @enum {string}
             */
            provider: "s3";
            /** @description AWS access key ID. */
            awsAccessKeyId: string;
            /** @description AWS access key secret. This is not returned in the API. */
            awsSecretAccessKey: string;
            /** @description AWS region in which the S3 bucket is located. */
            region: string;
            /** @description AWS S3 bucket name. */
            s3BucketName: string;
            /** @description The path prefix for the uploaded recording. Ex. "recordings/" */
            s3PathPrefix: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        TogetherAICredential: {
            /** @enum {string} */
            provider: "together-ai";
            /** @description This is not returned in the API. */
            apiKey: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
        };
        TwilioCredential: {
            /** @enum {string} */
            provider: "twilio";
            /** @description This is not returned in the API. */
            authToken: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
            accountSid: string;
        };
        VonageCredential: {
            /** @description This is not returned in the API. */
            vonageApplicationPrivateKey: string;
            /** @enum {string} */
            provider: "vonage";
            /** @description This is not returned in the API. */
            apiSecret: string;
            /** @description This is the unique identifier for the credential. */
            id: string;
            /** @description This is the unique identifier for the org that this credential belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the credential was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the assistant was last updated.
             */
            updatedAt: string;
            /** @description This is the Vonage Application ID for the credential.
             *
             *     Only relevant for Vonage credentials. */
            vonageApplicationId: string;
            apiKey: string;
        };
        CreateAnyscaleCredentialDTO: {
            /** @enum {string} */
            provider: "anyscale";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateAnthropicCredentialDTO: {
            /** @enum {string} */
            provider: "anthropic";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateAzureOpenAICredentialDTO: {
            /** @enum {string} */
            provider: "azure-openai";
            /** @enum {string} */
            region: "australia" | "canada" | "eastus2" | "eastus" | "france" | "india" | "japan" | "northcentralus" | "norway" | "southcentralus" | "sweden" | "switzerland" | "uk" | "westus" | "westus3";
            /**
             * @example [
             *       "gpt-4-0125-preview",
             *       "gpt-4-0613"
             *     ]
             * @enum {string}
             */
            models: "gpt-4o-2024-05-13-global";
            /** @description This is not returned in the API. */
            openAIKey: string;
            openAIEndpoint: string;
        };
        CreateByoSipTrunkCredentialDTO: {
            /**
             * @description This can be used to bring your own SIP trunks or to connect to a Carrier.
             * @enum {string}
             */
            provider?: "byo-sip-trunk";
            /** @description This is the list of SIP trunk's gateways. */
            gateways: components["schemas"]["SipTrunkGateway"][];
            /** @description This is the name of the SIP trunk. This is just for your reference. */
            name?: string;
            /** @description This can be used to configure the outbound authentication if required by the SIP trunk. */
            outboundAuthenticationPlan?: components["schemas"]["SipTrunkOutboundAuthenticationPlan"];
            /** @description This ensures the outbound origination attempts have a leading plus. Defaults to false to match conventional telecom behavior. */
            outboundLeadingPlusEnabled?: boolean;
            /** @description This is an advanced configuration for enterprise deployments. This uses the onprem SBC to trunk into the SIP trunk's `gateways`, rather than the managed SBC provided by Vapi. */
            sbcConfiguration?: components["schemas"]["SbcConfiguration"];
        };
        CreateCartesiaCredentialDTO: {
            /** @enum {string} */
            provider: "cartesia";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateCustomLLMCredentialDTO: {
            /** @enum {string} */
            provider: "custom-llm";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateDeepgramCredentialDTO: {
            /** @enum {string} */
            provider: "deepgram";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateGladiaCredentialDTO: {
            /** @enum {string} */
            provider: "gladia";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateDeepInfraCredentialDTO: {
            /** @enum {string} */
            provider: "deepinfra";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateElevenLabsCredentialDTO: {
            /** @enum {string} */
            provider: "11labs";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateGoHighLevelCredentialDTO: {
            /** @enum {string} */
            provider: "gohighlevel";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateGroqCredentialDTO: {
            /** @enum {string} */
            provider: "groq";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateLmntCredentialDTO: {
            /** @enum {string} */
            provider: "lmnt";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateMakeCredentialDTO: {
            /** @enum {string} */
            provider: "make";
            /** @description Team ID */
            teamId: string;
            /** @description Region of your application. For example: eu1, eu2, us1, us2 */
            region: string;
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateOpenAICredentialDTO: {
            /** @enum {string} */
            provider: "openai";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateOpenRouterCredentialDTO: {
            /** @enum {string} */
            provider: "openrouter";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreatePerplexityAICredentialDTO: {
            /** @enum {string} */
            provider: "perplexity-ai";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreatePlayHTCredentialDTO: {
            /** @enum {string} */
            provider: "playht";
            /** @description This is not returned in the API. */
            apiKey: string;
            userId: string;
        };
        CreateRimeAICredentialDTO: {
            /** @enum {string} */
            provider: "rime-ai";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateRunpodCredentialDTO: {
            /** @enum {string} */
            provider: "runpod";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateS3CredentialDTO: {
            /**
             * @description Credential provider. Only allowed value is s3
             * @enum {string}
             */
            provider: "s3";
            /** @description AWS access key ID. */
            awsAccessKeyId: string;
            /** @description AWS access key secret. This is not returned in the API. */
            awsSecretAccessKey: string;
            /** @description AWS region in which the S3 bucket is located. */
            region: string;
            /** @description AWS S3 bucket name. */
            s3BucketName: string;
            /** @description The path prefix for the uploaded recording. Ex. "recordings/" */
            s3PathPrefix: string;
        };
        CreateTogetherAICredentialDTO: {
            /** @enum {string} */
            provider: "together-ai";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        CreateTwilioCredentialDTO: {
            /** @enum {string} */
            provider: "twilio";
            /** @description This is not returned in the API. */
            authToken: string;
            accountSid: string;
        };
        CreateVonageCredentialDTO: {
            /** @enum {string} */
            provider: "vonage";
            /** @description This is not returned in the API. */
            apiSecret: string;
            apiKey: string;
        };
        UpdateAnyscaleCredentialDTO: {
            /** @enum {string} */
            provider: "anyscale";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateAnthropicCredentialDTO: {
            /** @enum {string} */
            provider: "anthropic";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateAzureOpenAICredentialDTO: {
            /** @enum {string} */
            provider: "azure-openai";
            /** @enum {string} */
            region: "australia" | "canada" | "eastus2" | "eastus" | "france" | "india" | "japan" | "northcentralus" | "norway" | "southcentralus" | "sweden" | "switzerland" | "uk" | "westus" | "westus3";
            /**
             * @example [
             *       "gpt-4-0125-preview",
             *       "gpt-4-0613"
             *     ]
             * @enum {string}
             */
            models: "gpt-4o-2024-05-13-global";
            /** @description This is not returned in the API. */
            openAIKey: string;
            openAIEndpoint: string;
        };
        UpdateByoSipTrunkCredentialDTO: {
            /**
             * @description This can be used to bring your own SIP trunks or to connect to a Carrier.
             * @enum {string}
             */
            provider?: "byo-sip-trunk";
            /** @description This is the list of SIP trunk's gateways. */
            gateways: components["schemas"]["SipTrunkGateway"][];
            /** @description This is the name of the SIP trunk. This is just for your reference. */
            name?: string;
            /** @description This can be used to configure the outbound authentication if required by the SIP trunk. */
            outboundAuthenticationPlan?: components["schemas"]["SipTrunkOutboundAuthenticationPlan"];
            /** @description This ensures the outbound origination attempts have a leading plus. Defaults to false to match conventional telecom behavior. */
            outboundLeadingPlusEnabled?: boolean;
            /** @description This is an advanced configuration for enterprise deployments. This uses the onprem SBC to trunk into the SIP trunk's `gateways`, rather than the managed SBC provided by Vapi. */
            sbcConfiguration?: components["schemas"]["SbcConfiguration"];
        };
        UpdateCartesiaCredentialDTO: {
            /** @enum {string} */
            provider: "cartesia";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateCustomLLMCredentialDTO: {
            /** @enum {string} */
            provider: "custom-llm";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateDeepInfraCredentialDTO: {
            /** @enum {string} */
            provider: "deepinfra";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateDeepgramCredentialDTO: {
            /** @enum {string} */
            provider: "deepgram";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateGladiaCredentialDTO: {
            /** @enum {string} */
            provider: "gladia";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateElevenLabsCredentialDTO: {
            /** @enum {string} */
            provider: "11labs";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateGoHighLevelCredentialDTO: {
            /** @enum {string} */
            provider: "gohighlevel";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateGroqCredentialDTO: {
            /** @enum {string} */
            provider: "groq";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateLmntCredentialDTO: {
            /** @enum {string} */
            provider: "lmnt";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateMakeCredentialDTO: {
            /** @enum {string} */
            provider: "make";
            /** @description Team ID */
            teamId: string;
            /** @description Region of your application. For example: eu1, eu2, us1, us2 */
            region: string;
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateOpenAICredentialDTO: {
            /** @enum {string} */
            provider: "openai";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateOpenRouterCredentialDTO: {
            /** @enum {string} */
            provider: "openrouter";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdatePerplexityAICredentialDTO: {
            /** @enum {string} */
            provider: "perplexity-ai";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdatePlayHTCredentialDTO: {
            /** @enum {string} */
            provider: "playht";
            /** @description This is not returned in the API. */
            apiKey: string;
            userId: string;
        };
        UpdateRimeAICredentialDTO: {
            /** @enum {string} */
            provider: "rime-ai";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateRunpodCredentialDTO: {
            /** @enum {string} */
            provider: "runpod";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateS3CredentialDTO: {
            /**
             * @description Credential provider. Only allowed value is s3
             * @enum {string}
             */
            provider: "s3";
            /** @description AWS access key ID. */
            awsAccessKeyId: string;
            /** @description AWS access key secret. This is not returned in the API. */
            awsSecretAccessKey: string;
            /** @description AWS region in which the S3 bucket is located. */
            region: string;
            /** @description AWS S3 bucket name. */
            s3BucketName: string;
            /** @description The path prefix for the uploaded recording. Ex. "recordings/" */
            s3PathPrefix: string;
        };
        UpdateTogetherAICredentialDTO: {
            /** @enum {string} */
            provider: "together-ai";
            /** @description This is not returned in the API. */
            apiKey: string;
        };
        UpdateTwilioCredentialDTO: {
            /** @enum {string} */
            provider: "twilio";
            /** @description This is not returned in the API. */
            authToken: string;
            accountSid: string;
        };
        UpdateVonageCredentialDTO: {
            /** @enum {string} */
            provider: "vonage";
            /** @description This is not returned in the API. */
            apiSecret: string;
            apiKey: string;
        };
        CreateOrgDTO: {
            /**
             * @description When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false.
             *     When HIPAA is enabled, only OpenAI/Custom LLM or Azure Providers will be available for LLM and Voice respectively.
             *     This is due to the compliance requirements of HIPAA. Other providers may not meet these requirements.
             * @example false
             */
            hipaaEnabled?: boolean;
            /** @description This is the name of the org. This is just for your own reference. */
            name?: string;
            /** @description This is the monthly billing limit for the org. To go beyond $1000/mo, please contact us at support@vapi.ai. */
            billingLimit?: number;
            /** @description This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.
             *
             *     All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation. */
            serverUrl?: string;
            /** @description This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret. */
            serverUrlSecret?: string;
            /** @description This is the concurrency limit for the org. This is the maximum number of calls that can be active at any given time. To go beyond 10, please contact us at support@vapi.ai. */
            concurrencyLimit?: number;
        };
        OrgPlan: {
            includedProviders?: Record<string, never>[];
            includedMinutes?: number;
            costPerOverageMinute?: number;
        };
        Org: {
            /**
             * @description When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false.
             *     When HIPAA is enabled, only OpenAI/Custom LLM or Azure Providers will be available for LLM and Voice respectively.
             *     This is due to the compliance requirements of HIPAA. Other providers may not meet these requirements.
             * @example false
             */
            hipaaEnabled?: boolean;
            /** @description This is the unique identifier for the org. */
            id: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the org was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the org was last updated.
             */
            updatedAt: string;
            /** @description This is the Stripe customer for the org. */
            stripeCustomerId?: string;
            /** @description This is the subscription for the org. */
            stripeSubscriptionId?: string;
            /** @description This is the subscription's subscription item. */
            stripeSubscriptionItemId?: string;
            /**
             * Format: date-time
             * @description This is the subscription's current period start.
             */
            stripeSubscriptionCurrentPeriodStart?: string;
            /** @description This is the subscription's status. */
            stripeSubscriptionStatus?: string;
            /** @description This is the plan for the org. */
            plan?: components["schemas"]["OrgPlan"];
            /** @description This is the name of the org. This is just for your own reference. */
            name?: string;
            /** @description This is the monthly billing limit for the org. To go beyond $1000/mo, please contact us at support@vapi.ai. */
            billingLimit?: number;
            /** @description This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.
             *
             *     All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation. */
            serverUrl?: string;
            /** @description This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret. */
            serverUrlSecret?: string;
            /** @description This is the concurrency limit for the org. This is the maximum number of calls that can be active at any given time. To go beyond 10, please contact us at support@vapi.ai. */
            concurrencyLimit?: number;
        };
        UpdateOrgDTO: {
            /**
             * @description When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false.
             *     When HIPAA is enabled, only OpenAI/Custom LLM or Azure Providers will be available for LLM and Voice respectively.
             *     This is due to the compliance requirements of HIPAA. Other providers may not meet these requirements.
             * @example false
             */
            hipaaEnabled?: boolean;
            /** @description This is the name of the org. This is just for your own reference. */
            name?: string;
            /** @description This is the monthly billing limit for the org. To go beyond $1000/mo, please contact us at support@vapi.ai. */
            billingLimit?: number;
            /** @description This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.
             *
             *     All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation. */
            serverUrl?: string;
            /** @description This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret. */
            serverUrlSecret?: string;
            /** @description This is the concurrency limit for the org. This is the maximum number of calls that can be active at any given time. To go beyond 10, please contact us at support@vapi.ai. */
            concurrencyLimit?: number;
        };
        User: {
            /** @description This is the unique identifier for the profile or user. */
            id: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the profile was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the profile was last updated.
             */
            updatedAt: string;
            /** @description This is the email of the user that is associated with the profile. */
            email: string;
            /** @description This is the full name of the user that is associated with the profile. */
            fullName?: string;
        };
        InviteUserDTO: {
            email: string;
        };
        VoiceLibraryVoiceResponse: {
            voiceId: string;
            name: string;
            publicOwnerId?: string;
            description?: string;
            gender?: string;
            age?: Record<string, never>;
            accent?: string;
        };
        AddVoiceToProviderDTO: {
            /** @description This is the owner_id of your shared voice which you want to add to your provider Account from Provider Voice Library */
            ownerId: string;
            /** @description This is the voice_id of the shared voice which you want to add to your provider Account from Provider Voice Library */
            voiceId: string;
            /** @description This is the new name of the voice which you want to have once you have added voice to your provider Account from Provider Voice Library */
            name: string;
        };
        VoiceLibrary: {
            /**
             * @description This is the voice provider that will be used.
             * @enum {string}
             */
            provider?: "11labs" | "azure" | "cartesia" | "deepgram" | "lmnt" | "neets" | "openai" | "playht" | "rime-ai";
            /** @description The ID of the voice provided by the provider. */
            providerId?: string;
            /** @description The unique slug of the voice. */
            slug?: string;
            /** @description The name of the voice. */
            name?: string;
            /** @description The language of the voice. */
            language?: string;
            /** @description The language code of the voice. */
            languageCode?: string;
            /** @description The model of the voice. */
            model?: string;
            /** @description The supported models of the voice. */
            supportedModels?: string;
            /**
             * @description The gender of the voice.
             * @enum {string}
             */
            gender?: "male" | "female";
            /** @description The accent of the voice. */
            accent?: string;
            /** @description The preview URL of the voice. */
            previewUrl?: string;
            /** @description The description of the voice. */
            description?: string;
            /** @description The credential ID of the voice. */
            credentialId?: string;
            /** @description The unique identifier for the voice library. */
            id: string;
            /** @description The unique identifier for the organization that this voice library belongs to. */
            orgId: string;
            /** @description The Public voice is shared accross all the organizations. */
            isPublic: boolean;
            /** @description The deletion status of the voice. */
            isDeleted: boolean;
            /**
             * Format: date-time
             * @description The ISO 8601 date-time string of when the voice library was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description The ISO 8601 date-time string of when the voice library was last updated.
             */
            updatedAt: string;
        };
        CloneVoiceDTO: {
            /** @description This is the name of the cloned voice in the provider account. */
            name: string;
            /** @description This is the description of your cloned voice. */
            description?: string;
            /** @description Serialized labels dictionary for the voice. */
            labels?: string;
            /** @description These are the files you want to use to clone your voice. Only Audio files are supported. */
            files: string[];
        };
        ToolTemplateSetup: {
            title: string;
            description?: string;
            videoUrl?: string;
            docsUrl?: string;
        };
        MakeToolProviderDetails: {
            /** @description This is the Template URL or the Snapshot URL corresponding to the Template. */
            templateUrl?: string;
            setupInstructions?: components["schemas"]["ToolTemplateSetup"][];
            /**
             * @description The type of tool. "make" for Make tool.
             * @enum {string}
             */
            type: "make";
            scenarioId?: number;
            scenarioName?: string;
            triggerHookId?: number;
            triggerHookName?: string;
        };
        GhlToolProviderDetails: {
            /** @description This is the Template URL or the Snapshot URL corresponding to the Template. */
            templateUrl?: string;
            setupInstructions?: components["schemas"]["ToolTemplateSetup"][];
            /**
             * @description The type of tool. "ghl" for GHL tool.
             * @enum {string}
             */
            type: "ghl";
            workflowId?: string;
            workflowName?: string;
            webhookHookId?: string;
            webhookHookName?: string;
            locationId?: string;
        };
        FunctionToolProviderDetails: {
            /** @description This is the Template URL or the Snapshot URL corresponding to the Template. */
            templateUrl?: string;
            setupInstructions?: components["schemas"]["ToolTemplateSetup"][];
            /**
             * @description The type of tool. "function" for Function tool.
             * @enum {string}
             */
            type: "function";
        };
        ToolTemplateMetadata: {
            collectionType?: string;
            collectionId?: string;
            collectionName?: string;
        };
        CreateToolTemplateDTO: {
            details?: components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"];
            providerDetails?: components["schemas"]["MakeToolProviderDetails"] | components["schemas"]["GhlToolProviderDetails"] | components["schemas"]["FunctionToolProviderDetails"];
            metadata?: components["schemas"]["ToolTemplateMetadata"];
            /**
             * @default private
             * @enum {string}
             */
            visibility: "public" | "private";
            /**
             * @default tool
             * @enum {string}
             */
            type: "tool";
            /** @description The name of the template. This is just for your own reference. */
            name?: string;
            /** @enum {string} */
            provider?: "make" | "gohighlevel" | "function";
        };
        Template: {
            details?: components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"];
            providerDetails?: components["schemas"]["MakeToolProviderDetails"] | components["schemas"]["GhlToolProviderDetails"] | components["schemas"]["FunctionToolProviderDetails"];
            metadata?: components["schemas"]["ToolTemplateMetadata"];
            /**
             * @default private
             * @enum {string}
             */
            visibility: "public" | "private";
            /**
             * @default tool
             * @enum {string}
             */
            type: "tool";
            /** @description The name of the template. This is just for your own reference. */
            name?: string;
            /** @enum {string} */
            provider?: "make" | "gohighlevel" | "function";
            /** @description The unique identifier for the template. */
            id: string;
            /** @description The unique identifier for the organization that this template belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description The ISO 8601 date-time string of when the template was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description The ISO 8601 date-time string of when the template was last updated.
             */
            updatedAt: string;
        };
        UpdateToolTemplateDTO: {
            details?: components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateVoicemailToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"];
            providerDetails?: components["schemas"]["MakeToolProviderDetails"] | components["schemas"]["GhlToolProviderDetails"] | components["schemas"]["FunctionToolProviderDetails"];
            metadata?: components["schemas"]["ToolTemplateMetadata"];
            /**
             * @default private
             * @enum {string}
             */
            visibility: "public" | "private";
            /**
             * @default tool
             * @enum {string}
             */
            type: "tool";
            /** @description The name of the template. This is just for your own reference. */
            name?: string;
            /** @enum {string} */
            provider?: "make" | "gohighlevel" | "function";
        };
        TokenRestrictions: {
            /** @description This determines whether the token is enabled or disabled. Default is true, it's enabled. */
            enabled?: boolean;
            /** @description This determines the allowed origins for this token. Validates the `Origin` header. Default is any origin.
             *
             *     Only relevant for `public` tokens. */
            allowedOrigins?: string[];
            /** @description This determines which assistantIds can be used when creating a call. Default is any assistantId.
             *
             *     Only relevant for `public` tokens. */
            allowedAssistantIds?: string[];
            /** @description This determines whether transient assistants can be used when creating a call. Default is true.
             *
             *     If `allowedAssistantIds` is provided, this is automatically false.
             *
             *     Only relevant for `public` tokens. */
            allowTransientAssistant?: boolean;
        };
        CreateTokenDTO: {
            /**
             * @description This is the tag for the token. It represents its scope.
             * @enum {string}
             */
            tag?: "private" | "public";
            /** @description This is the name of the token. This is just for your own reference. */
            name?: string;
            /** @description This are the restrictions for the token. */
            restrictions?: components["schemas"]["TokenRestrictions"];
        };
        Token: {
            /**
             * @description This is the tag for the token. It represents its scope.
             * @enum {string}
             */
            tag?: "private" | "public";
            /** @description This is the unique identifier for the token. */
            id: string;
            /** @description This is unique identifier for the org that this token belongs to. */
            orgId: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the token was created.
             */
            createdAt: string;
            /**
             * Format: date-time
             * @description This is the ISO 8601 date-time string of when the token was last updated.
             */
            updatedAt: string;
            /** @description This is the token key. */
            value: string;
            /** @description This is the name of the token. This is just for your own reference. */
            name?: string;
            /** @description This are the restrictions for the token. */
            restrictions?: components["schemas"]["TokenRestrictions"];
        };
        SyncVoiceLibraryDTO: {
            /**
             * @description List of providers you want to sync.
             * @enum {string}
             */
            providers?: "11labs" | "azure" | "cartesia" | "deepgram" | "lmnt" | "neets" | "openai" | "playht" | "rime-ai";
        };
        ToolCallFunction: {
            /** @description This is the name of the function the model called. */
            name: string;
            /** @description These are the arguments that the function was called with. */
            arguments: Record<string, never>;
        };
        ToolCall: {
            /**
             * @description This is the type of tool the model called.
             * @enum {string}
             */
            type: "function";
            /** @description This is the function the model called. */
            function: components["schemas"]["ToolCallFunction"];
            /** @description This is the unique identifier for the tool call. */
            id: string;
        };
        FunctionToolWithToolCall: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "function" for Function tool.
             * @enum {string}
             */
            type: "function";
            toolCall: components["schemas"]["ToolCall"];
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        GhlToolWithToolCall: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "ghl" for GHL tool.
             * @enum {string}
             */
            type: "ghl";
            toolCall: components["schemas"]["ToolCall"];
            metadata: components["schemas"]["GhlToolMetadata"];
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        MakeToolWithToolCall: {
            /**
             * @description This determines if the tool is async.
             *
             *     If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.
             *
             *     If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.
             *
             *     Defaults to synchronous (`false`).
             * @example false
             */
            async?: boolean;
            /** @description These are the messages that will be spoken to the user as the tool is running.
             *
             *     For some tools, this is auto-filled based on special fields like `tool.destinations`. For others like the function tool, these can be custom configured. */
            messages?: (components["schemas"]["ToolMessageStart"] | components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"] | components["schemas"]["ToolMessageDelayed"])[];
            /**
             * @description The type of tool. "make" for Make tool.
             * @enum {string}
             */
            type: "make";
            toolCall: components["schemas"]["ToolCall"];
            metadata: components["schemas"]["MakeToolMetadata"];
            /** @description This is the function definition of the tool.
             *
             *     For `endCall`, `transferCall`, and `dtmf` tools, this is auto-filled based on tool-specific fields like `tool.destinations`. But, even in those cases, you can provide a custom function definition for advanced use cases.
             *
             *     An example of an advanced use case is if you want to customize the message that's spoken for `endCall` tool. You can specify a function where it returns an argument "reason". Then, in `messages` array, you can have many "request-complete" messages. One of these messages will be triggered if the `messages[].conditions` matches the "reason" argument. */
            function?: components["schemas"]["OpenAIFunction"];
            /** @description This is the server that will be hit when this tool is requested by the model.
             *
             *     All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.
             *
             *     This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: highest tool.server.url, then assistant.serverUrl, then phoneNumber.serverUrl, then org.serverUrl. */
            server?: components["schemas"]["Server"];
        };
        UserMessage: {
            /** @description The role of the user in the conversation. */
            role: string;
            /** @description The message content from the user. */
            message: string;
            /** @description The timestamp when the message was sent. */
            time: number;
            /** @description The timestamp when the message ended. */
            endTime: number;
            /** @description The number of seconds from the start of the conversation. */
            secondsFromStart: number;
            /** @description The duration of the message in seconds. */
            duration?: number;
        };
        SystemMessage: {
            /** @description The role of the system in the conversation. */
            role: string;
            /** @description The message content from the system. */
            message: string;
            /** @description The timestamp when the message was sent. */
            time: number;
            /** @description The number of seconds from the start of the conversation. */
            secondsFromStart: number;
        };
        BotMessage: {
            /** @description The role of the bot in the conversation. */
            role: string;
            /** @description The message content from the bot. */
            message: string;
            /** @description The timestamp when the message was sent. */
            time: number;
            /** @description The timestamp when the message ended. */
            endTime: number;
            /** @description The number of seconds from the start of the conversation. */
            secondsFromStart: number;
            /** @description The source of the message. */
            source?: string;
        };
        FunctionCallMessage: {
            /** @description The role of the function call in the conversation. */
            role: string;
            /** @description The message content for the function call. */
            message: string;
            /** @description The name of the function being called. */
            name: string;
            /** @description The arguments for the function call in JSON format. */
            args: string;
            /** @description The timestamp when the message was sent. */
            time: number;
            /** @description The number of seconds from the start of the conversation. */
            secondsFromStart: number;
        };
        ToolCallMessage: {
            /** @description The role of the tool call in the conversation. */
            role: string;
            /** @description The list of tool calls made during the conversation. */
            toolCalls: Record<string, never>[];
            /** @description The message content for the tool call. */
            message: string;
            /** @description The timestamp when the message was sent. */
            time: number;
            /** @description The number of seconds from the start of the conversation. */
            secondsFromStart: number;
        };
        ToolCallResultMessage: {
            /** @description The role of the tool call result in the conversation. */
            role: string;
            /** @description The ID of the tool call. */
            toolCallId: string;
            /** @description The name of the tool that returned the result. */
            name: string;
            /** @description The result of the tool call in JSON format. */
            result: string;
            /** @description The timestamp when the message was sent. */
            time: number;
            /** @description The number of seconds from the start of the conversation. */
            secondsFromStart: number;
        };
        FunctionResultMessage: {
            /** @description The role of the function result in the conversation. */
            role: string;
            /** @description The name of the function that returned the result. */
            name: string;
            /** @description The result of the function call in JSON format. */
            result: string;
            /** @description The timestamp when the message was sent. */
            time: number;
            /** @description The number of seconds from the start of the conversation. */
            secondsFromStart: number;
        };
        ClientMessageConversationUpdate: {
            /**
             * @description This is the type of the message. "conversation-update" is sent when an update is committed to the conversation history.
             * @enum {string}
             */
            type: "conversation-update";
            /** @description This is the most up-to-date conversation history at the time the message is sent. */
            messagesOpenAIFormatted: components["schemas"]["OpenAIMessage"][];
        };
        ClientMessageFunctionCall: {
            /**
             * @deprecated
             * @description This is the type of the message. "function-call" is sent to call a function.
             * @enum {string}
             */
            type: "function-call";
            /**
             * @deprecated
             * @description This is the function call content.
             */
            functionCall: Record<string, never>;
        };
        ClientMessageFunctionCallResult: {
            /**
             * @deprecated
             * @description This is the type of the message. "function-call-result" is sent to forward the result of a function call to the client.
             * @enum {string}
             */
            type: "function-call-result";
            /** @description This is the result of the function call. */
            functionCallResult: Record<string, never>;
        };
        ClientMessageHang: {
            /**
             * @description This is the type of the message. "hang" is sent when the assistant is hanging due to a delay. The delay can be caused by many factors, such as:
             *     - the model is too slow to respond
             *     - the voice is too slow to respond
             *     - the tool call is still waiting for a response from your server
             *     - etc.
             * @enum {string}
             */
            type: "hang";
        };
        ClientMessageMetadata: {
            /**
             * @description This is the type of the message. "metadata" is sent to forward metadata to the client.
             * @enum {string}
             */
            type: "metadata";
            /** @description This is the metadata content */
            metadata: string;
        };
        ClientMessageModelOutput: {
            /**
             * @description This is the type of the message. "model-output" is sent as the model outputs tokens.
             * @enum {string}
             */
            type: "model-output";
            /** @description This is the output of the model. It can be a token or tool call. */
            output: Record<string, never>;
        };
        ClientMessageSpeechUpdate: {
            /**
             * @description This is the type of the message. "speech-update" is sent whenever assistant or user start or stop speaking.
             * @enum {string}
             */
            type: "speech-update";
            /**
             * @description This is the status of the speech update.
             * @enum {string}
             */
            status: "started" | "stopped";
            /**
             * @description This is the role which the speech update is for.
             * @enum {string}
             */
            role: "assistant" | "user";
        };
        ClientMessageTranscript: {
            /**
             * @description This is the type of the message. "transcript" is sent as transcriber outputs partial or final transcript.
             * @enum {string}
             */
            type: "transcript";
            /**
             * @description This is the role for which the transcript is for.
             * @enum {string}
             */
            role: "assistant" | "user";
            /**
             * @description This is the type of the transcript.
             * @enum {string}
             */
            transcriptType: "partial" | "final";
            /** @description This is the transcript content. */
            transcript: string;
        };
        ClientMessageToolCalls: {
            /**
             * @description This is the type of the message. "tool-calls" is sent to call a tool.
             * @enum {string}
             */
            type?: "tool-calls";
            /** @description This is the list of tools calls that the model is requesting along with the original tool configuration. */
            toolWithToolCallList: (components["schemas"]["FunctionToolWithToolCall"] | components["schemas"]["GhlToolWithToolCall"] | components["schemas"]["MakeToolWithToolCall"])[];
            /** @description This is the list of tool calls that the model is requesting. */
            toolCallList: components["schemas"]["ToolCall"][];
        };
        ClientMessageToolCallsResult: {
            /**
             * @description This is the type of the message. "tool-calls-result" is sent to forward the result of a tool call to the client.
             * @enum {string}
             */
            type: "tool-calls-result";
            /** @description This is the result of the tool call. */
            toolCallResult: Record<string, never>;
        };
        ClientMessageUserInterrupted: {
            /**
             * @description This is the type of the message. "user-interrupted" is sent when the user interrupts the assistant.
             * @enum {string}
             */
            type: "user-interrupted";
        };
        ClientMessageVoiceInput: {
            /**
             * @description This is the type of the message. "voice-input" is sent when a generation is requested from voice provider.
             * @enum {string}
             */
            type: "voice-input";
            /** @description This is the voice input content */
            input: string;
        };
        ClientMessage: {
            /** @description These are all the messages that can be sent to the client-side SDKs during the call. Configure the messages you'd like to receive in `assistant.clientMessages`. */
            message: components["schemas"]["ClientMessageConversationUpdate"] | components["schemas"]["ClientMessageHang"] | components["schemas"]["ClientMessageMetadata"] | components["schemas"]["ClientMessageModelOutput"] | components["schemas"]["ClientMessageSpeechUpdate"] | components["schemas"]["ClientMessageTranscript"] | components["schemas"]["ClientMessageToolCalls"] | components["schemas"]["ClientMessageToolCallsResult"] | components["schemas"]["ClientMessageUserInterrupted"] | components["schemas"]["ClientMessageVoiceInput"];
        };
        ServerMessageAssistantRequest: {
            /**
             * @description This is the type of the message. "assistant-request" is sent to fetch assistant configuration for an incoming call.
             * @enum {string}
             */
            type: "assistant-request";
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
        };
        ServerMessageConversationUpdate: {
            /**
             * @description This is the type of the message. "conversation-update" is sent when an update is committed to the conversation history.
             * @enum {string}
             */
            type: "conversation-update";
            /** @description This is the most up-to-date conversation history at the time the message is sent. */
            messagesOpenAIFormatted: components["schemas"]["OpenAIMessage"][];
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
        };
        ServerMessageEndOfCallReport: {
            /**
             * @description This is the type of the message. "end-of-call-report" is sent when the call ends and post-processing is complete.
             * @enum {string}
             */
            type: "end-of-call-report";
            /**
             * @description This is the reason the call ended.
             * @enum {string}
             */
            endedReason: "assistant-error" | "assistant-not-found" | "db-error" | "no-server-available" | "pipeline-error-openai-llm-failed" | "pipeline-error-azure-openai-llm-failed" | "pipeline-error-groq-llm-failed" | "pipeline-error-openai-voice-failed" | "pipeline-error-cartesia-voice-failed" | "pipeline-error-deepgram-transcriber-failed" | "pipeline-error-deepgram-voice-failed" | "pipeline-error-gladia-transcriber-failed" | "pipeline-error-eleven-labs-voice-failed" | "pipeline-error-playht-voice-failed" | "pipeline-error-lmnt-voice-failed" | "pipeline-error-azure-voice-failed" | "pipeline-error-rime-ai-voice-failed" | "pipeline-error-neets-voice-failed" | "pipeline-no-available-model" | "worker-shutdown" | "twilio-failed-to-connect-call" | "unknown-error" | "vonage-disconnected" | "vonage-failed-to-connect-call" | "phone-call-provider-bypass-enabled-but-no-call-received" | "vapi-error-phone-call-worker-setup-socket-error" | "vapi-error-phone-call-worker-worker-setup-socket-timeout" | "vapi-error-phone-call-worker-could-not-find-call" | "vapi-error-phone-call-worker-call-never-connected" | "vapi-error-web-call-worker-setup-failed" | "assistant-not-invalid" | "assistant-not-provided" | "call-start-error-neither-assistant-nor-server-set" | "assistant-request-failed" | "assistant-request-returned-error" | "assistant-request-returned-unspeakable-error" | "assistant-request-returned-invalid-assistant" | "assistant-request-returned-no-assistant" | "assistant-request-returned-forwarding-phone-number" | "assistant-ended-call" | "assistant-said-end-call-phrase" | "assistant-forwarded-call" | "assistant-join-timed-out" | "customer-busy" | "customer-ended-call" | "customer-did-not-answer" | "customer-did-not-give-microphone-permission" | "assistant-said-message-with-end-call-enabled" | "exceeded-max-duration" | "manually-canceled" | "phone-call-provider-closed-websocket" | "pipeline-error-anthropic-llm-failed" | "pipeline-error-together-ai-llm-failed" | "pipeline-error-anyscale-llm-failed" | "pipeline-error-openrouter-llm-failed" | "pipeline-error-perplexity-ai-llm-failed" | "pipeline-error-deepinfra-llm-failed" | "pipeline-error-runpod-llm-failed" | "pipeline-error-custom-llm-llm-failed" | "pipeline-error-eleven-labs-voice-not-found" | "pipeline-error-eleven-labs-quota-exceeded" | "pipeline-error-eleven-labs-unauthorized-access" | "pipeline-error-eleven-labs-unauthorized-to-access-model" | "pipeline-error-eleven-labs-professional-voices-only-for-creator-plus" | "pipeline-error-eleven-labs-blocked-free-plan-and-requested-upgrade" | "pipeline-error-eleven-labs-blocked-concurrent-requests-and-requested-upgrade" | "pipeline-error-eleven-labs-blocked-using-instant-voice-clone-and-requested-upgrade" | "pipeline-error-eleven-labs-system-busy-and-requested-upgrade" | "pipeline-error-eleven-labs-voice-not-fine-tuned" | "pipeline-error-eleven-labs-invalid-api-key" | "pipeline-error-eleven-labs-invalid-voice-samples" | "pipeline-error-eleven-labs-voice-disabled-by-owner" | "pipeline-error-eleven-labs-blocked-account-in-probation" | "pipeline-error-eleven-labs-blocked-content-against-their-policy" | "pipeline-error-playht-request-timed-out" | "pipeline-error-playht-invalid-voice" | "pipeline-error-playht-unexpected-error" | "pipeline-error-playht-out-of-credits" | "pipeline-error-playht-rate-limit-exceeded" | "pipeline-error-playht-502-gateway-error" | "pipeline-error-playht-504-gateway-error" | "pipeline-error-gladia-transcriber-failed" | "sip-gateway-failed-to-connect-call" | "silence-timed-out" | "voicemail" | "vonage-rejected";
            /** @description These are the message history of the call. The format is not OpenAI format but a custom VAPI format. */
            messages?: (components["schemas"]["UserMessage"] | components["schemas"]["SystemMessage"] | components["schemas"]["BotMessage"] | components["schemas"]["FunctionCallMessage"] | components["schemas"]["ToolCallMessage"] | components["schemas"]["ToolCallResultMessage"] | components["schemas"]["FunctionResultMessage"])[];
            /** @description This is the URL of the call recording. */
            recordingUrl?: string;
            /** @description This is the URL of the stereo call recording. */
            stereoRecordingUrl?: string;
            /** @description This is the WAV buffer of the call recording. */
            recordingWav?: Record<string, never>;
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the artifacts from the call. */
            artifact?: components["schemas"]["Artifact"];
            /** @description This is the timestamp of the message. */
            timestamp?: string;
            /** @description This is the transcript of the call. */
            transcript: string;
            /** @description This is the summary of the call. */
            summary: string;
            /** @description This is the analysis of the call. */
            analysis: components["schemas"]["Analysis"];
        };
        ServerMessageHang: {
            /**
             * @description This is the type of the message. "hang" is sent when the assistant is hanging due to a delay. The delay can be caused by many factors, such as:
             *     - the model is too slow to respond
             *     - the voice is too slow to respond
             *     - the tool call is still waiting for a response from your server
             *     - etc.
             * @enum {string}
             */
            type: "hang";
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
        };
        ServerMessageModelOutput: {
            /**
             * @description This is the type of the message. "model-output" is sent as the model outputs tokens.
             * @enum {string}
             */
            type: "model-output";
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
            /** @description This is the output of the model. It can be a token or tool call. */
            output: Record<string, never>;
        };
        ServerMessagePhoneCallControl: {
            /**
             * @description This is the type of the message. "phone-call-control" is an advanced type of message.
             *
             *     When it is requested in `assistant.serverMessages`, the hangup and forwarding responsibilities are delegated to your server. Vapi will no longer do the actual transfer and hangup.
             * @enum {string}
             */
            type: "phone-call-control";
            /**
             * @description This is the request to control the phone call.
             * @enum {string}
             */
            request: "forward" | "hang-up";
            /** @description This is the destination to forward the call to if the request is "forward". */
            destination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
        };
        ServerMessageSpeechUpdate: {
            /**
             * @description This is the type of the message. "speech-update" is sent whenever assistant or user start or stop speaking.
             * @enum {string}
             */
            type: "speech-update";
            /**
             * @description This is the status of the speech update.
             * @enum {string}
             */
            status: "started" | "stopped";
            /**
             * @description This is the role which the speech update is for.
             * @enum {string}
             */
            role: "assistant" | "user";
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
        };
        ServerMessageStatusUpdate: {
            /**
             * @description This is the type of the message. "status-update" is sent whenever the `call.status` changes.
             * @enum {string}
             */
            type: "status-update";
            /**
             * @description This is the status of the call.
             * @enum {string}
             */
            status: "queued" | "ringing" | "in-progress" | "forwarding" | "ended";
            /**
             * @description This is the reason the call ended. This is only sent if the status is "ended".
             * @enum {string}
             */
            endedReason?: "assistant-error" | "assistant-not-found" | "db-error" | "no-server-available" | "pipeline-error-openai-llm-failed" | "pipeline-error-azure-openai-llm-failed" | "pipeline-error-groq-llm-failed" | "pipeline-error-openai-voice-failed" | "pipeline-error-cartesia-voice-failed" | "pipeline-error-deepgram-transcriber-failed" | "pipeline-error-deepgram-voice-failed" | "pipeline-error-gladia-transcriber-failed" | "pipeline-error-eleven-labs-voice-failed" | "pipeline-error-playht-voice-failed" | "pipeline-error-lmnt-voice-failed" | "pipeline-error-azure-voice-failed" | "pipeline-error-rime-ai-voice-failed" | "pipeline-error-neets-voice-failed" | "pipeline-no-available-model" | "worker-shutdown" | "twilio-failed-to-connect-call" | "unknown-error" | "vonage-disconnected" | "vonage-failed-to-connect-call" | "phone-call-provider-bypass-enabled-but-no-call-received" | "vapi-error-phone-call-worker-setup-socket-error" | "vapi-error-phone-call-worker-worker-setup-socket-timeout" | "vapi-error-phone-call-worker-could-not-find-call" | "vapi-error-phone-call-worker-call-never-connected" | "vapi-error-web-call-worker-setup-failed" | "assistant-not-invalid" | "assistant-not-provided" | "call-start-error-neither-assistant-nor-server-set" | "assistant-request-failed" | "assistant-request-returned-error" | "assistant-request-returned-unspeakable-error" | "assistant-request-returned-invalid-assistant" | "assistant-request-returned-no-assistant" | "assistant-request-returned-forwarding-phone-number" | "assistant-ended-call" | "assistant-said-end-call-phrase" | "assistant-forwarded-call" | "assistant-join-timed-out" | "customer-busy" | "customer-ended-call" | "customer-did-not-answer" | "customer-did-not-give-microphone-permission" | "assistant-said-message-with-end-call-enabled" | "exceeded-max-duration" | "manually-canceled" | "phone-call-provider-closed-websocket" | "pipeline-error-anthropic-llm-failed" | "pipeline-error-together-ai-llm-failed" | "pipeline-error-anyscale-llm-failed" | "pipeline-error-openrouter-llm-failed" | "pipeline-error-perplexity-ai-llm-failed" | "pipeline-error-deepinfra-llm-failed" | "pipeline-error-runpod-llm-failed" | "pipeline-error-custom-llm-llm-failed" | "pipeline-error-eleven-labs-voice-not-found" | "pipeline-error-eleven-labs-quota-exceeded" | "pipeline-error-eleven-labs-unauthorized-access" | "pipeline-error-eleven-labs-unauthorized-to-access-model" | "pipeline-error-eleven-labs-professional-voices-only-for-creator-plus" | "pipeline-error-eleven-labs-blocked-free-plan-and-requested-upgrade" | "pipeline-error-eleven-labs-blocked-concurrent-requests-and-requested-upgrade" | "pipeline-error-eleven-labs-blocked-using-instant-voice-clone-and-requested-upgrade" | "pipeline-error-eleven-labs-system-busy-and-requested-upgrade" | "pipeline-error-eleven-labs-voice-not-fine-tuned" | "pipeline-error-eleven-labs-invalid-api-key" | "pipeline-error-eleven-labs-invalid-voice-samples" | "pipeline-error-eleven-labs-voice-disabled-by-owner" | "pipeline-error-eleven-labs-blocked-account-in-probation" | "pipeline-error-eleven-labs-blocked-content-against-their-policy" | "pipeline-error-playht-request-timed-out" | "pipeline-error-playht-invalid-voice" | "pipeline-error-playht-unexpected-error" | "pipeline-error-playht-out-of-credits" | "pipeline-error-playht-rate-limit-exceeded" | "pipeline-error-playht-502-gateway-error" | "pipeline-error-playht-504-gateway-error" | "pipeline-error-gladia-transcriber-failed" | "sip-gateway-failed-to-connect-call" | "silence-timed-out" | "voicemail" | "vonage-rejected";
            /** @description These are the conversation messages of the call. This is only sent if the status is "forwarding". */
            messages?: (components["schemas"]["UserMessage"] | components["schemas"]["SystemMessage"] | components["schemas"]["BotMessage"] | components["schemas"]["FunctionCallMessage"] | components["schemas"]["ToolCallMessage"] | components["schemas"]["ToolCallResultMessage"] | components["schemas"]["FunctionResultMessage"])[];
            /** @description These are the conversation messages of the call. This is only sent if the status is "forwarding". */
            messagesOpenAIFormatted?: components["schemas"]["OpenAIMessage"][];
            /** @description This is the destination the call is being transferred to. This is only sent if the status is "forwarding". */
            destination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
            /** @description This is the transcript of the call. This is only sent if the status is "forwarding". */
            transcript?: string;
            /** @description This is the inbound phone call debugging artifacts. This is only sent if the status is "ended" and there was an error accepting the inbound phone call.
             *
             *     This will include any errors related to the "assistant-request" if one was made. */
            inboundPhoneCallDebuggingArtifacts?: Record<string, never>;
        };
        ServerMessageToolCalls: {
            /**
             * @description This is the type of the message. "tool-calls" is sent to call a tool.
             * @enum {string}
             */
            type?: "tool-calls";
            /** @description This is the list of tools calls that the model is requesting along with the original tool configuration. */
            toolWithToolCallList: (components["schemas"]["FunctionToolWithToolCall"] | components["schemas"]["GhlToolWithToolCall"] | components["schemas"]["MakeToolWithToolCall"])[];
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
            /** @description This is the list of tool calls that the model is requesting. */
            toolCallList: components["schemas"]["ToolCall"][];
        };
        ServerMessageTransferDestinationRequest: {
            /**
             * @description This is the type of the message. "transfer-destination-request" is sent when the model is requesting transfer but destination is unknown.
             * @enum {string}
             */
            type: "transfer-destination-request";
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
        };
        ServerMessageTransferUpdate: {
            /**
             * @description This is the type of the message. "transfer-update" is sent whenever a transfer happens.
             * @enum {string}
             */
            type: "transfer-update";
            /** @description This is the destination of the transfer. */
            destination?: components["schemas"]["TransferDestinationAssistant"] | components["schemas"]["TransferDestinationStep"] | components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
            /** @description This is the assistant that the call is being transferred to. This is only sent if `destination.type` is "assistant". */
            toAssistant?: components["schemas"]["CreateAssistantDTO"];
            /** @description This is the assistant that the call is being transferred from. This is only sent if `destination.type` is "assistant". */
            fromAssistant?: components["schemas"]["CreateAssistantDTO"];
        };
        ServerMessageTranscript: {
            /**
             * @description This is the type of the message. "transcript" is sent as transcriber outputs partial or final transcript.
             * @enum {string}
             */
            type: "transcript";
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
            /**
             * @description This is the role for which the transcript is for.
             * @enum {string}
             */
            role: "assistant" | "user";
            /**
             * @description This is the type of the transcript.
             * @enum {string}
             */
            transcriptType: "partial" | "final";
            /** @description This is the transcript content. */
            transcript: string;
        };
        ServerMessageUserInterrupted: {
            /**
             * @description This is the type of the message. "user-interrupted" is sent when the user interrupts the assistant.
             * @enum {string}
             */
            type: "user-interrupted";
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
        };
        ServerMessageVoiceInput: {
            /**
             * @description This is the type of the message. "voice-input" is sent when a generation is requested from voice provider.
             * @enum {string}
             */
            type: "voice-input";
            /** @description The phone number associated with the call. This either directly matches `call.phoneNumber` or is expanded from `call.phoneNumberId`. */
            phoneNumber?: Record<string, never>;
            /** @description The customer associated with the call. This either directly matches `call.customer` or is expanded from `call.customerId`. */
            customer: Record<string, never>;
            /** @description This is the main `call` object of the call. */
            call: Record<string, never>;
            /** @description These are the live artifacts of the call. */
            artifact?: Record<string, never>;
            /** @description This is the timestamp of the message. */
            timestamp?: string;
            /** @description This is the voice input content */
            input: string;
        };
        ServerMessage: {
            /** @description These are all the messages that can be sent to your server before, after and during the call. Configure the messages you'd like to receive in `assistant.serverMessages`.
             *
             *     The server where the message is sent is determined by the following precedence order:
             *
             *     1. `tool.server.url` (if configured, and only for "tool-calls" message)
             *     2. `assistant.serverUrl` (if configure)
             *     3. `phoneNumber.serverUrl` (if configured)
             *     4. `org.serverUrl` (if configured) */
            message: components["schemas"]["ServerMessageAssistantRequest"] | components["schemas"]["ServerMessageConversationUpdate"] | components["schemas"]["ServerMessageEndOfCallReport"] | components["schemas"]["ServerMessageHang"] | components["schemas"]["ServerMessageModelOutput"] | components["schemas"]["ServerMessagePhoneCallControl"] | components["schemas"]["ServerMessageSpeechUpdate"] | components["schemas"]["ServerMessageStatusUpdate"] | components["schemas"]["ServerMessageToolCalls"] | components["schemas"]["ServerMessageTransferDestinationRequest"] | components["schemas"]["ServerMessageTransferUpdate"] | components["schemas"]["ServerMessageTranscript"] | components["schemas"]["ServerMessageUserInterrupted"] | components["schemas"]["ServerMessageVoiceInput"];
        };
        ServerMessageResponseAssistantRequest: {
            /** @description This is the destination to transfer the inbound call to. This will immediately transfer without using any assistants.
             *
             *     If this is sent, `assistantId`, `assistant`, `squadId`, and `squad` are ignored. */
            destination?: components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /** @description This is the assistant that will be used for the call. To use a transient assistant, use `assistant` instead. */
            assistantId?: string | null;
            /** @description This is the assistant that will be used for the call. To use an existing assistant, use `assistantId` instead.
             *
             *     If you're unsure why you're getting an invalid assistant, try logging your response and send the JSON blob to POST /assistant which will return the validation errors. */
            assistant?: components["schemas"]["CreateAssistantDTO"];
            /** @description These are the overrides for the `assistant` or `assistantId`'s settings and template variables. */
            assistantOverrides?: components["schemas"]["AssistantOverrides"];
            /** @description This is the squad that will be used for the call. To use a transient squad, use `squad` instead. */
            squadId?: string;
            /** @description This is a squad that will be used for the call. To use an existing squad, use `squadId` instead. */
            squad?: components["schemas"]["CreateSquadDTO"];
            /** @description This is the error if the call shouldn't be accepted. This is spoken to the customer.
             *
             *     If this is sent, `assistantId`, `assistant`, `squadId`, `squad`, and `destination` are ignored. */
            error?: string;
        };
        ToolCallResult: {
            /** @description This is the message that will be spoken to the user.
             *
             *     If this is not returned, assistant will speak:
             *     1. a `request-complete` or `request-failed` message from `tool.messages`, if it exists
             *     2. a response generated by the model, if not */
            message?: (components["schemas"]["ToolMessageComplete"] | components["schemas"]["ToolMessageFailed"])[];
            /** @description This is the name of the function the model called. */
            name: string;
            /** @description This is the unique identifier for the tool call. */
            toolCallId: string;
            /** @description This is the result if the tool call was successful. This is added to the conversation history.
             *
             *     Further, if this is returned, assistant will speak:
             *     1. the `message`, if it exists and is of type `request-complete`
             *     2. a `request-complete` message from `tool.messages`, if it exists
             *     3. a response generated by the model, if neither exist */
            result?: string;
            /** @description This is the error if the tool call was not successful. This is added to the conversation history.
             *
             *     Further, if this is returned, assistant will speak:
             *     1. the `message`, if it exists and is of type `request-failed`
             *     2. a `request-failed` message from `tool.messages`, if it exists
             *     3. a response generated by the model, if neither exist */
            error?: string;
        };
        ServerMessageResponseToolCalls: {
            /** @description These are the results of the "tool-calls" message. */
            results?: components["schemas"]["ToolCallResult"][];
            /** @description This is the error message if the tool call was not successful. */
            error?: string;
        };
        ServerMessageResponseTransferDestinationRequest: {
            /** @description This is the destination you'd like the call to be transferred to. */
            destination?: components["schemas"]["TransferDestinationAssistant"] | components["schemas"]["TransferDestinationStep"] | components["schemas"]["TransferDestinationNumber"] | components["schemas"]["TransferDestinationSip"];
            /** @description This is the error message if the transfer should not be made. */
            error?: string;
        };
        ServerMessageResponse: {
            /** @description This is the response that is expected from the server to the message.
             *
             *     Note: Most messages don't expect a response. Only "assistant-request", "tool-calls" and "transfer-destination-request" do. */
            messageResponse: components["schemas"]["ServerMessageResponseAssistantRequest"] | components["schemas"]["ServerMessageResponseToolCalls"] | components["schemas"]["ServerMessageResponseTransferDestinationRequest"];
        };
        ClientInboundMessageAddMessage: {
            /**
             * @description This is the type of the message. Send "add-message" message to add a message to the conversation history.
             * @enum {string}
             */
            type: "add-message";
            /** @description This is the message to add to the conversation. */
            message: components["schemas"]["OpenAIMessage"];
        };
        ClientInboundMessageControl: {
            /**
             * @description This is the type of the message. Send "control" message to control the assistant. `control` options are:
             *     - "mute-assistant" - mute the assistant
             *     - "unmute-assistant" - unmute the assistant
             *     - "say-first-message" - say the first message (this is used when video recording is enabled and the conversation is only started once the client side kicks off the recording)
             * @enum {string}
             */
            type: "control";
            /**
             * @description This is the control action
             * @enum {string}
             */
            control: "mute-assistant" | "unmute-assistant" | "say-first-message";
        };
        ClientInboundMessageSay: {
            /**
             * @description This is the type of the message. Send "say" message to make the assistant say something.
             * @enum {string}
             */
            type?: "say";
            /** @description This is the content to say. */
            content?: string;
            /** @description This is the flag to end call after content is spoken. */
            endCallAfterSpoken?: boolean;
        };
        ClientInboundMessage: {
            /** @description These are the messages that can be sent from client-side SDKs to control the call. */
            message: components["schemas"]["ClientInboundMessageAddMessage"] | components["schemas"]["ClientInboundMessageControl"] | components["schemas"]["ClientInboundMessageSay"];
        };
    };
    responses: never;
    parameters: never;
    requestBodies: never;
    headers: never;
    pathItems: never;
}
export type $defs = Record<string, never>;
export interface operations {
    CallController_findAll: {
        parameters: {
            query?: {
                /** @description This will return calls with the specified assistantId. */
                assistantId?: string;
                /** @description This is the maximum number of items to return. Defaults to 100. */
                limit?: number;
                /** @description This will return items where the createdAt is greater than the specified value. */
                createdAtGt?: string;
                /** @description This will return items where the createdAt is less than the specified value. */
                createdAtLt?: string;
                /** @description This will return items where the createdAt is greater than or equal to the specified value. */
                createdAtGe?: string;
                /** @description This will return items where the createdAt is less than or equal to the specified value. */
                createdAtLe?: string;
                /** @description This will return items where the updatedAt is greater than the specified value. */
                updatedAtGt?: string;
                /** @description This will return items where the updatedAt is less than the specified value. */
                updatedAtLt?: string;
                /** @description This will return items where the updatedAt is greater than or equal to the specified value. */
                updatedAtGe?: string;
                /** @description This will return items where the updatedAt is less than or equal to the specified value. */
                updatedAtLe?: string;
            };
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Call"][];
                };
            };
        };
    };
    CallController_create: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["CreateCallDTO"];
            };
        };
        responses: {
            201: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Call"];
                };
            };
        };
    };
    CallController_findOne: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Call"];
                };
            };
        };
    };
    CallController_deleteCallData: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Call"];
                };
            };
        };
    };
    CallController_update: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["UpdateCallDTO"];
            };
        };
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Call"];
                };
            };
        };
    };
    AssistantController_findAll: {
        parameters: {
            query?: {
                /** @description This is the maximum number of items to return. Defaults to 100. */
                limit?: number;
                /** @description This will return items where the createdAt is greater than the specified value. */
                createdAtGt?: string;
                /** @description This will return items where the createdAt is less than the specified value. */
                createdAtLt?: string;
                /** @description This will return items where the createdAt is greater than or equal to the specified value. */
                createdAtGe?: string;
                /** @description This will return items where the createdAt is less than or equal to the specified value. */
                createdAtLe?: string;
                /** @description This will return items where the updatedAt is greater than the specified value. */
                updatedAtGt?: string;
                /** @description This will return items where the updatedAt is less than the specified value. */
                updatedAtLt?: string;
                /** @description This will return items where the updatedAt is greater than or equal to the specified value. */
                updatedAtGe?: string;
                /** @description This will return items where the updatedAt is less than or equal to the specified value. */
                updatedAtLe?: string;
            };
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Assistant"][];
                };
            };
        };
    };
    AssistantController_create: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["CreateAssistantDTO"];
            };
        };
        responses: {
            201: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Assistant"];
                };
            };
        };
    };
    AssistantController_findOne: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Assistant"];
                };
            };
        };
    };
    AssistantController_remove: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Assistant"];
                };
            };
        };
    };
    AssistantController_update: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["UpdateAssistantDTO"];
            };
        };
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Assistant"];
                };
            };
        };
    };
    PhoneNumberController_findAll: {
        parameters: {
            query?: {
                /** @description This is the maximum number of items to return. Defaults to 100. */
                limit?: number;
                /** @description This will return items where the createdAt is greater than the specified value. */
                createdAtGt?: string;
                /** @description This will return items where the createdAt is less than the specified value. */
                createdAtLt?: string;
                /** @description This will return items where the createdAt is greater than or equal to the specified value. */
                createdAtGe?: string;
                /** @description This will return items where the createdAt is less than or equal to the specified value. */
                createdAtLe?: string;
                /** @description This will return items where the updatedAt is greater than the specified value. */
                updatedAtGt?: string;
                /** @description This will return items where the updatedAt is less than the specified value. */
                updatedAtLt?: string;
                /** @description This will return items where the updatedAt is greater than or equal to the specified value. */
                updatedAtGe?: string;
                /** @description This will return items where the updatedAt is less than or equal to the specified value. */
                updatedAtLe?: string;
            };
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": (components["schemas"]["ByoPhoneNumber"] | components["schemas"]["TwilioPhoneNumber"] | components["schemas"]["VonagePhoneNumber"] | components["schemas"]["VapiPhoneNumber"])[];
                };
            };
        };
    };
    PhoneNumberController_create: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["CreateByoPhoneNumberDTO"] | components["schemas"]["CreateTwilioPhoneNumberDTO"] | components["schemas"]["CreateVonagePhoneNumberDTO"] | components["schemas"]["CreateVapiPhoneNumberDTO"];
            };
        };
        responses: {
            201: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ByoPhoneNumber"] | components["schemas"]["TwilioPhoneNumber"] | components["schemas"]["VonagePhoneNumber"] | components["schemas"]["VapiPhoneNumber"];
                };
            };
        };
    };
    PhoneNumberController_findOne: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ByoPhoneNumber"] | components["schemas"]["TwilioPhoneNumber"] | components["schemas"]["VonagePhoneNumber"] | components["schemas"]["VapiPhoneNumber"];
                };
            };
        };
    };
    PhoneNumberController_remove: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ByoPhoneNumber"] | components["schemas"]["TwilioPhoneNumber"] | components["schemas"]["VonagePhoneNumber"] | components["schemas"]["VapiPhoneNumber"];
                };
            };
        };
    };
    PhoneNumberController_update: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["UpdatePhoneNumberDTO"];
            };
        };
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ByoPhoneNumber"] | components["schemas"]["TwilioPhoneNumber"] | components["schemas"]["VonagePhoneNumber"] | components["schemas"]["VapiPhoneNumber"];
                };
            };
        };
    };
    SquadController_findAll: {
        parameters: {
            query?: {
                /** @description This is the maximum number of items to return. Defaults to 100. */
                limit?: number;
                /** @description This will return items where the createdAt is greater than the specified value. */
                createdAtGt?: string;
                /** @description This will return items where the createdAt is less than the specified value. */
                createdAtLt?: string;
                /** @description This will return items where the createdAt is greater than or equal to the specified value. */
                createdAtGe?: string;
                /** @description This will return items where the createdAt is less than or equal to the specified value. */
                createdAtLe?: string;
                /** @description This will return items where the updatedAt is greater than the specified value. */
                updatedAtGt?: string;
                /** @description This will return items where the updatedAt is less than the specified value. */
                updatedAtLt?: string;
                /** @description This will return items where the updatedAt is greater than or equal to the specified value. */
                updatedAtGe?: string;
                /** @description This will return items where the updatedAt is less than or equal to the specified value. */
                updatedAtLe?: string;
            };
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Squad"][];
                };
            };
        };
    };
    SquadController_create: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["CreateSquadDTO"];
            };
        };
        responses: {
            201: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Squad"];
                };
            };
        };
    };
    SquadController_findOne: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Squad"];
                };
            };
        };
    };
    SquadController_remove: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Squad"];
                };
            };
        };
    };
    SquadController_update: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["UpdateSquadDTO"];
            };
        };
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["Squad"];
                };
            };
        };
    };
    BlockController_findAll: {
        parameters: {
            query?: {
                /** @description This is the maximum number of items to return. Defaults to 100. */
                limit?: number;
                /** @description This will return items where the createdAt is greater than the specified value. */
                createdAtGt?: string;
                /** @description This will return items where the createdAt is less than the specified value. */
                createdAtLt?: string;
                /** @description This will return items where the createdAt is greater than or equal to the specified value. */
                createdAtGe?: string;
                /** @description This will return items where the createdAt is less than or equal to the specified value. */
                createdAtLe?: string;
                /** @description This will return items where the updatedAt is greater than the specified value. */
                updatedAtGt?: string;
                /** @description This will return items where the updatedAt is less than the specified value. */
                updatedAtLt?: string;
                /** @description This will return items where the updatedAt is greater than or equal to the specified value. */
                updatedAtGe?: string;
                /** @description This will return items where the updatedAt is less than or equal to the specified value. */
                updatedAtLe?: string;
            };
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": (components["schemas"]["ConversationBlock"] | components["schemas"]["ToolCallBlock"] | components["schemas"]["WorkflowBlock"])[];
                };
            };
        };
    };
    BlockController_create: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["CreateConversationBlockDTO"] | components["schemas"]["CreateToolCallBlockDTO"] | components["schemas"]["CreateWorkflowBlockDTO"];
            };
        };
        responses: {
            201: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ConversationBlock"] | components["schemas"]["ToolCallBlock"] | components["schemas"]["WorkflowBlock"];
                };
            };
        };
    };
    BlockController_findOne: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ConversationBlock"] | components["schemas"]["ToolCallBlock"] | components["schemas"]["WorkflowBlock"];
                };
            };
        };
    };
    BlockController_remove: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ConversationBlock"] | components["schemas"]["ToolCallBlock"] | components["schemas"]["WorkflowBlock"];
                };
            };
        };
    };
    BlockController_update: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["UpdateBlockDTO"];
            };
        };
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["ConversationBlock"] | components["schemas"]["ToolCallBlock"] | components["schemas"]["WorkflowBlock"];
                };
            };
        };
    };
    ToolController_findAll: {
        parameters: {
            query?: {
                /** @description This is the maximum number of items to return. Defaults to 100. */
                limit?: number;
                /** @description This will return items where the createdAt is greater than the specified value. */
                createdAtGt?: string;
                /** @description This will return items where the createdAt is less than the specified value. */
                createdAtLt?: string;
                /** @description This will return items where the createdAt is greater than or equal to the specified value. */
                createdAtGe?: string;
                /** @description This will return items where the createdAt is less than or equal to the specified value. */
                createdAtLe?: string;
                /** @description This will return items where the updatedAt is greater than the specified value. */
                updatedAtGt?: string;
                /** @description This will return items where the updatedAt is less than the specified value. */
                updatedAtLt?: string;
                /** @description This will return items where the updatedAt is greater than or equal to the specified value. */
                updatedAtGe?: string;
                /** @description This will return items where the updatedAt is less than or equal to the specified value. */
                updatedAtLe?: string;
            };
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": (components["schemas"]["DtmfTool"] | components["schemas"]["EndCallTool"] | components["schemas"]["FunctionTool"] | components["schemas"]["GhlTool"] | components["schemas"]["MakeTool"] | components["schemas"]["TransferCallTool"])[];
                };
            };
        };
    };
    ToolController_create: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["CreateDtmfToolDTO"] | components["schemas"]["CreateEndCallToolDTO"] | components["schemas"]["CreateFunctionToolDTO"] | components["schemas"]["CreateGhlToolDTO"] | components["schemas"]["CreateMakeToolDTO"] | components["schemas"]["CreateTransferCallToolDTO"];
            };
        };
        responses: {
            201: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["DtmfTool"] | components["schemas"]["EndCallTool"] | components["schemas"]["FunctionTool"] | components["schemas"]["GhlTool"] | components["schemas"]["MakeTool"] | components["schemas"]["TransferCallTool"];
                };
            };
        };
    };
    ToolController_findOne: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["DtmfTool"] | components["schemas"]["EndCallTool"] | components["schemas"]["FunctionTool"] | components["schemas"]["GhlTool"] | components["schemas"]["MakeTool"] | components["schemas"]["TransferCallTool"];
                };
            };
        };
    };
    ToolController_remove: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["DtmfTool"] | components["schemas"]["EndCallTool"] | components["schemas"]["FunctionTool"] | components["schemas"]["GhlTool"] | components["schemas"]["MakeTool"] | components["schemas"]["TransferCallTool"];
                };
            };
        };
    };
    ToolController_update: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["UpdateToolDTO"];
            };
        };
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["DtmfTool"] | components["schemas"]["EndCallTool"] | components["schemas"]["FunctionTool"] | components["schemas"]["GhlTool"] | components["schemas"]["MakeTool"] | components["schemas"]["TransferCallTool"];
                };
            };
        };
    };
    FileController_findAll: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["File"][];
                };
            };
        };
    };
    FileController_create: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "multipart/form-data": components["schemas"]["CreateFileDTO"];
            };
        };
        responses: {
            /** @description File uploaded successfully */
            201: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["File"];
                };
            };
            /** @description Invalid file */
            400: {
                headers: {
                    [name: string]: unknown;
                };
                content?: never;
            };
        };
    };
    FileController_findOne: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["File"];
                };
            };
        };
    };
    FileController_remove: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody?: never;
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["File"];
                };
            };
        };
    };
    FileController_update: {
        parameters: {
            query?: never;
            header?: never;
            path: {
                id: string;
            };
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["UpdateFileDTO"];
            };
        };
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["File"];
                };
            };
        };
    };
    AnalyticsController_query: {
        parameters: {
            query?: never;
            header?: never;
            path?: never;
            cookie?: never;
        };
        requestBody: {
            content: {
                "application/json": components["schemas"]["AnalyticsQueryDTO"];
            };
        };
        responses: {
            200: {
                headers: {
                    [name: string]: unknown;
                };
                content: {
                    "application/json": components["schemas"]["AnalyticsQueryResult"][];
                };
            };
            201: {
                headers: {
                    [name: string]: unknown;
                };
                content?: never;
            };
        };
    };
}
